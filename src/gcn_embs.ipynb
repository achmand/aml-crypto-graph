{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import cryptoaml.datareader as cdr\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipticdr = cdr.get_data(\"elliptic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN model \n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_features, \n",
    "                 out_features, \n",
    "                 bias=True):\n",
    "        \n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 nfeat, \n",
    "                 nhid, \n",
    "                 nclass, \n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        # https://github.com/tkipf/pygcn/issues/26#issuecomment-435801483\n",
    "        # \"In this case itâ€™s best to simply take the embeddings just before doing \n",
    "        # the last linear projection to the softmax logits. \n",
    "        # In other words, if the last layer is softmax(A*H*W), \n",
    "        # take either the embedding H directly or A*H.\"\n",
    "       \n",
    "        # extract node embeddings (A*H)\n",
    "        self.node_embeddings = torch.mm(adj, x)\n",
    "        \n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def load_elliptic(datareader):\n",
    "    \n",
    "    # get all labels  \n",
    "    data = datareader.dataset_.copy()\n",
    "    labelled_data = data[(data[\"class\"] != -1)]\n",
    "    \n",
    "    # get features \n",
    "    feature_cols = [\"txId\"] + datareader.feature_cols_AF_\n",
    "    labelled_features = labelled_data[feature_cols].copy()\n",
    "    labelled_features.set_index(\"txId\", inplace=True) \n",
    "    features = sp.csr_matrix(labelled_features.values, dtype=np.float32)    \n",
    "    \n",
    "    # build edges \n",
    "    tx_ids = labelled_features.index\n",
    "    idx_map = {j: i for i, j in enumerate(tx_ids)}\n",
    "    edges_unordered = datareader.edges_.copy() \n",
    "    edges_unordered = edges_unordered[(edges_unordered[\"txId2\"].isin(set(tx_ids))) & \n",
    "                               (edges_unordered[\"txId1\"].isin(set(tx_ids)))].values\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)    \n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                    shape=(labelled_features.shape[0], labelled_features.shape[0]),\n",
    "                    dtype=np.float32)\n",
    "    \n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    \n",
    "    # get idx for train and test \n",
    "    total_train = labelled_data[labelled_data[\"ts\"] <= 34].shape[0]\n",
    "    idx_train = range(total_train)\n",
    "    idx_train = torch.LongTensor(idx_train)   \n",
    "    total_test = labelled_data[labelled_data[\"ts\"] > 34].shape[0]\n",
    "    idx_test = range(total_train, total_train+total_test)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    # change data to torch tensors \n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labelled_data[\"class\"].values)\n",
    "    \n",
    "    return adj, features, labels, idx_train, idx_test\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def precision(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return precision_score(labels, preds, average=\"binary\")\n",
    "\n",
    "def recall(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return recall_score(labels, preds, average=\"binary\")\n",
    "\n",
    "def fscore_micro(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return f1_score(labels, preds, average=\"micro\")\n",
    "\n",
    "def fscore(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return f1_score(labels, preds, average=\"binary\")\n",
    "\n",
    "def confusion(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setup \n",
    "\n",
    "# build graph data \n",
    "adj, features, labels, idx_train, idx_test = load_elliptic(ellipticdr)\n",
    "\n",
    "# model \n",
    "n_classes = 2\n",
    "n_features = 166\n",
    "n_hidden = 100\n",
    "dropout = 0.5\n",
    "gcn = GCN(nfeat=n_features,\n",
    "          nhid=n_hidden,\n",
    "          nclass=n_classes,\n",
    "          dropout=dropout)\n",
    "\n",
    "# optimizer \n",
    "learning_rate = 0.001\n",
    "gcn_params = gcn.parameters()\n",
    "optimizer = optim.Adam(gcn_params,\n",
    "                       lr=learning_rate)\n",
    "weight_ratio = torch.FloatTensor([0.3, 0.7])\n",
    "loss = nn.CrossEntropyLoss(weight=weight_ratio)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     gcn.cuda()\n",
    "#     features = features.cuda()\n",
    "#     adj = adj.cuda()\n",
    "#     labels = labels.cuda()\n",
    "#     idx_train = idx_train.cuda()\n",
    "#     idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.9864 f1_train: 0.1301 time: 0.2631s\n",
      "Epoch: 0002 loss_train: 2.2996 f1_train: 0.1462 time: 0.2377s\n",
      "Epoch: 0003 loss_train: 1.8138 f1_train: 0.1747 time: 0.2420s\n",
      "Epoch: 0004 loss_train: 1.5091 f1_train: 0.2004 time: 0.2239s\n",
      "Epoch: 0005 loss_train: 1.3594 f1_train: 0.2336 time: 0.2305s\n",
      "Epoch: 0006 loss_train: 1.2972 f1_train: 0.2551 time: 0.2213s\n",
      "Epoch: 0007 loss_train: 1.0842 f1_train: 0.3038 time: 0.2179s\n",
      "Epoch: 0008 loss_train: 1.0645 f1_train: 0.3214 time: 0.1938s\n",
      "Epoch: 0009 loss_train: 0.9673 f1_train: 0.3639 time: 0.2153s\n",
      "Epoch: 0010 loss_train: 0.9324 f1_train: 0.3805 time: 0.2257s\n",
      "Epoch: 0011 loss_train: 0.9233 f1_train: 0.3921 time: 0.2127s\n",
      "Epoch: 0012 loss_train: 0.8872 f1_train: 0.4134 time: 0.2282s\n",
      "Epoch: 0013 loss_train: 0.8575 f1_train: 0.4342 time: 0.2169s\n",
      "Epoch: 0014 loss_train: 0.8317 f1_train: 0.4576 time: 0.2178s\n",
      "Epoch: 0015 loss_train: 0.7878 f1_train: 0.4632 time: 0.2167s\n",
      "Epoch: 0016 loss_train: 0.7728 f1_train: 0.4803 time: 0.2219s\n",
      "Epoch: 0017 loss_train: 0.7717 f1_train: 0.4790 time: 0.2342s\n",
      "Epoch: 0018 loss_train: 0.7496 f1_train: 0.4857 time: 0.2572s\n",
      "Epoch: 0019 loss_train: 0.7273 f1_train: 0.4921 time: 0.2659s\n",
      "Epoch: 0020 loss_train: 0.7054 f1_train: 0.5082 time: 0.2486s\n",
      "Epoch: 0021 loss_train: 0.7092 f1_train: 0.5071 time: 0.2261s\n",
      "Epoch: 0022 loss_train: 0.7267 f1_train: 0.5001 time: 0.2405s\n",
      "Epoch: 0023 loss_train: 0.6701 f1_train: 0.5241 time: 0.2384s\n",
      "Epoch: 0024 loss_train: 0.6626 f1_train: 0.5256 time: 0.2165s\n",
      "Epoch: 0025 loss_train: 0.6541 f1_train: 0.5325 time: 0.2287s\n",
      "Epoch: 0026 loss_train: 0.6356 f1_train: 0.5364 time: 0.2488s\n",
      "Epoch: 0027 loss_train: 0.6158 f1_train: 0.5407 time: 0.2527s\n",
      "Epoch: 0028 loss_train: 0.6111 f1_train: 0.5436 time: 0.2430s\n",
      "Epoch: 0029 loss_train: 0.6119 f1_train: 0.5490 time: 0.2382s\n",
      "Epoch: 0030 loss_train: 0.6261 f1_train: 0.5356 time: 0.3424s\n",
      "Epoch: 0031 loss_train: 0.5784 f1_train: 0.5596 time: 0.3050s\n",
      "Epoch: 0032 loss_train: 0.5746 f1_train: 0.5571 time: 0.2628s\n",
      "Epoch: 0033 loss_train: 0.5933 f1_train: 0.5497 time: 0.1883s\n",
      "Epoch: 0034 loss_train: 0.5627 f1_train: 0.5620 time: 0.2193s\n",
      "Epoch: 0035 loss_train: 0.5509 f1_train: 0.5679 time: 0.2284s\n",
      "Epoch: 0036 loss_train: 0.5370 f1_train: 0.5777 time: 0.2329s\n",
      "Epoch: 0037 loss_train: 0.5437 f1_train: 0.5756 time: 0.2394s\n",
      "Epoch: 0038 loss_train: 0.5243 f1_train: 0.5754 time: 0.2258s\n",
      "Epoch: 0039 loss_train: 0.5237 f1_train: 0.5839 time: 0.2254s\n",
      "Epoch: 0040 loss_train: 0.5163 f1_train: 0.5778 time: 0.2566s\n",
      "Epoch: 0041 loss_train: 0.4981 f1_train: 0.5834 time: 0.2587s\n",
      "Epoch: 0042 loss_train: 0.4893 f1_train: 0.5899 time: 0.2291s\n",
      "Epoch: 0043 loss_train: 0.4910 f1_train: 0.5832 time: 0.2278s\n",
      "Epoch: 0044 loss_train: 0.4750 f1_train: 0.5893 time: 0.2565s\n",
      "Epoch: 0045 loss_train: 0.4676 f1_train: 0.5957 time: 0.2679s\n",
      "Epoch: 0046 loss_train: 0.4543 f1_train: 0.5985 time: 0.3080s\n",
      "Epoch: 0047 loss_train: 0.4797 f1_train: 0.5904 time: 0.3510s\n",
      "Epoch: 0048 loss_train: 0.4474 f1_train: 0.6013 time: 0.2897s\n",
      "Epoch: 0049 loss_train: 0.4385 f1_train: 0.5962 time: 0.2530s\n",
      "Epoch: 0050 loss_train: 0.4511 f1_train: 0.5984 time: 0.2413s\n",
      "Epoch: 0051 loss_train: 0.4332 f1_train: 0.6054 time: 0.2636s\n",
      "Epoch: 0052 loss_train: 0.4148 f1_train: 0.6132 time: 0.2481s\n",
      "Epoch: 0053 loss_train: 0.4200 f1_train: 0.6137 time: 0.2362s\n",
      "Epoch: 0054 loss_train: 0.4004 f1_train: 0.6250 time: 0.2487s\n",
      "Epoch: 0055 loss_train: 0.4082 f1_train: 0.6185 time: 0.2701s\n",
      "Epoch: 0056 loss_train: 0.3940 f1_train: 0.6218 time: 0.2744s\n",
      "Epoch: 0057 loss_train: 0.3872 f1_train: 0.6279 time: 0.3233s\n",
      "Epoch: 0058 loss_train: 0.3863 f1_train: 0.6331 time: 0.2481s\n",
      "Epoch: 0059 loss_train: 0.3846 f1_train: 0.6256 time: 0.2692s\n",
      "Epoch: 0060 loss_train: 0.3906 f1_train: 0.6212 time: 0.2658s\n",
      "Epoch: 0061 loss_train: 0.3670 f1_train: 0.6326 time: 0.3075s\n",
      "Epoch: 0062 loss_train: 0.3819 f1_train: 0.6354 time: 0.3630s\n",
      "Epoch: 0063 loss_train: 0.3689 f1_train: 0.6292 time: 0.3800s\n",
      "Epoch: 0064 loss_train: 0.3481 f1_train: 0.6441 time: 0.2682s\n",
      "Epoch: 0065 loss_train: 0.3743 f1_train: 0.6342 time: 0.2534s\n",
      "Epoch: 0066 loss_train: 0.3566 f1_train: 0.6444 time: 0.2033s\n",
      "Epoch: 0067 loss_train: 0.3488 f1_train: 0.6508 time: 0.2161s\n",
      "Epoch: 0068 loss_train: 0.3446 f1_train: 0.6453 time: 0.2425s\n",
      "Epoch: 0069 loss_train: 0.3410 f1_train: 0.6528 time: 0.2166s\n",
      "Epoch: 0070 loss_train: 0.3417 f1_train: 0.6554 time: 0.2469s\n",
      "Epoch: 0071 loss_train: 0.3326 f1_train: 0.6521 time: 0.2604s\n",
      "Epoch: 0072 loss_train: 0.3282 f1_train: 0.6625 time: 0.2127s\n",
      "Epoch: 0073 loss_train: 0.3235 f1_train: 0.6594 time: 0.1934s\n",
      "Epoch: 0074 loss_train: 0.3180 f1_train: 0.6700 time: 0.1780s\n",
      "Epoch: 0075 loss_train: 0.3112 f1_train: 0.6681 time: 0.1867s\n",
      "Epoch: 0076 loss_train: 0.3156 f1_train: 0.6690 time: 0.1896s\n",
      "Epoch: 0077 loss_train: 0.3089 f1_train: 0.6709 time: 0.2006s\n",
      "Epoch: 0078 loss_train: 0.2989 f1_train: 0.6754 time: 0.2081s\n",
      "Epoch: 0079 loss_train: 0.3020 f1_train: 0.6714 time: 0.1809s\n",
      "Epoch: 0080 loss_train: 0.2915 f1_train: 0.6779 time: 0.2270s\n",
      "Epoch: 0081 loss_train: 0.2967 f1_train: 0.6771 time: 0.2234s\n",
      "Epoch: 0082 loss_train: 0.2921 f1_train: 0.6836 time: 0.2489s\n",
      "Epoch: 0083 loss_train: 0.2893 f1_train: 0.6868 time: 0.2235s\n",
      "Epoch: 0084 loss_train: 0.2758 f1_train: 0.6921 time: 0.2327s\n",
      "Epoch: 0085 loss_train: 0.2814 f1_train: 0.6912 time: 0.2162s\n",
      "Epoch: 0086 loss_train: 0.2846 f1_train: 0.6801 time: 0.2183s\n",
      "Epoch: 0087 loss_train: 0.2712 f1_train: 0.6860 time: 0.2296s\n",
      "Epoch: 0088 loss_train: 0.2777 f1_train: 0.6933 time: 0.2344s\n",
      "Epoch: 0089 loss_train: 0.2813 f1_train: 0.6886 time: 0.2181s\n",
      "Epoch: 0090 loss_train: 0.2702 f1_train: 0.6917 time: 0.2312s\n",
      "Epoch: 0091 loss_train: 0.2690 f1_train: 0.7003 time: 0.2313s\n",
      "Epoch: 0092 loss_train: 0.2639 f1_train: 0.7001 time: 0.2206s\n",
      "Epoch: 0093 loss_train: 0.2543 f1_train: 0.7021 time: 0.2285s\n",
      "Epoch: 0094 loss_train: 0.2609 f1_train: 0.7005 time: 0.2262s\n",
      "Epoch: 0095 loss_train: 0.2638 f1_train: 0.7001 time: 0.2161s\n",
      "Epoch: 0096 loss_train: 0.2577 f1_train: 0.7019 time: 0.2227s\n",
      "Epoch: 0097 loss_train: 0.2556 f1_train: 0.7067 time: 0.2283s\n",
      "Epoch: 0098 loss_train: 0.2565 f1_train: 0.7076 time: 0.2198s\n",
      "Epoch: 0099 loss_train: 0.2516 f1_train: 0.7058 time: 0.2216s\n",
      "Epoch: 0100 loss_train: 0.2473 f1_train: 0.7166 time: 0.2242s\n",
      "Epoch: 0101 loss_train: 0.2444 f1_train: 0.7105 time: 0.2278s\n",
      "Epoch: 0102 loss_train: 0.2383 f1_train: 0.7229 time: 0.2386s\n",
      "Epoch: 0103 loss_train: 0.2417 f1_train: 0.7152 time: 0.2367s\n",
      "Epoch: 0104 loss_train: 0.2438 f1_train: 0.7128 time: 0.2252s\n",
      "Epoch: 0105 loss_train: 0.2380 f1_train: 0.7262 time: 0.2324s\n",
      "Epoch: 0106 loss_train: 0.2439 f1_train: 0.7224 time: 0.1899s\n",
      "Epoch: 0107 loss_train: 0.2361 f1_train: 0.7222 time: 0.2008s\n",
      "Epoch: 0108 loss_train: 0.2405 f1_train: 0.7181 time: 0.1854s\n",
      "Epoch: 0109 loss_train: 0.2329 f1_train: 0.7282 time: 0.1951s\n",
      "Epoch: 0110 loss_train: 0.2337 f1_train: 0.7250 time: 0.1791s\n",
      "Epoch: 0111 loss_train: 0.2318 f1_train: 0.7215 time: 0.1951s\n",
      "Epoch: 0112 loss_train: 0.2308 f1_train: 0.7287 time: 0.1748s\n",
      "Epoch: 0113 loss_train: 0.2287 f1_train: 0.7312 time: 0.2129s\n",
      "Epoch: 0114 loss_train: 0.2299 f1_train: 0.7294 time: 0.1791s\n",
      "Epoch: 0115 loss_train: 0.2222 f1_train: 0.7355 time: 0.1882s\n",
      "Epoch: 0116 loss_train: 0.2204 f1_train: 0.7369 time: 0.2366s\n",
      "Epoch: 0117 loss_train: 0.2151 f1_train: 0.7389 time: 0.3116s\n",
      "Epoch: 0118 loss_train: 0.2176 f1_train: 0.7381 time: 0.2299s\n",
      "Epoch: 0119 loss_train: 0.2215 f1_train: 0.7379 time: 0.1785s\n",
      "Epoch: 0120 loss_train: 0.2194 f1_train: 0.7378 time: 0.2027s\n",
      "Epoch: 0121 loss_train: 0.2197 f1_train: 0.7416 time: 0.1875s\n",
      "Epoch: 0122 loss_train: 0.2155 f1_train: 0.7401 time: 0.1962s\n",
      "Epoch: 0123 loss_train: 0.2206 f1_train: 0.7370 time: 0.1884s\n",
      "Epoch: 0124 loss_train: 0.2154 f1_train: 0.7493 time: 0.1813s\n",
      "Epoch: 0125 loss_train: 0.2133 f1_train: 0.7457 time: 0.1998s\n",
      "Epoch: 0126 loss_train: 0.2121 f1_train: 0.7439 time: 0.1961s\n",
      "Epoch: 0127 loss_train: 0.2113 f1_train: 0.7441 time: 0.1896s\n",
      "Epoch: 0128 loss_train: 0.2133 f1_train: 0.7438 time: 0.1794s\n",
      "Epoch: 0129 loss_train: 0.2071 f1_train: 0.7475 time: 0.1853s\n",
      "Epoch: 0130 loss_train: 0.2091 f1_train: 0.7503 time: 0.2000s\n",
      "Epoch: 0131 loss_train: 0.2112 f1_train: 0.7466 time: 0.1872s\n",
      "Epoch: 0132 loss_train: 0.2117 f1_train: 0.7513 time: 0.1895s\n",
      "Epoch: 0133 loss_train: 0.2059 f1_train: 0.7544 time: 0.1778s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0134 loss_train: 0.2069 f1_train: 0.7530 time: 0.2059s\n",
      "Epoch: 0135 loss_train: 0.2071 f1_train: 0.7527 time: 0.1892s\n",
      "Epoch: 0136 loss_train: 0.2069 f1_train: 0.7541 time: 0.2011s\n",
      "Epoch: 0137 loss_train: 0.2085 f1_train: 0.7536 time: 0.1813s\n",
      "Epoch: 0138 loss_train: 0.2036 f1_train: 0.7570 time: 0.1824s\n",
      "Epoch: 0139 loss_train: 0.2004 f1_train: 0.7546 time: 0.1799s\n",
      "Epoch: 0140 loss_train: 0.2056 f1_train: 0.7606 time: 0.1839s\n",
      "Epoch: 0141 loss_train: 0.2032 f1_train: 0.7587 time: 0.1743s\n",
      "Epoch: 0142 loss_train: 0.2020 f1_train: 0.7519 time: 0.1806s\n",
      "Epoch: 0143 loss_train: 0.2032 f1_train: 0.7556 time: 0.2137s\n",
      "Epoch: 0144 loss_train: 0.1986 f1_train: 0.7567 time: 0.2352s\n",
      "Epoch: 0145 loss_train: 0.1988 f1_train: 0.7559 time: 0.2206s\n",
      "Epoch: 0146 loss_train: 0.1947 f1_train: 0.7605 time: 0.2595s\n",
      "Epoch: 0147 loss_train: 0.2010 f1_train: 0.7568 time: 0.1797s\n",
      "Epoch: 0148 loss_train: 0.1974 f1_train: 0.7552 time: 0.2011s\n",
      "Epoch: 0149 loss_train: 0.1959 f1_train: 0.7604 time: 0.1937s\n",
      "Epoch: 0150 loss_train: 0.1955 f1_train: 0.7650 time: 0.1976s\n",
      "Epoch: 0151 loss_train: 0.1944 f1_train: 0.7651 time: 0.2309s\n",
      "Epoch: 0152 loss_train: 0.1942 f1_train: 0.7669 time: 0.2237s\n",
      "Epoch: 0153 loss_train: 0.1902 f1_train: 0.7678 time: 0.2401s\n",
      "Epoch: 0154 loss_train: 0.1916 f1_train: 0.7706 time: 0.2205s\n",
      "Epoch: 0155 loss_train: 0.1899 f1_train: 0.7671 time: 0.2221s\n",
      "Epoch: 0156 loss_train: 0.1853 f1_train: 0.7774 time: 0.2195s\n",
      "Epoch: 0157 loss_train: 0.1934 f1_train: 0.7683 time: 0.2281s\n",
      "Epoch: 0158 loss_train: 0.1863 f1_train: 0.7712 time: 0.2390s\n",
      "Epoch: 0159 loss_train: 0.1895 f1_train: 0.7720 time: 0.2515s\n",
      "Epoch: 0160 loss_train: 0.1923 f1_train: 0.7631 time: 0.2312s\n",
      "Epoch: 0161 loss_train: 0.1882 f1_train: 0.7747 time: 0.2009s\n",
      "Epoch: 0162 loss_train: 0.1894 f1_train: 0.7666 time: 0.1842s\n",
      "Epoch: 0163 loss_train: 0.1851 f1_train: 0.7755 time: 0.1987s\n",
      "Epoch: 0164 loss_train: 0.1871 f1_train: 0.7787 time: 0.2091s\n",
      "Epoch: 0165 loss_train: 0.1825 f1_train: 0.7819 time: 0.1869s\n",
      "Epoch: 0166 loss_train: 0.1905 f1_train: 0.7764 time: 0.1902s\n",
      "Epoch: 0167 loss_train: 0.1832 f1_train: 0.7821 time: 0.1760s\n",
      "Epoch: 0168 loss_train: 0.1868 f1_train: 0.7738 time: 0.1846s\n",
      "Epoch: 0169 loss_train: 0.1864 f1_train: 0.7779 time: 0.2134s\n",
      "Epoch: 0170 loss_train: 0.1854 f1_train: 0.7743 time: 0.2059s\n",
      "Epoch: 0171 loss_train: 0.1816 f1_train: 0.7797 time: 0.2431s\n",
      "Epoch: 0172 loss_train: 0.1825 f1_train: 0.7759 time: 0.2478s\n",
      "Epoch: 0173 loss_train: 0.1859 f1_train: 0.7785 time: 0.2578s\n",
      "Epoch: 0174 loss_train: 0.1817 f1_train: 0.7784 time: 0.2219s\n",
      "Epoch: 0175 loss_train: 0.1795 f1_train: 0.7811 time: 0.2115s\n",
      "Epoch: 0176 loss_train: 0.1793 f1_train: 0.7823 time: 0.2221s\n",
      "Epoch: 0177 loss_train: 0.1832 f1_train: 0.7764 time: 0.2630s\n",
      "Epoch: 0178 loss_train: 0.1826 f1_train: 0.7859 time: 0.2344s\n",
      "Epoch: 0179 loss_train: 0.1772 f1_train: 0.7810 time: 0.2168s\n",
      "Epoch: 0180 loss_train: 0.1829 f1_train: 0.7836 time: 0.2194s\n",
      "Epoch: 0181 loss_train: 0.1807 f1_train: 0.7844 time: 0.2346s\n",
      "Epoch: 0182 loss_train: 0.1782 f1_train: 0.7875 time: 0.2202s\n",
      "Epoch: 0183 loss_train: 0.1735 f1_train: 0.7877 time: 0.2228s\n",
      "Epoch: 0184 loss_train: 0.1789 f1_train: 0.7883 time: 0.2311s\n",
      "Epoch: 0185 loss_train: 0.1743 f1_train: 0.7893 time: 0.2323s\n",
      "Epoch: 0186 loss_train: 0.1738 f1_train: 0.7840 time: 0.2209s\n",
      "Epoch: 0187 loss_train: 0.1761 f1_train: 0.7898 time: 0.2695s\n",
      "Epoch: 0188 loss_train: 0.1799 f1_train: 0.7860 time: 0.2310s\n",
      "Epoch: 0189 loss_train: 0.1744 f1_train: 0.7865 time: 0.2725s\n",
      "Epoch: 0190 loss_train: 0.1771 f1_train: 0.7863 time: 0.2347s\n",
      "Epoch: 0191 loss_train: 0.1784 f1_train: 0.7813 time: 0.2222s\n",
      "Epoch: 0192 loss_train: 0.1744 f1_train: 0.7957 time: 0.2321s\n",
      "Epoch: 0193 loss_train: 0.1746 f1_train: 0.7861 time: 0.2151s\n",
      "Epoch: 0194 loss_train: 0.1751 f1_train: 0.7848 time: 0.2216s\n",
      "Epoch: 0195 loss_train: 0.1749 f1_train: 0.7879 time: 0.2172s\n",
      "Epoch: 0196 loss_train: 0.1782 f1_train: 0.7848 time: 0.2248s\n",
      "Epoch: 0197 loss_train: 0.1720 f1_train: 0.7913 time: 0.2615s\n",
      "Epoch: 0198 loss_train: 0.1733 f1_train: 0.7910 time: 0.2807s\n",
      "Epoch: 0199 loss_train: 0.1731 f1_train: 0.7865 time: 0.2543s\n",
      "Epoch: 0200 loss_train: 0.1687 f1_train: 0.7971 time: 0.2737s\n",
      "Epoch: 0201 loss_train: 0.1741 f1_train: 0.7922 time: 0.2384s\n",
      "Epoch: 0202 loss_train: 0.1724 f1_train: 0.7955 time: 0.2151s\n",
      "Epoch: 0203 loss_train: 0.1693 f1_train: 0.7931 time: 0.2917s\n",
      "Epoch: 0204 loss_train: 0.1702 f1_train: 0.7994 time: 0.2158s\n",
      "Epoch: 0205 loss_train: 0.1726 f1_train: 0.7948 time: 0.1843s\n",
      "Epoch: 0206 loss_train: 0.1696 f1_train: 0.7865 time: 0.1950s\n",
      "Epoch: 0207 loss_train: 0.1685 f1_train: 0.7924 time: 0.2234s\n",
      "Epoch: 0208 loss_train: 0.1710 f1_train: 0.7929 time: 0.2147s\n",
      "Epoch: 0209 loss_train: 0.1712 f1_train: 0.7912 time: 0.2200s\n",
      "Epoch: 0210 loss_train: 0.1693 f1_train: 0.7972 time: 0.2196s\n",
      "Epoch: 0211 loss_train: 0.1707 f1_train: 0.7959 time: 0.2398s\n",
      "Epoch: 0212 loss_train: 0.1678 f1_train: 0.8015 time: 0.2266s\n",
      "Epoch: 0213 loss_train: 0.1678 f1_train: 0.8005 time: 0.2191s\n",
      "Epoch: 0214 loss_train: 0.1667 f1_train: 0.7938 time: 0.2286s\n",
      "Epoch: 0215 loss_train: 0.1696 f1_train: 0.7949 time: 0.2440s\n",
      "Epoch: 0216 loss_train: 0.1638 f1_train: 0.8035 time: 0.2249s\n",
      "Epoch: 0217 loss_train: 0.1665 f1_train: 0.8015 time: 0.2155s\n",
      "Epoch: 0218 loss_train: 0.1638 f1_train: 0.7972 time: 0.2574s\n",
      "Epoch: 0219 loss_train: 0.1660 f1_train: 0.8014 time: 0.2562s\n",
      "Epoch: 0220 loss_train: 0.1703 f1_train: 0.7949 time: 0.2031s\n",
      "Epoch: 0221 loss_train: 0.1626 f1_train: 0.8012 time: 0.1970s\n",
      "Epoch: 0222 loss_train: 0.1693 f1_train: 0.7949 time: 0.1819s\n",
      "Epoch: 0223 loss_train: 0.1707 f1_train: 0.7943 time: 0.1894s\n",
      "Epoch: 0224 loss_train: 0.1655 f1_train: 0.7959 time: 0.2284s\n",
      "Epoch: 0225 loss_train: 0.1680 f1_train: 0.7979 time: 0.2113s\n",
      "Epoch: 0226 loss_train: 0.1621 f1_train: 0.8042 time: 0.2138s\n",
      "Epoch: 0227 loss_train: 0.1648 f1_train: 0.8018 time: 0.2430s\n",
      "Epoch: 0228 loss_train: 0.1656 f1_train: 0.8015 time: 0.2109s\n",
      "Epoch: 0229 loss_train: 0.1612 f1_train: 0.8098 time: 0.2304s\n",
      "Epoch: 0230 loss_train: 0.1644 f1_train: 0.8053 time: 0.2579s\n",
      "Epoch: 0231 loss_train: 0.1627 f1_train: 0.8008 time: 0.2131s\n",
      "Epoch: 0232 loss_train: 0.1624 f1_train: 0.8019 time: 0.2359s\n",
      "Epoch: 0233 loss_train: 0.1668 f1_train: 0.8012 time: 0.2211s\n",
      "Epoch: 0234 loss_train: 0.1614 f1_train: 0.8054 time: 0.2192s\n",
      "Epoch: 0235 loss_train: 0.1638 f1_train: 0.8083 time: 0.2137s\n",
      "Epoch: 0236 loss_train: 0.1639 f1_train: 0.8028 time: 0.2170s\n",
      "Epoch: 0237 loss_train: 0.1602 f1_train: 0.8050 time: 0.2242s\n",
      "Epoch: 0238 loss_train: 0.1622 f1_train: 0.8072 time: 0.2249s\n",
      "Epoch: 0239 loss_train: 0.1602 f1_train: 0.8094 time: 0.2167s\n",
      "Epoch: 0240 loss_train: 0.1621 f1_train: 0.8061 time: 0.2114s\n",
      "Epoch: 0241 loss_train: 0.1616 f1_train: 0.8046 time: 0.2151s\n",
      "Epoch: 0242 loss_train: 0.1650 f1_train: 0.7998 time: 0.2237s\n",
      "Epoch: 0243 loss_train: 0.1584 f1_train: 0.8049 time: 0.2306s\n",
      "Epoch: 0244 loss_train: 0.1604 f1_train: 0.8038 time: 0.2129s\n",
      "Epoch: 0245 loss_train: 0.1627 f1_train: 0.8022 time: 0.2188s\n",
      "Epoch: 0246 loss_train: 0.1641 f1_train: 0.8020 time: 0.2202s\n",
      "Epoch: 0247 loss_train: 0.1615 f1_train: 0.8095 time: 0.2150s\n",
      "Epoch: 0248 loss_train: 0.1603 f1_train: 0.8041 time: 0.2159s\n",
      "Epoch: 0249 loss_train: 0.1587 f1_train: 0.8106 time: 0.2293s\n",
      "Epoch: 0250 loss_train: 0.1585 f1_train: 0.8107 time: 0.2172s\n",
      "Epoch: 0251 loss_train: 0.1566 f1_train: 0.8092 time: 0.2007s\n",
      "Epoch: 0252 loss_train: 0.1620 f1_train: 0.8079 time: 0.1885s\n",
      "Epoch: 0253 loss_train: 0.1579 f1_train: 0.8104 time: 0.1815s\n",
      "Epoch: 0254 loss_train: 0.1581 f1_train: 0.8017 time: 0.1791s\n",
      "Epoch: 0255 loss_train: 0.1576 f1_train: 0.8126 time: 0.1799s\n",
      "Epoch: 0256 loss_train: 0.1538 f1_train: 0.8186 time: 0.1754s\n",
      "Epoch: 0257 loss_train: 0.1581 f1_train: 0.8106 time: 0.1832s\n",
      "Epoch: 0258 loss_train: 0.1548 f1_train: 0.8096 time: 0.1734s\n",
      "Epoch: 0259 loss_train: 0.1591 f1_train: 0.8108 time: 0.1792s\n",
      "Epoch: 0260 loss_train: 0.1596 f1_train: 0.8094 time: 0.1739s\n",
      "Epoch: 0261 loss_train: 0.1558 f1_train: 0.8132 time: 0.1810s\n",
      "Epoch: 0262 loss_train: 0.1564 f1_train: 0.8140 time: 0.2166s\n",
      "Epoch: 0263 loss_train: 0.1554 f1_train: 0.8145 time: 0.2151s\n",
      "Epoch: 0264 loss_train: 0.1542 f1_train: 0.8130 time: 0.2121s\n",
      "Epoch: 0265 loss_train: 0.1562 f1_train: 0.8101 time: 0.2178s\n",
      "Epoch: 0266 loss_train: 0.1542 f1_train: 0.8112 time: 0.2120s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0267 loss_train: 0.1562 f1_train: 0.8090 time: 0.2168s\n",
      "Epoch: 0268 loss_train: 0.1513 f1_train: 0.8190 time: 0.2237s\n",
      "Epoch: 0269 loss_train: 0.1538 f1_train: 0.8166 time: 0.2216s\n",
      "Epoch: 0270 loss_train: 0.1548 f1_train: 0.8169 time: 0.2178s\n",
      "Epoch: 0271 loss_train: 0.1543 f1_train: 0.8195 time: 0.2206s\n",
      "Epoch: 0272 loss_train: 0.1519 f1_train: 0.8177 time: 0.2124s\n",
      "Epoch: 0273 loss_train: 0.1512 f1_train: 0.8203 time: 0.2149s\n",
      "Epoch: 0274 loss_train: 0.1537 f1_train: 0.8096 time: 0.2100s\n",
      "Epoch: 0275 loss_train: 0.1534 f1_train: 0.8143 time: 0.2117s\n",
      "Epoch: 0276 loss_train: 0.1523 f1_train: 0.8148 time: 0.2151s\n",
      "Epoch: 0277 loss_train: 0.1524 f1_train: 0.8127 time: 0.2124s\n",
      "Epoch: 0278 loss_train: 0.1531 f1_train: 0.8092 time: 0.2184s\n",
      "Epoch: 0279 loss_train: 0.1535 f1_train: 0.8183 time: 0.2225s\n",
      "Epoch: 0280 loss_train: 0.1542 f1_train: 0.8147 time: 0.2252s\n",
      "Epoch: 0281 loss_train: 0.1542 f1_train: 0.8151 time: 0.2190s\n",
      "Epoch: 0282 loss_train: 0.1511 f1_train: 0.8161 time: 0.2164s\n",
      "Epoch: 0283 loss_train: 0.1497 f1_train: 0.8195 time: 0.2156s\n",
      "Epoch: 0284 loss_train: 0.1519 f1_train: 0.8229 time: 0.2165s\n",
      "Epoch: 0285 loss_train: 0.1514 f1_train: 0.8175 time: 0.2132s\n",
      "Epoch: 0286 loss_train: 0.1531 f1_train: 0.8164 time: 0.2120s\n",
      "Epoch: 0287 loss_train: 0.1527 f1_train: 0.8175 time: 0.2180s\n",
      "Epoch: 0288 loss_train: 0.1508 f1_train: 0.8190 time: 0.2224s\n",
      "Epoch: 0289 loss_train: 0.1538 f1_train: 0.8163 time: 0.2187s\n",
      "Epoch: 0290 loss_train: 0.1505 f1_train: 0.8148 time: 0.2078s\n",
      "Epoch: 0291 loss_train: 0.1507 f1_train: 0.8161 time: 0.2145s\n",
      "Epoch: 0292 loss_train: 0.1516 f1_train: 0.8173 time: 0.2091s\n",
      "Epoch: 0293 loss_train: 0.1535 f1_train: 0.8195 time: 0.2114s\n",
      "Epoch: 0294 loss_train: 0.1495 f1_train: 0.8189 time: 0.2199s\n",
      "Epoch: 0295 loss_train: 0.1507 f1_train: 0.8187 time: 0.2151s\n",
      "Epoch: 0296 loss_train: 0.1515 f1_train: 0.8213 time: 0.2227s\n",
      "Epoch: 0297 loss_train: 0.1479 f1_train: 0.8237 time: 0.2199s\n",
      "Epoch: 0298 loss_train: 0.1495 f1_train: 0.8204 time: 0.2117s\n",
      "Epoch: 0299 loss_train: 0.1499 f1_train: 0.8232 time: 0.2113s\n",
      "Epoch: 0300 loss_train: 0.1473 f1_train: 0.8227 time: 0.2193s\n",
      "Epoch: 0301 loss_train: 0.1479 f1_train: 0.8154 time: 0.2161s\n",
      "Epoch: 0302 loss_train: 0.1477 f1_train: 0.8243 time: 0.2195s\n",
      "Epoch: 0303 loss_train: 0.1445 f1_train: 0.8245 time: 0.2193s\n",
      "Epoch: 0304 loss_train: 0.1502 f1_train: 0.8197 time: 0.2208s\n",
      "Epoch: 0305 loss_train: 0.1488 f1_train: 0.8212 time: 0.2206s\n",
      "Epoch: 0306 loss_train: 0.1476 f1_train: 0.8200 time: 0.2187s\n",
      "Epoch: 0307 loss_train: 0.1520 f1_train: 0.8163 time: 0.2151s\n",
      "Epoch: 0308 loss_train: 0.1497 f1_train: 0.8218 time: 0.2142s\n",
      "Epoch: 0309 loss_train: 0.1451 f1_train: 0.8202 time: 0.2183s\n",
      "Epoch: 0310 loss_train: 0.1485 f1_train: 0.8197 time: 0.2203s\n",
      "Epoch: 0311 loss_train: 0.1488 f1_train: 0.8188 time: 0.2108s\n",
      "Epoch: 0312 loss_train: 0.1457 f1_train: 0.8294 time: 0.2106s\n",
      "Epoch: 0313 loss_train: 0.1491 f1_train: 0.8228 time: 0.2203s\n",
      "Epoch: 0314 loss_train: 0.1471 f1_train: 0.8252 time: 0.2244s\n",
      "Epoch: 0315 loss_train: 0.1487 f1_train: 0.8226 time: 0.2213s\n",
      "Epoch: 0316 loss_train: 0.1463 f1_train: 0.8266 time: 0.2173s\n",
      "Epoch: 0317 loss_train: 0.1471 f1_train: 0.8265 time: 0.1817s\n",
      "Epoch: 0318 loss_train: 0.1465 f1_train: 0.8290 time: 0.1779s\n",
      "Epoch: 0319 loss_train: 0.1453 f1_train: 0.8273 time: 0.1741s\n",
      "Epoch: 0320 loss_train: 0.1466 f1_train: 0.8270 time: 0.1793s\n",
      "Epoch: 0321 loss_train: 0.1451 f1_train: 0.8265 time: 0.1759s\n",
      "Epoch: 0322 loss_train: 0.1468 f1_train: 0.8241 time: 0.1800s\n",
      "Epoch: 0323 loss_train: 0.1431 f1_train: 0.8259 time: 0.1735s\n",
      "Epoch: 0324 loss_train: 0.1487 f1_train: 0.8231 time: 0.1810s\n",
      "Epoch: 0325 loss_train: 0.1453 f1_train: 0.8307 time: 0.1880s\n",
      "Epoch: 0326 loss_train: 0.1456 f1_train: 0.8301 time: 0.1756s\n",
      "Epoch: 0327 loss_train: 0.1467 f1_train: 0.8237 time: 0.1971s\n",
      "Epoch: 0328 loss_train: 0.1453 f1_train: 0.8221 time: 0.1757s\n",
      "Epoch: 0329 loss_train: 0.1451 f1_train: 0.8271 time: 0.1734s\n",
      "Epoch: 0330 loss_train: 0.1460 f1_train: 0.8269 time: 0.1780s\n",
      "Epoch: 0331 loss_train: 0.1447 f1_train: 0.8266 time: 0.1815s\n",
      "Epoch: 0332 loss_train: 0.1437 f1_train: 0.8281 time: 0.1783s\n",
      "Epoch: 0333 loss_train: 0.1418 f1_train: 0.8318 time: 0.1748s\n",
      "Epoch: 0334 loss_train: 0.1440 f1_train: 0.8233 time: 0.1951s\n",
      "Epoch: 0335 loss_train: 0.1436 f1_train: 0.8299 time: 0.1758s\n",
      "Epoch: 0336 loss_train: 0.1433 f1_train: 0.8276 time: 0.1991s\n",
      "Epoch: 0337 loss_train: 0.1436 f1_train: 0.8316 time: 0.1733s\n",
      "Epoch: 0338 loss_train: 0.1453 f1_train: 0.8248 time: 0.2086s\n",
      "Epoch: 0339 loss_train: 0.1411 f1_train: 0.8304 time: 0.2091s\n",
      "Epoch: 0340 loss_train: 0.1401 f1_train: 0.8288 time: 0.1743s\n",
      "Epoch: 0341 loss_train: 0.1433 f1_train: 0.8277 time: 0.1798s\n",
      "Epoch: 0342 loss_train: 0.1426 f1_train: 0.8243 time: 0.1796s\n",
      "Epoch: 0343 loss_train: 0.1405 f1_train: 0.8331 time: 0.1797s\n",
      "Epoch: 0344 loss_train: 0.1404 f1_train: 0.8311 time: 0.1758s\n",
      "Epoch: 0345 loss_train: 0.1427 f1_train: 0.8299 time: 0.1790s\n",
      "Epoch: 0346 loss_train: 0.1406 f1_train: 0.8326 time: 0.1741s\n",
      "Epoch: 0347 loss_train: 0.1417 f1_train: 0.8312 time: 0.1917s\n",
      "Epoch: 0348 loss_train: 0.1403 f1_train: 0.8315 time: 0.1772s\n",
      "Epoch: 0349 loss_train: 0.1438 f1_train: 0.8267 time: 0.2264s\n",
      "Epoch: 0350 loss_train: 0.1443 f1_train: 0.8251 time: 0.2156s\n",
      "Epoch: 0351 loss_train: 0.1397 f1_train: 0.8301 time: 0.2189s\n",
      "Epoch: 0352 loss_train: 0.1425 f1_train: 0.8248 time: 0.2190s\n",
      "Epoch: 0353 loss_train: 0.1411 f1_train: 0.8274 time: 0.2138s\n",
      "Epoch: 0354 loss_train: 0.1417 f1_train: 0.8300 time: 0.2292s\n",
      "Epoch: 0355 loss_train: 0.1391 f1_train: 0.8295 time: 0.1816s\n",
      "Epoch: 0356 loss_train: 0.1435 f1_train: 0.8304 time: 0.1933s\n",
      "Epoch: 0357 loss_train: 0.1406 f1_train: 0.8337 time: 0.1999s\n",
      "Epoch: 0358 loss_train: 0.1376 f1_train: 0.8363 time: 0.1810s\n",
      "Epoch: 0359 loss_train: 0.1390 f1_train: 0.8366 time: 0.1801s\n",
      "Epoch: 0360 loss_train: 0.1374 f1_train: 0.8352 time: 0.1728s\n",
      "Epoch: 0361 loss_train: 0.1406 f1_train: 0.8314 time: 0.1874s\n",
      "Epoch: 0362 loss_train: 0.1384 f1_train: 0.8366 time: 0.1772s\n",
      "Epoch: 0363 loss_train: 0.1400 f1_train: 0.8324 time: 0.1798s\n",
      "Epoch: 0364 loss_train: 0.1388 f1_train: 0.8321 time: 0.1894s\n",
      "Epoch: 0365 loss_train: 0.1416 f1_train: 0.8269 time: 0.2029s\n",
      "Epoch: 0366 loss_train: 0.1402 f1_train: 0.8352 time: 0.1741s\n",
      "Epoch: 0367 loss_train: 0.1377 f1_train: 0.8299 time: 0.1794s\n",
      "Epoch: 0368 loss_train: 0.1398 f1_train: 0.8319 time: 0.1777s\n",
      "Epoch: 0369 loss_train: 0.1361 f1_train: 0.8327 time: 0.2195s\n",
      "Epoch: 0370 loss_train: 0.1386 f1_train: 0.8391 time: 0.2118s\n",
      "Epoch: 0371 loss_train: 0.1368 f1_train: 0.8370 time: 0.2151s\n",
      "Epoch: 0372 loss_train: 0.1394 f1_train: 0.8336 time: 0.2115s\n",
      "Epoch: 0373 loss_train: 0.1363 f1_train: 0.8350 time: 0.2136s\n",
      "Epoch: 0374 loss_train: 0.1359 f1_train: 0.8331 time: 0.2173s\n",
      "Epoch: 0375 loss_train: 0.1379 f1_train: 0.8337 time: 0.2168s\n",
      "Epoch: 0376 loss_train: 0.1391 f1_train: 0.8324 time: 0.2122s\n",
      "Epoch: 0377 loss_train: 0.1373 f1_train: 0.8309 time: 0.2117s\n",
      "Epoch: 0378 loss_train: 0.1360 f1_train: 0.8371 time: 0.2202s\n",
      "Epoch: 0379 loss_train: 0.1366 f1_train: 0.8394 time: 0.2167s\n",
      "Epoch: 0380 loss_train: 0.1400 f1_train: 0.8306 time: 0.2156s\n",
      "Epoch: 0381 loss_train: 0.1372 f1_train: 0.8340 time: 0.2194s\n",
      "Epoch: 0382 loss_train: 0.1386 f1_train: 0.8355 time: 0.2220s\n",
      "Epoch: 0383 loss_train: 0.1361 f1_train: 0.8350 time: 0.2158s\n",
      "Epoch: 0384 loss_train: 0.1356 f1_train: 0.8347 time: 0.2155s\n",
      "Epoch: 0385 loss_train: 0.1382 f1_train: 0.8302 time: 0.2168s\n",
      "Epoch: 0386 loss_train: 0.1362 f1_train: 0.8349 time: 0.2174s\n",
      "Epoch: 0387 loss_train: 0.1363 f1_train: 0.8356 time: 0.2191s\n",
      "Epoch: 0388 loss_train: 0.1371 f1_train: 0.8308 time: 0.2155s\n",
      "Epoch: 0389 loss_train: 0.1358 f1_train: 0.8373 time: 0.2103s\n",
      "Epoch: 0390 loss_train: 0.1358 f1_train: 0.8364 time: 0.2144s\n",
      "Epoch: 0391 loss_train: 0.1340 f1_train: 0.8395 time: 0.2151s\n",
      "Epoch: 0392 loss_train: 0.1336 f1_train: 0.8376 time: 0.2165s\n",
      "Epoch: 0393 loss_train: 0.1361 f1_train: 0.8358 time: 0.2091s\n",
      "Epoch: 0394 loss_train: 0.1392 f1_train: 0.8345 time: 0.2097s\n",
      "Epoch: 0395 loss_train: 0.1366 f1_train: 0.8372 time: 0.2257s\n",
      "Epoch: 0396 loss_train: 0.1322 f1_train: 0.8402 time: 0.2085s\n",
      "Epoch: 0397 loss_train: 0.1326 f1_train: 0.8428 time: 0.2299s\n",
      "Epoch: 0398 loss_train: 0.1345 f1_train: 0.8419 time: 0.2167s\n",
      "Epoch: 0399 loss_train: 0.1348 f1_train: 0.8414 time: 0.2128s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0400 loss_train: 0.1353 f1_train: 0.8324 time: 0.2164s\n",
      "Epoch: 0401 loss_train: 0.1331 f1_train: 0.8408 time: 0.2079s\n",
      "Epoch: 0402 loss_train: 0.1359 f1_train: 0.8367 time: 0.2103s\n",
      "Epoch: 0403 loss_train: 0.1327 f1_train: 0.8399 time: 0.2146s\n",
      "Epoch: 0404 loss_train: 0.1336 f1_train: 0.8403 time: 0.2147s\n",
      "Epoch: 0405 loss_train: 0.1329 f1_train: 0.8424 time: 0.2217s\n",
      "Epoch: 0406 loss_train: 0.1328 f1_train: 0.8418 time: 0.2102s\n",
      "Epoch: 0407 loss_train: 0.1306 f1_train: 0.8412 time: 0.2088s\n",
      "Epoch: 0408 loss_train: 0.1307 f1_train: 0.8419 time: 0.2160s\n",
      "Epoch: 0409 loss_train: 0.1349 f1_train: 0.8383 time: 0.2249s\n",
      "Epoch: 0410 loss_train: 0.1315 f1_train: 0.8427 time: 0.2510s\n",
      "Epoch: 0411 loss_train: 0.1328 f1_train: 0.8364 time: 0.2294s\n",
      "Epoch: 0412 loss_train: 0.1333 f1_train: 0.8408 time: 0.2521s\n",
      "Epoch: 0413 loss_train: 0.1315 f1_train: 0.8388 time: 0.2383s\n",
      "Epoch: 0414 loss_train: 0.1315 f1_train: 0.8420 time: 0.2308s\n",
      "Epoch: 0415 loss_train: 0.1360 f1_train: 0.8347 time: 0.2265s\n",
      "Epoch: 0416 loss_train: 0.1318 f1_train: 0.8429 time: 0.2268s\n",
      "Epoch: 0417 loss_train: 0.1319 f1_train: 0.8384 time: 0.2316s\n",
      "Epoch: 0418 loss_train: 0.1314 f1_train: 0.8380 time: 0.2474s\n",
      "Epoch: 0419 loss_train: 0.1303 f1_train: 0.8416 time: 0.2360s\n",
      "Epoch: 0420 loss_train: 0.1330 f1_train: 0.8414 time: 0.2269s\n",
      "Epoch: 0421 loss_train: 0.1323 f1_train: 0.8389 time: 0.2310s\n",
      "Epoch: 0422 loss_train: 0.1305 f1_train: 0.8410 time: 0.2298s\n",
      "Epoch: 0423 loss_train: 0.1303 f1_train: 0.8439 time: 0.2273s\n",
      "Epoch: 0424 loss_train: 0.1325 f1_train: 0.8402 time: 0.2278s\n",
      "Epoch: 0425 loss_train: 0.1315 f1_train: 0.8428 time: 0.2282s\n",
      "Epoch: 0426 loss_train: 0.1330 f1_train: 0.8427 time: 0.2277s\n",
      "Epoch: 0427 loss_train: 0.1316 f1_train: 0.8432 time: 0.2301s\n",
      "Epoch: 0428 loss_train: 0.1300 f1_train: 0.8435 time: 0.2258s\n",
      "Epoch: 0429 loss_train: 0.1305 f1_train: 0.8428 time: 0.2288s\n",
      "Epoch: 0430 loss_train: 0.1302 f1_train: 0.8431 time: 0.2257s\n",
      "Epoch: 0431 loss_train: 0.1322 f1_train: 0.8436 time: 0.2369s\n",
      "Epoch: 0432 loss_train: 0.1305 f1_train: 0.8398 time: 0.2286s\n",
      "Epoch: 0433 loss_train: 0.1301 f1_train: 0.8440 time: 0.2393s\n",
      "Epoch: 0434 loss_train: 0.1303 f1_train: 0.8430 time: 0.2313s\n",
      "Epoch: 0435 loss_train: 0.1301 f1_train: 0.8419 time: 0.2356s\n",
      "Epoch: 0436 loss_train: 0.1321 f1_train: 0.8431 time: 0.2358s\n",
      "Epoch: 0437 loss_train: 0.1338 f1_train: 0.8399 time: 0.2456s\n",
      "Epoch: 0438 loss_train: 0.1316 f1_train: 0.8443 time: 0.2330s\n",
      "Epoch: 0439 loss_train: 0.1306 f1_train: 0.8500 time: 0.2317s\n",
      "Epoch: 0440 loss_train: 0.1282 f1_train: 0.8451 time: 0.2328s\n",
      "Epoch: 0441 loss_train: 0.1299 f1_train: 0.8429 time: 0.2470s\n",
      "Epoch: 0442 loss_train: 0.1301 f1_train: 0.8430 time: 0.2261s\n",
      "Epoch: 0443 loss_train: 0.1307 f1_train: 0.8425 time: 0.2277s\n",
      "Epoch: 0444 loss_train: 0.1326 f1_train: 0.8445 time: 0.2337s\n",
      "Epoch: 0445 loss_train: 0.1291 f1_train: 0.8477 time: 0.2279s\n",
      "Epoch: 0446 loss_train: 0.1306 f1_train: 0.8391 time: 0.2326s\n",
      "Epoch: 0447 loss_train: 0.1301 f1_train: 0.8454 time: 0.2277s\n",
      "Epoch: 0448 loss_train: 0.1282 f1_train: 0.8450 time: 0.2040s\n",
      "Epoch: 0449 loss_train: 0.1289 f1_train: 0.8428 time: 0.2343s\n",
      "Epoch: 0450 loss_train: 0.1290 f1_train: 0.8436 time: 0.2324s\n",
      "Epoch: 0451 loss_train: 0.1281 f1_train: 0.8451 time: 0.2264s\n",
      "Epoch: 0452 loss_train: 0.1281 f1_train: 0.8433 time: 0.2326s\n",
      "Epoch: 0453 loss_train: 0.1294 f1_train: 0.8458 time: 0.2317s\n",
      "Epoch: 0454 loss_train: 0.1290 f1_train: 0.8442 time: 0.2325s\n",
      "Epoch: 0455 loss_train: 0.1297 f1_train: 0.8459 time: 0.2299s\n",
      "Epoch: 0456 loss_train: 0.1281 f1_train: 0.8455 time: 0.2330s\n",
      "Epoch: 0457 loss_train: 0.1262 f1_train: 0.8475 time: 0.2265s\n",
      "Epoch: 0458 loss_train: 0.1279 f1_train: 0.8465 time: 0.2360s\n",
      "Epoch: 0459 loss_train: 0.1276 f1_train: 0.8509 time: 0.2339s\n",
      "Epoch: 0460 loss_train: 0.1289 f1_train: 0.8483 time: 0.2317s\n",
      "Epoch: 0461 loss_train: 0.1273 f1_train: 0.8450 time: 0.2331s\n",
      "Epoch: 0462 loss_train: 0.1281 f1_train: 0.8435 time: 0.2318s\n",
      "Epoch: 0463 loss_train: 0.1272 f1_train: 0.8462 time: 0.2287s\n",
      "Epoch: 0464 loss_train: 0.1289 f1_train: 0.8433 time: 0.2476s\n",
      "Epoch: 0465 loss_train: 0.1280 f1_train: 0.8479 time: 0.2262s\n",
      "Epoch: 0466 loss_train: 0.1269 f1_train: 0.8463 time: 0.2472s\n",
      "Epoch: 0467 loss_train: 0.1291 f1_train: 0.8460 time: 0.2322s\n",
      "Epoch: 0468 loss_train: 0.1306 f1_train: 0.8440 time: 0.2414s\n",
      "Epoch: 0469 loss_train: 0.1255 f1_train: 0.8453 time: 0.2312s\n",
      "Epoch: 0470 loss_train: 0.1260 f1_train: 0.8453 time: 0.2320s\n",
      "Epoch: 0471 loss_train: 0.1241 f1_train: 0.8513 time: 0.2315s\n",
      "Epoch: 0472 loss_train: 0.1277 f1_train: 0.8460 time: 0.2316s\n",
      "Epoch: 0473 loss_train: 0.1262 f1_train: 0.8472 time: 0.2394s\n",
      "Epoch: 0474 loss_train: 0.1274 f1_train: 0.8441 time: 0.2289s\n",
      "Epoch: 0475 loss_train: 0.1260 f1_train: 0.8526 time: 0.2349s\n",
      "Epoch: 0476 loss_train: 0.1290 f1_train: 0.8441 time: 0.2327s\n",
      "Epoch: 0477 loss_train: 0.1235 f1_train: 0.8549 time: 0.2314s\n",
      "Epoch: 0478 loss_train: 0.1251 f1_train: 0.8505 time: 0.2419s\n",
      "Epoch: 0479 loss_train: 0.1249 f1_train: 0.8530 time: 0.2291s\n",
      "Epoch: 0480 loss_train: 0.1266 f1_train: 0.8454 time: 0.2530s\n",
      "Epoch: 0481 loss_train: 0.1274 f1_train: 0.8424 time: 0.2279s\n",
      "Epoch: 0482 loss_train: 0.1271 f1_train: 0.8461 time: 0.2361s\n",
      "Epoch: 0483 loss_train: 0.1252 f1_train: 0.8484 time: 0.2292s\n",
      "Epoch: 0484 loss_train: 0.1270 f1_train: 0.8464 time: 0.2462s\n",
      "Epoch: 0485 loss_train: 0.1265 f1_train: 0.8486 time: 0.2333s\n",
      "Epoch: 0486 loss_train: 0.1282 f1_train: 0.8471 time: 0.2515s\n",
      "Epoch: 0487 loss_train: 0.1260 f1_train: 0.8506 time: 0.2330s\n",
      "Epoch: 0488 loss_train: 0.1265 f1_train: 0.8489 time: 0.2551s\n",
      "Epoch: 0489 loss_train: 0.1256 f1_train: 0.8511 time: 0.2446s\n",
      "Epoch: 0490 loss_train: 0.1250 f1_train: 0.8481 time: 0.2333s\n",
      "Epoch: 0491 loss_train: 0.1247 f1_train: 0.8527 time: 0.2336s\n",
      "Epoch: 0492 loss_train: 0.1265 f1_train: 0.8476 time: 0.2334s\n",
      "Epoch: 0493 loss_train: 0.1253 f1_train: 0.8487 time: 0.2379s\n",
      "Epoch: 0494 loss_train: 0.1249 f1_train: 0.8493 time: 0.2323s\n",
      "Epoch: 0495 loss_train: 0.1237 f1_train: 0.8511 time: 0.2336s\n",
      "Epoch: 0496 loss_train: 0.1263 f1_train: 0.8483 time: 0.2329s\n",
      "Epoch: 0497 loss_train: 0.1228 f1_train: 0.8521 time: 0.2307s\n",
      "Epoch: 0498 loss_train: 0.1230 f1_train: 0.8513 time: 0.2390s\n",
      "Epoch: 0499 loss_train: 0.1245 f1_train: 0.8501 time: 0.2342s\n",
      "Epoch: 0500 loss_train: 0.1194 f1_train: 0.8563 time: 0.2349s\n",
      "Epoch: 0501 loss_train: 0.1241 f1_train: 0.8540 time: 0.2336s\n",
      "Epoch: 0502 loss_train: 0.1268 f1_train: 0.8504 time: 0.2413s\n",
      "Epoch: 0503 loss_train: 0.1229 f1_train: 0.8536 time: 0.2350s\n",
      "Epoch: 0504 loss_train: 0.1251 f1_train: 0.8485 time: 0.2320s\n",
      "Epoch: 0505 loss_train: 0.1231 f1_train: 0.8505 time: 0.2328s\n",
      "Epoch: 0506 loss_train: 0.1229 f1_train: 0.8513 time: 0.2438s\n",
      "Epoch: 0507 loss_train: 0.1239 f1_train: 0.8554 time: 0.1896s\n",
      "Epoch: 0508 loss_train: 0.1215 f1_train: 0.8526 time: 0.1945s\n",
      "Epoch: 0509 loss_train: 0.1244 f1_train: 0.8511 time: 0.1883s\n",
      "Epoch: 0510 loss_train: 0.1242 f1_train: 0.8545 time: 0.1900s\n",
      "Epoch: 0511 loss_train: 0.1199 f1_train: 0.8548 time: 0.2044s\n",
      "Epoch: 0512 loss_train: 0.1216 f1_train: 0.8517 time: 0.1895s\n",
      "Epoch: 0513 loss_train: 0.1219 f1_train: 0.8550 time: 0.1918s\n",
      "Epoch: 0514 loss_train: 0.1231 f1_train: 0.8544 time: 0.1927s\n",
      "Epoch: 0515 loss_train: 0.1223 f1_train: 0.8498 time: 0.1894s\n",
      "Epoch: 0516 loss_train: 0.1246 f1_train: 0.8519 time: 0.2085s\n",
      "Epoch: 0517 loss_train: 0.1217 f1_train: 0.8499 time: 0.1902s\n",
      "Epoch: 0518 loss_train: 0.1229 f1_train: 0.8516 time: 0.2179s\n",
      "Epoch: 0519 loss_train: 0.1230 f1_train: 0.8531 time: 0.1936s\n",
      "Epoch: 0520 loss_train: 0.1234 f1_train: 0.8542 time: 0.1908s\n",
      "Epoch: 0521 loss_train: 0.1233 f1_train: 0.8563 time: 0.1954s\n",
      "Epoch: 0522 loss_train: 0.1208 f1_train: 0.8563 time: 0.1919s\n",
      "Epoch: 0523 loss_train: 0.1200 f1_train: 0.8584 time: 0.1877s\n",
      "Epoch: 0524 loss_train: 0.1198 f1_train: 0.8532 time: 0.1929s\n",
      "Epoch: 0525 loss_train: 0.1198 f1_train: 0.8549 time: 0.1884s\n",
      "Epoch: 0526 loss_train: 0.1180 f1_train: 0.8577 time: 0.1932s\n",
      "Epoch: 0527 loss_train: 0.1227 f1_train: 0.8535 time: 0.1967s\n",
      "Epoch: 0528 loss_train: 0.1207 f1_train: 0.8564 time: 0.1889s\n",
      "Epoch: 0529 loss_train: 0.1211 f1_train: 0.8523 time: 0.1889s\n",
      "Epoch: 0530 loss_train: 0.1205 f1_train: 0.8534 time: 0.1887s\n",
      "Epoch: 0531 loss_train: 0.1219 f1_train: 0.8544 time: 0.1883s\n",
      "Epoch: 0532 loss_train: 0.1218 f1_train: 0.8558 time: 0.2056s\n",
      "Epoch: 0533 loss_train: 0.1166 f1_train: 0.8573 time: 0.1913s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0534 loss_train: 0.1197 f1_train: 0.8563 time: 0.1900s\n",
      "Epoch: 0535 loss_train: 0.1210 f1_train: 0.8527 time: 0.1880s\n",
      "Epoch: 0536 loss_train: 0.1218 f1_train: 0.8546 time: 0.1871s\n",
      "Epoch: 0537 loss_train: 0.1211 f1_train: 0.8495 time: 0.1928s\n",
      "Epoch: 0538 loss_train: 0.1221 f1_train: 0.8551 time: 0.1838s\n",
      "Epoch: 0539 loss_train: 0.1213 f1_train: 0.8563 time: 0.1841s\n",
      "Epoch: 0540 loss_train: 0.1213 f1_train: 0.8513 time: 0.2143s\n",
      "Epoch: 0541 loss_train: 0.1186 f1_train: 0.8576 time: 0.2162s\n",
      "Epoch: 0542 loss_train: 0.1189 f1_train: 0.8553 time: 0.1973s\n",
      "Epoch: 0543 loss_train: 0.1207 f1_train: 0.8518 time: 0.2144s\n",
      "Epoch: 0544 loss_train: 0.1200 f1_train: 0.8563 time: 0.2865s\n",
      "Epoch: 0545 loss_train: 0.1211 f1_train: 0.8583 time: 0.3104s\n",
      "Epoch: 0546 loss_train: 0.1151 f1_train: 0.8623 time: 0.2442s\n",
      "Epoch: 0547 loss_train: 0.1207 f1_train: 0.8568 time: 0.2536s\n",
      "Epoch: 0548 loss_train: 0.1200 f1_train: 0.8566 time: 0.2250s\n",
      "Epoch: 0549 loss_train: 0.1200 f1_train: 0.8532 time: 0.2566s\n",
      "Epoch: 0550 loss_train: 0.1181 f1_train: 0.8565 time: 0.2846s\n",
      "Epoch: 0551 loss_train: 0.1189 f1_train: 0.8578 time: 0.3061s\n",
      "Epoch: 0552 loss_train: 0.1175 f1_train: 0.8559 time: 0.2923s\n",
      "Epoch: 0553 loss_train: 0.1186 f1_train: 0.8562 time: 0.2072s\n",
      "Epoch: 0554 loss_train: 0.1179 f1_train: 0.8596 time: 0.3518s\n",
      "Epoch: 0555 loss_train: 0.1188 f1_train: 0.8537 time: 0.2321s\n",
      "Epoch: 0556 loss_train: 0.1185 f1_train: 0.8556 time: 0.3492s\n",
      "Epoch: 0557 loss_train: 0.1157 f1_train: 0.8642 time: 0.3341s\n",
      "Epoch: 0558 loss_train: 0.1213 f1_train: 0.8564 time: 0.2826s\n",
      "Epoch: 0559 loss_train: 0.1190 f1_train: 0.8581 time: 0.2419s\n",
      "Epoch: 0560 loss_train: 0.1154 f1_train: 0.8624 time: 0.2398s\n",
      "Epoch: 0561 loss_train: 0.1199 f1_train: 0.8546 time: 0.2465s\n",
      "Epoch: 0562 loss_train: 0.1161 f1_train: 0.8599 time: 0.2942s\n",
      "Epoch: 0563 loss_train: 0.1188 f1_train: 0.8581 time: 0.2392s\n",
      "Epoch: 0564 loss_train: 0.1179 f1_train: 0.8529 time: 0.2501s\n",
      "Epoch: 0565 loss_train: 0.1177 f1_train: 0.8584 time: 0.2490s\n",
      "Epoch: 0566 loss_train: 0.1156 f1_train: 0.8612 time: 0.2400s\n",
      "Epoch: 0567 loss_train: 0.1209 f1_train: 0.8579 time: 0.2366s\n",
      "Epoch: 0568 loss_train: 0.1172 f1_train: 0.8581 time: 0.2582s\n",
      "Epoch: 0569 loss_train: 0.1167 f1_train: 0.8595 time: 0.2373s\n",
      "Epoch: 0570 loss_train: 0.1181 f1_train: 0.8562 time: 0.2598s\n",
      "Epoch: 0571 loss_train: 0.1194 f1_train: 0.8575 time: 0.2687s\n",
      "Epoch: 0572 loss_train: 0.1161 f1_train: 0.8575 time: 0.3722s\n",
      "Epoch: 0573 loss_train: 0.1185 f1_train: 0.8558 time: 0.2713s\n",
      "Epoch: 0574 loss_train: 0.1152 f1_train: 0.8654 time: 0.2388s\n",
      "Epoch: 0575 loss_train: 0.1169 f1_train: 0.8610 time: 0.3047s\n",
      "Epoch: 0576 loss_train: 0.1162 f1_train: 0.8579 time: 0.2369s\n",
      "Epoch: 0577 loss_train: 0.1173 f1_train: 0.8554 time: 0.2400s\n",
      "Epoch: 0578 loss_train: 0.1159 f1_train: 0.8569 time: 0.2377s\n",
      "Epoch: 0579 loss_train: 0.1139 f1_train: 0.8656 time: 0.2329s\n",
      "Epoch: 0580 loss_train: 0.1137 f1_train: 0.8681 time: 0.2287s\n",
      "Epoch: 0581 loss_train: 0.1191 f1_train: 0.8577 time: 0.3402s\n",
      "Epoch: 0582 loss_train: 0.1178 f1_train: 0.8590 time: 0.2761s\n",
      "Epoch: 0583 loss_train: 0.1155 f1_train: 0.8629 time: 0.2761s\n",
      "Epoch: 0584 loss_train: 0.1183 f1_train: 0.8539 time: 0.2402s\n",
      "Epoch: 0585 loss_train: 0.1150 f1_train: 0.8633 time: 0.2381s\n",
      "Epoch: 0586 loss_train: 0.1162 f1_train: 0.8622 time: 0.2300s\n",
      "Epoch: 0587 loss_train: 0.1177 f1_train: 0.8598 time: 0.2340s\n",
      "Epoch: 0588 loss_train: 0.1170 f1_train: 0.8609 time: 0.2382s\n",
      "Epoch: 0589 loss_train: 0.1137 f1_train: 0.8596 time: 0.2356s\n",
      "Epoch: 0590 loss_train: 0.1147 f1_train: 0.8609 time: 0.2375s\n",
      "Epoch: 0591 loss_train: 0.1159 f1_train: 0.8592 time: 0.2384s\n",
      "Epoch: 0592 loss_train: 0.1152 f1_train: 0.8630 time: 0.2280s\n",
      "Epoch: 0593 loss_train: 0.1167 f1_train: 0.8583 time: 0.2401s\n",
      "Epoch: 0594 loss_train: 0.1156 f1_train: 0.8596 time: 0.2276s\n",
      "Epoch: 0595 loss_train: 0.1144 f1_train: 0.8613 time: 0.2384s\n",
      "Epoch: 0596 loss_train: 0.1140 f1_train: 0.8683 time: 0.2340s\n",
      "Epoch: 0597 loss_train: 0.1128 f1_train: 0.8623 time: 0.2353s\n",
      "Epoch: 0598 loss_train: 0.1149 f1_train: 0.8610 time: 0.2378s\n",
      "Epoch: 0599 loss_train: 0.1139 f1_train: 0.8630 time: 0.2418s\n",
      "Epoch: 0600 loss_train: 0.1141 f1_train: 0.8654 time: 0.2296s\n",
      "Epoch: 0601 loss_train: 0.1150 f1_train: 0.8642 time: 0.2503s\n",
      "Epoch: 0602 loss_train: 0.1159 f1_train: 0.8612 time: 0.2252s\n",
      "Epoch: 0603 loss_train: 0.1142 f1_train: 0.8641 time: 0.2326s\n",
      "Epoch: 0604 loss_train: 0.1114 f1_train: 0.8687 time: 0.2244s\n",
      "Epoch: 0605 loss_train: 0.1141 f1_train: 0.8658 time: 0.2418s\n",
      "Epoch: 0606 loss_train: 0.1146 f1_train: 0.8555 time: 0.2530s\n",
      "Epoch: 0607 loss_train: 0.1165 f1_train: 0.8613 time: 0.2280s\n",
      "Epoch: 0608 loss_train: 0.1136 f1_train: 0.8622 time: 0.2086s\n",
      "Epoch: 0609 loss_train: 0.1128 f1_train: 0.8619 time: 0.2058s\n",
      "Epoch: 0610 loss_train: 0.1157 f1_train: 0.8615 time: 0.1956s\n",
      "Epoch: 0611 loss_train: 0.1162 f1_train: 0.8595 time: 0.1955s\n",
      "Epoch: 0612 loss_train: 0.1148 f1_train: 0.8596 time: 0.1888s\n",
      "Epoch: 0613 loss_train: 0.1132 f1_train: 0.8626 time: 0.1976s\n",
      "Epoch: 0614 loss_train: 0.1126 f1_train: 0.8606 time: 0.1879s\n",
      "Epoch: 0615 loss_train: 0.1146 f1_train: 0.8571 time: 0.1989s\n",
      "Epoch: 0616 loss_train: 0.1145 f1_train: 0.8588 time: 0.1896s\n",
      "Epoch: 0617 loss_train: 0.1142 f1_train: 0.8652 time: 0.2153s\n",
      "Epoch: 0618 loss_train: 0.1142 f1_train: 0.8629 time: 0.2300s\n",
      "Epoch: 0619 loss_train: 0.1120 f1_train: 0.8636 time: 0.2342s\n",
      "Epoch: 0620 loss_train: 0.1136 f1_train: 0.8615 time: 0.2300s\n",
      "Epoch: 0621 loss_train: 0.1119 f1_train: 0.8650 time: 0.2509s\n",
      "Epoch: 0622 loss_train: 0.1145 f1_train: 0.8684 time: 0.2350s\n",
      "Epoch: 0623 loss_train: 0.1138 f1_train: 0.8620 time: 0.2329s\n",
      "Epoch: 0624 loss_train: 0.1122 f1_train: 0.8635 time: 0.2565s\n",
      "Epoch: 0625 loss_train: 0.1131 f1_train: 0.8606 time: 0.2869s\n",
      "Epoch: 0626 loss_train: 0.1156 f1_train: 0.8596 time: 0.3308s\n",
      "Epoch: 0627 loss_train: 0.1120 f1_train: 0.8615 time: 0.2484s\n",
      "Epoch: 0628 loss_train: 0.1135 f1_train: 0.8625 time: 0.2416s\n",
      "Epoch: 0629 loss_train: 0.1133 f1_train: 0.8632 time: 0.2946s\n",
      "Epoch: 0630 loss_train: 0.1141 f1_train: 0.8625 time: 0.2521s\n",
      "Epoch: 0631 loss_train: 0.1138 f1_train: 0.8620 time: 0.2672s\n",
      "Epoch: 0632 loss_train: 0.1109 f1_train: 0.8689 time: 0.2373s\n",
      "Epoch: 0633 loss_train: 0.1119 f1_train: 0.8623 time: 0.2699s\n",
      "Epoch: 0634 loss_train: 0.1101 f1_train: 0.8664 time: 0.2889s\n",
      "Epoch: 0635 loss_train: 0.1125 f1_train: 0.8650 time: 0.2700s\n",
      "Epoch: 0636 loss_train: 0.1128 f1_train: 0.8585 time: 0.2098s\n",
      "Epoch: 0637 loss_train: 0.1130 f1_train: 0.8646 time: 0.2124s\n",
      "Epoch: 0638 loss_train: 0.1130 f1_train: 0.8631 time: 0.2073s\n",
      "Epoch: 0639 loss_train: 0.1136 f1_train: 0.8618 time: 0.2255s\n",
      "Epoch: 0640 loss_train: 0.1121 f1_train: 0.8634 time: 0.2706s\n",
      "Epoch: 0641 loss_train: 0.1088 f1_train: 0.8688 time: 0.2137s\n",
      "Epoch: 0642 loss_train: 0.1119 f1_train: 0.8613 time: 0.2120s\n",
      "Epoch: 0643 loss_train: 0.1113 f1_train: 0.8657 time: 0.2199s\n",
      "Epoch: 0644 loss_train: 0.1120 f1_train: 0.8636 time: 0.1934s\n",
      "Epoch: 0645 loss_train: 0.1117 f1_train: 0.8656 time: 0.2228s\n",
      "Epoch: 0646 loss_train: 0.1134 f1_train: 0.8652 time: 0.2404s\n",
      "Epoch: 0647 loss_train: 0.1100 f1_train: 0.8642 time: 0.2158s\n",
      "Epoch: 0648 loss_train: 0.1120 f1_train: 0.8635 time: 0.2054s\n",
      "Epoch: 0649 loss_train: 0.1130 f1_train: 0.8623 time: 0.2555s\n",
      "Epoch: 0650 loss_train: 0.1125 f1_train: 0.8657 time: 0.2467s\n",
      "Epoch: 0651 loss_train: 0.1148 f1_train: 0.8615 time: 0.2002s\n",
      "Epoch: 0652 loss_train: 0.1141 f1_train: 0.8592 time: 0.2168s\n",
      "Epoch: 0653 loss_train: 0.1122 f1_train: 0.8665 time: 0.2383s\n",
      "Epoch: 0654 loss_train: 0.1094 f1_train: 0.8604 time: 0.2332s\n",
      "Epoch: 0655 loss_train: 0.1115 f1_train: 0.8667 time: 0.2558s\n",
      "Epoch: 0656 loss_train: 0.1125 f1_train: 0.8608 time: 0.2366s\n",
      "Epoch: 0657 loss_train: 0.1102 f1_train: 0.8657 time: 0.2322s\n",
      "Epoch: 0658 loss_train: 0.1111 f1_train: 0.8628 time: 0.2509s\n",
      "Epoch: 0659 loss_train: 0.1091 f1_train: 0.8650 time: 0.2464s\n",
      "Epoch: 0660 loss_train: 0.1100 f1_train: 0.8653 time: 0.2453s\n",
      "Epoch: 0661 loss_train: 0.1105 f1_train: 0.8613 time: 0.2854s\n",
      "Epoch: 0662 loss_train: 0.1130 f1_train: 0.8637 time: 0.2391s\n",
      "Epoch: 0663 loss_train: 0.1109 f1_train: 0.8618 time: 0.2477s\n",
      "Epoch: 0664 loss_train: 0.1101 f1_train: 0.8663 time: 0.2411s\n",
      "Epoch: 0665 loss_train: 0.1092 f1_train: 0.8671 time: 0.2595s\n",
      "Epoch: 0666 loss_train: 0.1099 f1_train: 0.8680 time: 0.2344s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0667 loss_train: 0.1083 f1_train: 0.8701 time: 0.2403s\n",
      "Epoch: 0668 loss_train: 0.1109 f1_train: 0.8692 time: 0.2213s\n",
      "Epoch: 0669 loss_train: 0.1142 f1_train: 0.8638 time: 0.2385s\n",
      "Epoch: 0670 loss_train: 0.1111 f1_train: 0.8642 time: 0.2364s\n",
      "Epoch: 0671 loss_train: 0.1109 f1_train: 0.8638 time: 0.2279s\n",
      "Epoch: 0672 loss_train: 0.1109 f1_train: 0.8622 time: 0.2376s\n",
      "Epoch: 0673 loss_train: 0.1096 f1_train: 0.8655 time: 0.2964s\n",
      "Epoch: 0674 loss_train: 0.1090 f1_train: 0.8665 time: 0.2924s\n",
      "Epoch: 0675 loss_train: 0.1087 f1_train: 0.8686 time: 0.2232s\n",
      "Epoch: 0676 loss_train: 0.1090 f1_train: 0.8640 time: 0.2109s\n",
      "Epoch: 0677 loss_train: 0.1070 f1_train: 0.8705 time: 0.1940s\n",
      "Epoch: 0678 loss_train: 0.1099 f1_train: 0.8692 time: 0.3176s\n",
      "Epoch: 0679 loss_train: 0.1099 f1_train: 0.8714 time: 0.2794s\n",
      "Epoch: 0680 loss_train: 0.1086 f1_train: 0.8633 time: 0.2382s\n",
      "Epoch: 0681 loss_train: 0.1096 f1_train: 0.8686 time: 0.2389s\n",
      "Epoch: 0682 loss_train: 0.1083 f1_train: 0.8683 time: 0.2710s\n",
      "Epoch: 0683 loss_train: 0.1094 f1_train: 0.8670 time: 0.3718s\n",
      "Epoch: 0684 loss_train: 0.1083 f1_train: 0.8661 time: 0.3622s\n",
      "Epoch: 0685 loss_train: 0.1091 f1_train: 0.8667 time: 0.3127s\n",
      "Epoch: 0686 loss_train: 0.1086 f1_train: 0.8649 time: 0.2533s\n",
      "Epoch: 0687 loss_train: 0.1106 f1_train: 0.8669 time: 0.2281s\n",
      "Epoch: 0688 loss_train: 0.1100 f1_train: 0.8672 time: 0.2458s\n",
      "Epoch: 0689 loss_train: 0.1055 f1_train: 0.8708 time: 0.2111s\n",
      "Epoch: 0690 loss_train: 0.1090 f1_train: 0.8641 time: 0.2088s\n",
      "Epoch: 0691 loss_train: 0.1092 f1_train: 0.8640 time: 0.2247s\n",
      "Epoch: 0692 loss_train: 0.1101 f1_train: 0.8668 time: 0.2250s\n",
      "Epoch: 0693 loss_train: 0.1096 f1_train: 0.8658 time: 0.2161s\n",
      "Epoch: 0694 loss_train: 0.1056 f1_train: 0.8681 time: 0.3010s\n",
      "Epoch: 0695 loss_train: 0.1100 f1_train: 0.8681 time: 0.2969s\n",
      "Epoch: 0696 loss_train: 0.1069 f1_train: 0.8659 time: 0.2737s\n",
      "Epoch: 0697 loss_train: 0.1074 f1_train: 0.8703 time: 0.2954s\n",
      "Epoch: 0698 loss_train: 0.1085 f1_train: 0.8674 time: 0.2446s\n",
      "Epoch: 0699 loss_train: 0.1082 f1_train: 0.8692 time: 0.2459s\n",
      "Epoch: 0700 loss_train: 0.1088 f1_train: 0.8661 time: 0.2515s\n",
      "Epoch: 0701 loss_train: 0.1096 f1_train: 0.8693 time: 0.2381s\n",
      "Epoch: 0702 loss_train: 0.1076 f1_train: 0.8728 time: 0.2389s\n",
      "Epoch: 0703 loss_train: 0.1100 f1_train: 0.8705 time: 0.2490s\n",
      "Epoch: 0704 loss_train: 0.1064 f1_train: 0.8673 time: 0.2573s\n",
      "Epoch: 0705 loss_train: 0.1089 f1_train: 0.8638 time: 0.2387s\n",
      "Epoch: 0706 loss_train: 0.1092 f1_train: 0.8660 time: 0.2322s\n",
      "Epoch: 0707 loss_train: 0.1098 f1_train: 0.8653 time: 0.2558s\n",
      "Epoch: 0708 loss_train: 0.1081 f1_train: 0.8704 time: 0.2554s\n",
      "Epoch: 0709 loss_train: 0.1072 f1_train: 0.8661 time: 0.2409s\n",
      "Epoch: 0710 loss_train: 0.1087 f1_train: 0.8670 time: 0.2346s\n",
      "Epoch: 0711 loss_train: 0.1033 f1_train: 0.8718 time: 0.2318s\n",
      "Epoch: 0712 loss_train: 0.1055 f1_train: 0.8723 time: 0.2455s\n",
      "Epoch: 0713 loss_train: 0.1066 f1_train: 0.8714 time: 0.2361s\n",
      "Epoch: 0714 loss_train: 0.1078 f1_train: 0.8710 time: 0.2348s\n",
      "Epoch: 0715 loss_train: 0.1061 f1_train: 0.8702 time: 0.2431s\n",
      "Epoch: 0716 loss_train: 0.1064 f1_train: 0.8644 time: 0.2385s\n",
      "Epoch: 0717 loss_train: 0.1087 f1_train: 0.8633 time: 0.2424s\n",
      "Epoch: 0718 loss_train: 0.1087 f1_train: 0.8614 time: 0.2469s\n",
      "Epoch: 0719 loss_train: 0.1067 f1_train: 0.8684 time: 0.2443s\n",
      "Epoch: 0720 loss_train: 0.1049 f1_train: 0.8742 time: 0.2552s\n",
      "Epoch: 0721 loss_train: 0.1062 f1_train: 0.8704 time: 0.2469s\n",
      "Epoch: 0722 loss_train: 0.1072 f1_train: 0.8696 time: 0.2872s\n",
      "Epoch: 0723 loss_train: 0.1070 f1_train: 0.8707 time: 0.2466s\n",
      "Epoch: 0724 loss_train: 0.1063 f1_train: 0.8697 time: 0.2599s\n",
      "Epoch: 0725 loss_train: 0.1095 f1_train: 0.8679 time: 0.2855s\n",
      "Epoch: 0726 loss_train: 0.1077 f1_train: 0.8658 time: 0.2743s\n",
      "Epoch: 0727 loss_train: 0.1092 f1_train: 0.8671 time: 0.2049s\n",
      "Epoch: 0728 loss_train: 0.1072 f1_train: 0.8695 time: 0.3233s\n",
      "Epoch: 0729 loss_train: 0.1045 f1_train: 0.8744 time: 0.3291s\n",
      "Epoch: 0730 loss_train: 0.1063 f1_train: 0.8715 time: 0.2760s\n",
      "Epoch: 0731 loss_train: 0.1089 f1_train: 0.8682 time: 0.2529s\n",
      "Epoch: 0732 loss_train: 0.1054 f1_train: 0.8684 time: 0.2487s\n",
      "Epoch: 0733 loss_train: 0.1053 f1_train: 0.8662 time: 0.2354s\n",
      "Epoch: 0734 loss_train: 0.1077 f1_train: 0.8653 time: 0.2334s\n",
      "Epoch: 0735 loss_train: 0.1072 f1_train: 0.8728 time: 0.2472s\n",
      "Epoch: 0736 loss_train: 0.1051 f1_train: 0.8717 time: 0.3237s\n",
      "Epoch: 0737 loss_train: 0.1053 f1_train: 0.8752 time: 0.2251s\n",
      "Epoch: 0738 loss_train: 0.1084 f1_train: 0.8648 time: 0.2384s\n",
      "Epoch: 0739 loss_train: 0.1096 f1_train: 0.8675 time: 0.2368s\n",
      "Epoch: 0740 loss_train: 0.1079 f1_train: 0.8681 time: 0.2347s\n",
      "Epoch: 0741 loss_train: 0.1063 f1_train: 0.8702 time: 0.2388s\n",
      "Epoch: 0742 loss_train: 0.1058 f1_train: 0.8690 time: 0.2409s\n",
      "Epoch: 0743 loss_train: 0.1085 f1_train: 0.8643 time: 0.2656s\n",
      "Epoch: 0744 loss_train: 0.1074 f1_train: 0.8666 time: 0.2307s\n",
      "Epoch: 0745 loss_train: 0.1036 f1_train: 0.8728 time: 0.2443s\n",
      "Epoch: 0746 loss_train: 0.1057 f1_train: 0.8692 time: 0.2545s\n",
      "Epoch: 0747 loss_train: 0.1046 f1_train: 0.8747 time: 0.2402s\n",
      "Epoch: 0748 loss_train: 0.1043 f1_train: 0.8732 time: 0.2350s\n",
      "Epoch: 0749 loss_train: 0.1046 f1_train: 0.8740 time: 0.2368s\n",
      "Epoch: 0750 loss_train: 0.1040 f1_train: 0.8740 time: 0.2557s\n",
      "Epoch: 0751 loss_train: 0.1016 f1_train: 0.8746 time: 0.2406s\n",
      "Epoch: 0752 loss_train: 0.1060 f1_train: 0.8762 time: 0.2364s\n",
      "Epoch: 0753 loss_train: 0.1051 f1_train: 0.8718 time: 0.2372s\n",
      "Epoch: 0754 loss_train: 0.1028 f1_train: 0.8691 time: 0.2371s\n",
      "Epoch: 0755 loss_train: 0.1055 f1_train: 0.8673 time: 0.2330s\n",
      "Epoch: 0756 loss_train: 0.1061 f1_train: 0.8681 time: 0.2628s\n",
      "Epoch: 0757 loss_train: 0.1022 f1_train: 0.8740 time: 0.2317s\n",
      "Epoch: 0758 loss_train: 0.1059 f1_train: 0.8690 time: 0.2380s\n",
      "Epoch: 0759 loss_train: 0.1051 f1_train: 0.8707 time: 0.2544s\n",
      "Epoch: 0760 loss_train: 0.1043 f1_train: 0.8725 time: 0.2376s\n",
      "Epoch: 0761 loss_train: 0.1045 f1_train: 0.8734 time: 0.2365s\n",
      "Epoch: 0762 loss_train: 0.1050 f1_train: 0.8709 time: 0.2542s\n",
      "Epoch: 0763 loss_train: 0.1044 f1_train: 0.8706 time: 0.2383s\n",
      "Epoch: 0764 loss_train: 0.1055 f1_train: 0.8707 time: 0.2321s\n",
      "Epoch: 0765 loss_train: 0.1049 f1_train: 0.8708 time: 0.2383s\n",
      "Epoch: 0766 loss_train: 0.1050 f1_train: 0.8716 time: 0.2451s\n",
      "Epoch: 0767 loss_train: 0.1031 f1_train: 0.8764 time: 0.2390s\n",
      "Epoch: 0768 loss_train: 0.1031 f1_train: 0.8736 time: 0.2311s\n",
      "Epoch: 0769 loss_train: 0.1048 f1_train: 0.8723 time: 0.2362s\n",
      "Epoch: 0770 loss_train: 0.1033 f1_train: 0.8745 time: 0.2328s\n",
      "Epoch: 0771 loss_train: 0.1048 f1_train: 0.8696 time: 0.2360s\n",
      "Epoch: 0772 loss_train: 0.1012 f1_train: 0.8737 time: 0.2330s\n",
      "Epoch: 0773 loss_train: 0.1017 f1_train: 0.8718 time: 0.2375s\n",
      "Epoch: 0774 loss_train: 0.1014 f1_train: 0.8790 time: 0.2437s\n",
      "Epoch: 0775 loss_train: 0.1025 f1_train: 0.8712 time: 0.2337s\n",
      "Epoch: 0776 loss_train: 0.1029 f1_train: 0.8736 time: 0.2067s\n",
      "Epoch: 0777 loss_train: 0.1018 f1_train: 0.8760 time: 0.1934s\n",
      "Epoch: 0778 loss_train: 0.1043 f1_train: 0.8704 time: 0.1952s\n",
      "Epoch: 0779 loss_train: 0.1044 f1_train: 0.8728 time: 0.1887s\n",
      "Epoch: 0780 loss_train: 0.1037 f1_train: 0.8748 time: 0.1986s\n",
      "Epoch: 0781 loss_train: 0.1025 f1_train: 0.8749 time: 0.1882s\n",
      "Epoch: 0782 loss_train: 0.1050 f1_train: 0.8722 time: 0.1966s\n",
      "Epoch: 0783 loss_train: 0.1011 f1_train: 0.8748 time: 0.1933s\n",
      "Epoch: 0784 loss_train: 0.1029 f1_train: 0.8695 time: 0.1973s\n",
      "Epoch: 0785 loss_train: 0.1029 f1_train: 0.8700 time: 0.1884s\n",
      "Epoch: 0786 loss_train: 0.1031 f1_train: 0.8739 time: 0.2296s\n",
      "Epoch: 0787 loss_train: 0.1025 f1_train: 0.8739 time: 0.2248s\n",
      "Epoch: 0788 loss_train: 0.1026 f1_train: 0.8753 time: 0.2470s\n",
      "Epoch: 0789 loss_train: 0.1042 f1_train: 0.8770 time: 0.2349s\n",
      "Epoch: 0790 loss_train: 0.1056 f1_train: 0.8705 time: 0.2438s\n",
      "Epoch: 0791 loss_train: 0.1038 f1_train: 0.8719 time: 0.2389s\n",
      "Epoch: 0792 loss_train: 0.1028 f1_train: 0.8728 time: 0.2363s\n",
      "Epoch: 0793 loss_train: 0.1036 f1_train: 0.8692 time: 0.2407s\n",
      "Epoch: 0794 loss_train: 0.1002 f1_train: 0.8797 time: 0.2314s\n",
      "Epoch: 0795 loss_train: 0.1035 f1_train: 0.8722 time: 0.2416s\n",
      "Epoch: 0796 loss_train: 0.1049 f1_train: 0.8706 time: 0.2431s\n",
      "Epoch: 0797 loss_train: 0.1029 f1_train: 0.8738 time: 0.2437s\n",
      "Epoch: 0798 loss_train: 0.1009 f1_train: 0.8786 time: 0.2388s\n",
      "Epoch: 0799 loss_train: 0.1024 f1_train: 0.8728 time: 0.2417s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0800 loss_train: 0.1016 f1_train: 0.8763 time: 0.2364s\n",
      "Epoch: 0801 loss_train: 0.1041 f1_train: 0.8734 time: 0.2347s\n",
      "Epoch: 0802 loss_train: 0.1015 f1_train: 0.8793 time: 0.2440s\n",
      "Epoch: 0803 loss_train: 0.1016 f1_train: 0.8721 time: 0.2325s\n",
      "Epoch: 0804 loss_train: 0.1029 f1_train: 0.8735 time: 0.2344s\n",
      "Epoch: 0805 loss_train: 0.1026 f1_train: 0.8764 time: 0.2339s\n",
      "Epoch: 0806 loss_train: 0.1017 f1_train: 0.8765 time: 0.2341s\n",
      "Epoch: 0807 loss_train: 0.1049 f1_train: 0.8745 time: 0.2345s\n",
      "Epoch: 0808 loss_train: 0.0999 f1_train: 0.8763 time: 0.2300s\n",
      "Epoch: 0809 loss_train: 0.1010 f1_train: 0.8776 time: 0.2372s\n",
      "Epoch: 0810 loss_train: 0.1021 f1_train: 0.8713 time: 0.2374s\n",
      "Epoch: 0811 loss_train: 0.1011 f1_train: 0.8746 time: 0.1938s\n",
      "Epoch: 0812 loss_train: 0.1025 f1_train: 0.8769 time: 0.1971s\n",
      "Epoch: 0813 loss_train: 0.1036 f1_train: 0.8732 time: 0.2029s\n",
      "Epoch: 0814 loss_train: 0.1010 f1_train: 0.8711 time: 0.1905s\n",
      "Epoch: 0815 loss_train: 0.1012 f1_train: 0.8766 time: 0.1966s\n",
      "Epoch: 0816 loss_train: 0.1023 f1_train: 0.8708 time: 0.1947s\n",
      "Epoch: 0817 loss_train: 0.1033 f1_train: 0.8729 time: 0.1921s\n",
      "Epoch: 0818 loss_train: 0.1031 f1_train: 0.8715 time: 0.1875s\n",
      "Epoch: 0819 loss_train: 0.1014 f1_train: 0.8776 time: 0.1943s\n",
      "Epoch: 0820 loss_train: 0.1040 f1_train: 0.8731 time: 0.1881s\n",
      "Epoch: 0821 loss_train: 0.1019 f1_train: 0.8768 time: 0.2430s\n",
      "Epoch: 0822 loss_train: 0.1007 f1_train: 0.8775 time: 0.2347s\n",
      "Epoch: 0823 loss_train: 0.1015 f1_train: 0.8710 time: 0.2531s\n",
      "Epoch: 0824 loss_train: 0.1042 f1_train: 0.8755 time: 0.2331s\n",
      "Epoch: 0825 loss_train: 0.1013 f1_train: 0.8770 time: 0.2404s\n",
      "Epoch: 0826 loss_train: 0.1006 f1_train: 0.8760 time: 0.2398s\n",
      "Epoch: 0827 loss_train: 0.1016 f1_train: 0.8740 time: 0.2359s\n",
      "Epoch: 0828 loss_train: 0.0985 f1_train: 0.8770 time: 0.2294s\n",
      "Epoch: 0829 loss_train: 0.0998 f1_train: 0.8739 time: 0.2211s\n",
      "Epoch: 0830 loss_train: 0.1003 f1_train: 0.8751 time: 0.1874s\n",
      "Epoch: 0831 loss_train: 0.1009 f1_train: 0.8770 time: 0.1960s\n",
      "Epoch: 0832 loss_train: 0.1028 f1_train: 0.8756 time: 0.1909s\n",
      "Epoch: 0833 loss_train: 0.0991 f1_train: 0.8739 time: 0.1948s\n",
      "Epoch: 0834 loss_train: 0.1001 f1_train: 0.8773 time: 0.1890s\n",
      "Epoch: 0835 loss_train: 0.0986 f1_train: 0.8791 time: 0.2030s\n",
      "Epoch: 0836 loss_train: 0.1006 f1_train: 0.8778 time: 0.1883s\n",
      "Epoch: 0837 loss_train: 0.1018 f1_train: 0.8713 time: 0.1944s\n",
      "Epoch: 0838 loss_train: 0.1000 f1_train: 0.8791 time: 0.1881s\n",
      "Epoch: 0839 loss_train: 0.1004 f1_train: 0.8776 time: 0.1983s\n",
      "Epoch: 0840 loss_train: 0.0999 f1_train: 0.8816 time: 0.1884s\n",
      "Epoch: 0841 loss_train: 0.1017 f1_train: 0.8748 time: 0.1972s\n",
      "Epoch: 0842 loss_train: 0.0996 f1_train: 0.8744 time: 0.1891s\n",
      "Epoch: 0843 loss_train: 0.1020 f1_train: 0.8718 time: 0.1952s\n",
      "Epoch: 0844 loss_train: 0.1006 f1_train: 0.8745 time: 0.1884s\n",
      "Epoch: 0845 loss_train: 0.1006 f1_train: 0.8762 time: 0.1968s\n",
      "Epoch: 0846 loss_train: 0.1006 f1_train: 0.8748 time: 0.1933s\n",
      "Epoch: 0847 loss_train: 0.1006 f1_train: 0.8771 time: 0.2045s\n",
      "Epoch: 0848 loss_train: 0.1005 f1_train: 0.8791 time: 0.1876s\n",
      "Epoch: 0849 loss_train: 0.0986 f1_train: 0.8796 time: 0.2003s\n",
      "Epoch: 0850 loss_train: 0.1013 f1_train: 0.8743 time: 0.1887s\n",
      "Epoch: 0851 loss_train: 0.0999 f1_train: 0.8787 time: 0.1972s\n",
      "Epoch: 0852 loss_train: 0.0998 f1_train: 0.8761 time: 0.1894s\n",
      "Epoch: 0853 loss_train: 0.1011 f1_train: 0.8730 time: 0.2007s\n",
      "Epoch: 0854 loss_train: 0.1010 f1_train: 0.8755 time: 0.1892s\n",
      "Epoch: 0855 loss_train: 0.1003 f1_train: 0.8775 time: 0.1960s\n",
      "Epoch: 0856 loss_train: 0.0970 f1_train: 0.8811 time: 0.1897s\n",
      "Epoch: 0857 loss_train: 0.0987 f1_train: 0.8738 time: 0.1967s\n",
      "Epoch: 0858 loss_train: 0.1022 f1_train: 0.8772 time: 0.1882s\n",
      "Epoch: 0859 loss_train: 0.0985 f1_train: 0.8810 time: 0.1980s\n",
      "Epoch: 0860 loss_train: 0.1010 f1_train: 0.8763 time: 0.1901s\n",
      "Epoch: 0861 loss_train: 0.0970 f1_train: 0.8791 time: 0.2017s\n",
      "Epoch: 0862 loss_train: 0.0990 f1_train: 0.8791 time: 0.1941s\n",
      "Epoch: 0863 loss_train: 0.1000 f1_train: 0.8740 time: 0.1937s\n",
      "Epoch: 0864 loss_train: 0.0986 f1_train: 0.8819 time: 0.1898s\n",
      "Epoch: 0865 loss_train: 0.0984 f1_train: 0.8789 time: 0.2231s\n",
      "Epoch: 0866 loss_train: 0.0962 f1_train: 0.8836 time: 0.2187s\n",
      "Epoch: 0867 loss_train: 0.0992 f1_train: 0.8726 time: 0.1982s\n",
      "Epoch: 0868 loss_train: 0.0972 f1_train: 0.8749 time: 0.2267s\n",
      "Epoch: 0869 loss_train: 0.1012 f1_train: 0.8780 time: 0.1979s\n",
      "Epoch: 0870 loss_train: 0.1001 f1_train: 0.8758 time: 0.1955s\n",
      "Epoch: 0871 loss_train: 0.0990 f1_train: 0.8738 time: 0.1951s\n",
      "Epoch: 0872 loss_train: 0.0971 f1_train: 0.8797 time: 0.1993s\n",
      "Epoch: 0873 loss_train: 0.0991 f1_train: 0.8762 time: 0.1968s\n",
      "Epoch: 0874 loss_train: 0.1017 f1_train: 0.8734 time: 0.1972s\n",
      "Epoch: 0875 loss_train: 0.1016 f1_train: 0.8772 time: 0.2167s\n",
      "Epoch: 0876 loss_train: 0.0990 f1_train: 0.8783 time: 0.1956s\n",
      "Epoch: 0877 loss_train: 0.0984 f1_train: 0.8785 time: 0.1940s\n",
      "Epoch: 0878 loss_train: 0.0964 f1_train: 0.8764 time: 0.2054s\n",
      "Epoch: 0879 loss_train: 0.1010 f1_train: 0.8782 time: 0.2248s\n",
      "Epoch: 0880 loss_train: 0.0971 f1_train: 0.8804 time: 0.1896s\n",
      "Epoch: 0881 loss_train: 0.0980 f1_train: 0.8819 time: 0.1977s\n",
      "Epoch: 0882 loss_train: 0.0991 f1_train: 0.8760 time: 0.2099s\n",
      "Epoch: 0883 loss_train: 0.0956 f1_train: 0.8841 time: 0.1891s\n",
      "Epoch: 0884 loss_train: 0.0989 f1_train: 0.8791 time: 0.1955s\n",
      "Epoch: 0885 loss_train: 0.0977 f1_train: 0.8792 time: 0.1892s\n",
      "Epoch: 0886 loss_train: 0.0985 f1_train: 0.8753 time: 0.1986s\n",
      "Epoch: 0887 loss_train: 0.0991 f1_train: 0.8768 time: 0.2047s\n",
      "Epoch: 0888 loss_train: 0.0979 f1_train: 0.8791 time: 0.2030s\n",
      "Epoch: 0889 loss_train: 0.0992 f1_train: 0.8804 time: 0.2356s\n",
      "Epoch: 0890 loss_train: 0.0977 f1_train: 0.8798 time: 0.2436s\n",
      "Epoch: 0891 loss_train: 0.1010 f1_train: 0.8710 time: 0.2533s\n",
      "Epoch: 0892 loss_train: 0.0968 f1_train: 0.8793 time: 0.2349s\n",
      "Epoch: 0893 loss_train: 0.0985 f1_train: 0.8835 time: 0.2326s\n",
      "Epoch: 0894 loss_train: 0.0982 f1_train: 0.8802 time: 0.2341s\n",
      "Epoch: 0895 loss_train: 0.0975 f1_train: 0.8787 time: 0.2343s\n",
      "Epoch: 0896 loss_train: 0.0956 f1_train: 0.8818 time: 0.2316s\n",
      "Epoch: 0897 loss_train: 0.0961 f1_train: 0.8832 time: 0.2420s\n",
      "Epoch: 0898 loss_train: 0.0976 f1_train: 0.8769 time: 0.2361s\n",
      "Epoch: 0899 loss_train: 0.0995 f1_train: 0.8760 time: 0.2344s\n",
      "Epoch: 0900 loss_train: 0.0976 f1_train: 0.8788 time: 0.2303s\n",
      "Epoch: 0901 loss_train: 0.0969 f1_train: 0.8773 time: 0.2313s\n",
      "Epoch: 0902 loss_train: 0.0973 f1_train: 0.8754 time: 0.2286s\n",
      "Epoch: 0903 loss_train: 0.0987 f1_train: 0.8808 time: 0.2411s\n",
      "Epoch: 0904 loss_train: 0.0969 f1_train: 0.8792 time: 0.2406s\n",
      "Epoch: 0905 loss_train: 0.0981 f1_train: 0.8795 time: 0.2406s\n",
      "Epoch: 0906 loss_train: 0.0978 f1_train: 0.8745 time: 0.1889s\n",
      "Epoch: 0907 loss_train: 0.0971 f1_train: 0.8799 time: 0.1973s\n",
      "Epoch: 0908 loss_train: 0.0955 f1_train: 0.8826 time: 0.1896s\n",
      "Epoch: 0909 loss_train: 0.0968 f1_train: 0.8829 time: 0.1962s\n",
      "Epoch: 0910 loss_train: 0.0963 f1_train: 0.8821 time: 0.1936s\n",
      "Epoch: 0911 loss_train: 0.0980 f1_train: 0.8795 time: 0.2101s\n",
      "Epoch: 0912 loss_train: 0.0982 f1_train: 0.8787 time: 0.1887s\n",
      "Epoch: 0913 loss_train: 0.0967 f1_train: 0.8770 time: 0.2101s\n",
      "Epoch: 0914 loss_train: 0.0982 f1_train: 0.8769 time: 0.1950s\n",
      "Epoch: 0915 loss_train: 0.0947 f1_train: 0.8778 time: 0.2004s\n",
      "Epoch: 0916 loss_train: 0.0966 f1_train: 0.8782 time: 0.1886s\n",
      "Epoch: 0917 loss_train: 0.0967 f1_train: 0.8814 time: 0.1980s\n",
      "Epoch: 0918 loss_train: 0.0971 f1_train: 0.8780 time: 0.1888s\n",
      "Epoch: 0919 loss_train: 0.0982 f1_train: 0.8772 time: 0.2070s\n",
      "Epoch: 0920 loss_train: 0.0968 f1_train: 0.8821 time: 0.2118s\n",
      "Epoch: 0921 loss_train: 0.0950 f1_train: 0.8806 time: 0.1923s\n",
      "Epoch: 0922 loss_train: 0.0982 f1_train: 0.8768 time: 0.1964s\n",
      "Epoch: 0923 loss_train: 0.0976 f1_train: 0.8789 time: 0.1988s\n",
      "Epoch: 0924 loss_train: 0.0949 f1_train: 0.8829 time: 0.1958s\n",
      "Epoch: 0925 loss_train: 0.0980 f1_train: 0.8806 time: 0.1953s\n",
      "Epoch: 0926 loss_train: 0.0965 f1_train: 0.8826 time: 0.1939s\n",
      "Epoch: 0927 loss_train: 0.0965 f1_train: 0.8765 time: 0.2110s\n",
      "Epoch: 0928 loss_train: 0.0969 f1_train: 0.8792 time: 0.1887s\n",
      "Epoch: 0929 loss_train: 0.0954 f1_train: 0.8850 time: 0.2050s\n",
      "Epoch: 0930 loss_train: 0.0954 f1_train: 0.8811 time: 0.2019s\n",
      "Epoch: 0931 loss_train: 0.0972 f1_train: 0.8807 time: 0.1954s\n",
      "Epoch: 0932 loss_train: 0.0945 f1_train: 0.8775 time: 0.1968s\n",
      "Epoch: 0933 loss_train: 0.0955 f1_train: 0.8831 time: 0.1884s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0934 loss_train: 0.0938 f1_train: 0.8829 time: 0.1989s\n",
      "Epoch: 0935 loss_train: 0.0932 f1_train: 0.8827 time: 0.2158s\n",
      "Epoch: 0936 loss_train: 0.0954 f1_train: 0.8841 time: 0.2006s\n",
      "Epoch: 0937 loss_train: 0.0959 f1_train: 0.8855 time: 0.1940s\n",
      "Epoch: 0938 loss_train: 0.0951 f1_train: 0.8788 time: 0.1948s\n",
      "Epoch: 0939 loss_train: 0.0961 f1_train: 0.8825 time: 0.2047s\n",
      "Epoch: 0940 loss_train: 0.0954 f1_train: 0.8839 time: 0.1901s\n",
      "Epoch: 0941 loss_train: 0.0972 f1_train: 0.8811 time: 0.1956s\n",
      "Epoch: 0942 loss_train: 0.0970 f1_train: 0.8853 time: 0.1885s\n",
      "Epoch: 0943 loss_train: 0.0948 f1_train: 0.8822 time: 0.1979s\n",
      "Epoch: 0944 loss_train: 0.0959 f1_train: 0.8852 time: 0.1893s\n",
      "Epoch: 0945 loss_train: 0.0951 f1_train: 0.8828 time: 0.2070s\n",
      "Epoch: 0946 loss_train: 0.0969 f1_train: 0.8819 time: 0.1956s\n",
      "Epoch: 0947 loss_train: 0.0963 f1_train: 0.8803 time: 0.1936s\n",
      "Epoch: 0948 loss_train: 0.0938 f1_train: 0.8843 time: 0.1879s\n",
      "Epoch: 0949 loss_train: 0.0946 f1_train: 0.8800 time: 0.1966s\n",
      "Epoch: 0950 loss_train: 0.0937 f1_train: 0.8866 time: 0.1954s\n",
      "Epoch: 0951 loss_train: 0.0946 f1_train: 0.8828 time: 0.2107s\n",
      "Epoch: 0952 loss_train: 0.0965 f1_train: 0.8796 time: 0.1899s\n",
      "Epoch: 0953 loss_train: 0.0972 f1_train: 0.8775 time: 0.1948s\n",
      "Epoch: 0954 loss_train: 0.0933 f1_train: 0.8812 time: 0.1943s\n",
      "Epoch: 0955 loss_train: 0.0934 f1_train: 0.8822 time: 0.1973s\n",
      "Epoch: 0956 loss_train: 0.0962 f1_train: 0.8839 time: 0.1908s\n",
      "Epoch: 0957 loss_train: 0.0965 f1_train: 0.8785 time: 0.1945s\n",
      "Epoch: 0958 loss_train: 0.0950 f1_train: 0.8821 time: 0.1886s\n",
      "Epoch: 0959 loss_train: 0.0969 f1_train: 0.8858 time: 0.1947s\n",
      "Epoch: 0960 loss_train: 0.0952 f1_train: 0.8825 time: 0.1888s\n",
      "Epoch: 0961 loss_train: 0.0963 f1_train: 0.8779 time: 0.1989s\n",
      "Epoch: 0962 loss_train: 0.0930 f1_train: 0.8793 time: 0.1907s\n",
      "Epoch: 0963 loss_train: 0.0938 f1_train: 0.8849 time: 0.1950s\n",
      "Epoch: 0964 loss_train: 0.0962 f1_train: 0.8765 time: 0.1881s\n",
      "Epoch: 0965 loss_train: 0.0933 f1_train: 0.8843 time: 0.2231s\n",
      "Epoch: 0966 loss_train: 0.0959 f1_train: 0.8808 time: 0.2454s\n",
      "Epoch: 0967 loss_train: 0.0938 f1_train: 0.8860 time: 0.2416s\n",
      "Epoch: 0968 loss_train: 0.0923 f1_train: 0.8844 time: 0.2436s\n",
      "Epoch: 0969 loss_train: 0.0937 f1_train: 0.8859 time: 0.2376s\n",
      "Epoch: 0970 loss_train: 0.0911 f1_train: 0.8862 time: 0.2472s\n",
      "Epoch: 0971 loss_train: 0.0938 f1_train: 0.8823 time: 0.2473s\n",
      "Epoch: 0972 loss_train: 0.0940 f1_train: 0.8847 time: 0.2384s\n",
      "Epoch: 0973 loss_train: 0.0954 f1_train: 0.8829 time: 0.2581s\n",
      "Epoch: 0974 loss_train: 0.0943 f1_train: 0.8818 time: 0.2461s\n",
      "Epoch: 0975 loss_train: 0.0930 f1_train: 0.8880 time: 0.2364s\n",
      "Epoch: 0976 loss_train: 0.0953 f1_train: 0.8805 time: 0.2310s\n",
      "Epoch: 0977 loss_train: 0.0948 f1_train: 0.8868 time: 0.2530s\n",
      "Epoch: 0978 loss_train: 0.0976 f1_train: 0.8801 time: 0.2396s\n",
      "Epoch: 0979 loss_train: 0.0940 f1_train: 0.8832 time: 0.2462s\n",
      "Epoch: 0980 loss_train: 0.0938 f1_train: 0.8829 time: 0.2451s\n",
      "Epoch: 0981 loss_train: 0.0951 f1_train: 0.8783 time: 0.2521s\n",
      "Epoch: 0982 loss_train: 0.0975 f1_train: 0.8804 time: 0.2364s\n",
      "Epoch: 0983 loss_train: 0.0933 f1_train: 0.8883 time: 0.2371s\n",
      "Epoch: 0984 loss_train: 0.0957 f1_train: 0.8810 time: 0.2287s\n",
      "Epoch: 0985 loss_train: 0.0941 f1_train: 0.8799 time: 0.2410s\n",
      "Epoch: 0986 loss_train: 0.0928 f1_train: 0.8878 time: 0.2340s\n",
      "Epoch: 0987 loss_train: 0.0943 f1_train: 0.8835 time: 0.2278s\n",
      "Epoch: 0988 loss_train: 0.0946 f1_train: 0.8804 time: 0.2496s\n",
      "Epoch: 0989 loss_train: 0.0933 f1_train: 0.8873 time: 0.2527s\n",
      "Epoch: 0990 loss_train: 0.0953 f1_train: 0.8821 time: 0.1885s\n",
      "Epoch: 0991 loss_train: 0.0949 f1_train: 0.8827 time: 0.1944s\n",
      "Epoch: 0992 loss_train: 0.0931 f1_train: 0.8871 time: 0.1936s\n",
      "Epoch: 0993 loss_train: 0.0944 f1_train: 0.8805 time: 0.1919s\n",
      "Epoch: 0994 loss_train: 0.0931 f1_train: 0.8864 time: 0.2054s\n",
      "Epoch: 0995 loss_train: 0.0960 f1_train: 0.8834 time: 0.2416s\n",
      "Epoch: 0996 loss_train: 0.0934 f1_train: 0.8848 time: 0.1945s\n",
      "Epoch: 0997 loss_train: 0.0944 f1_train: 0.8879 time: 0.1938s\n",
      "Epoch: 0998 loss_train: 0.0929 f1_train: 0.8828 time: 0.1890s\n",
      "Epoch: 0999 loss_train: 0.0927 f1_train: 0.8804 time: 0.2184s\n",
      "Epoch: 1000 loss_train: 0.0931 f1_train: 0.8837 time: 0.1927s\n",
      "Optimization Finished!\n",
      "torch.Size([29894, 100])\n"
     ]
    }
   ],
   "source": [
    "# setup training \n",
    "epochs = 1000\n",
    "best_loss = 1\n",
    "node_emb_train = None \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    gcn.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = gcn(features, adj)\n",
    "    loss_train = loss(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    f1_train = fscore(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epoch_finish = time.time() - epoch_start\n",
    "\n",
    "    if best_loss >= loss_train.item():\n",
    "        node_emb_train = gcn.node_embeddings[idx_train]\n",
    "\n",
    "    print(\n",
    "        \"Epoch: {:04d}\".format(epoch+1),\n",
    "        \"loss_train: {:.4f}\".format(loss_train.item()),\n",
    "        \"f1_train: {:.4f}\".format(f1_train),\n",
    "        \"time: {:.4f}s\".format(epoch_finish)\n",
    "    )\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(node_emb_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.3632 precision= 0.6916 recall= 0.5466 f1_test= 0.6106 f1_micro= 0.9547 confusion= [[15323   264]\n",
      " [  491   592]]\n"
     ]
    }
   ],
   "source": [
    "node_emb_test = None \n",
    "gcn.eval()\n",
    "output = gcn(features, adj)\n",
    "loss_test = loss(output[idx_test], labels[idx_test])\n",
    "\n",
    "\n",
    "precision_score = precision(output[idx_test], labels[idx_test])\n",
    "recall_score = recall(output[idx_test], labels[idx_test])\n",
    "f1_test = fscore(output[idx_test], labels[idx_test])\n",
    "f1_micro = fscore_micro(output[idx_test], labels[idx_test])\n",
    "confusion_score = confusion(output[idx_test], labels[idx_test])\n",
    "\n",
    "node_emb_test = gcn.node_embeddings[idx_test]\n",
    "print(\n",
    "    \"Test set results:\",\n",
    "    \"loss= {:.4f}\".format(loss_test.item()),\n",
    "    \"precision= {:.4f}\".format(precision_score),\n",
    "    \"recall= {:.4f}\".format(recall_score),\n",
    "    \"f1_test= {:.4f}\".format(f1_test),\n",
    "    \"f1_micro= {:.4f}\".format(f1_micro),\n",
    "    \"confusion= {}\".format(confusion_score)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.2144, 0.9432,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8666, 0.4166,  ..., 0.0000, 0.0163, 0.0000],\n",
      "        [0.0000, 0.2767, 0.2550,  ..., 0.1394, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.3190, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.4094, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4305, 0.0000,  ..., 1.0562, 0.0000, 0.0000]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "tensor([[0.0000, 0.9012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0121, 0.0000,  ..., 0.7758, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7476, 0.0000,  ..., 0.7062, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.1259, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0824, 0.0000,  ..., 0.8512, 0.6905, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0990, 0.0000, 0.0000]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "[[0.         0.21435647 0.94317627 ... 0.         0.         0.        ]\n",
      " [0.         0.8666105  0.41661337 ... 0.         0.01634093 0.        ]\n",
      " [0.         0.27673155 0.2549694  ... 0.13938369 0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.12590626 0.         0.        ]\n",
      " [0.         1.0824447  0.         ... 0.85117877 0.69049877 0.        ]\n",
      " [0.         0.         0.         ... 0.09902302 0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NE_0</th>\n",
       "      <th>NE_1</th>\n",
       "      <th>NE_2</th>\n",
       "      <th>NE_3</th>\n",
       "      <th>NE_4</th>\n",
       "      <th>NE_5</th>\n",
       "      <th>NE_6</th>\n",
       "      <th>NE_7</th>\n",
       "      <th>NE_8</th>\n",
       "      <th>NE_9</th>\n",
       "      <th>...</th>\n",
       "      <th>NE_90</th>\n",
       "      <th>NE_91</th>\n",
       "      <th>NE_92</th>\n",
       "      <th>NE_93</th>\n",
       "      <th>NE_94</th>\n",
       "      <th>NE_95</th>\n",
       "      <th>NE_96</th>\n",
       "      <th>NE_97</th>\n",
       "      <th>NE_98</th>\n",
       "      <th>NE_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214356</td>\n",
       "      <td>0.943176</td>\n",
       "      <td>0.196256</td>\n",
       "      <td>0.607267</td>\n",
       "      <td>0.361846</td>\n",
       "      <td>1.557223</td>\n",
       "      <td>1.574131</td>\n",
       "      <td>0.209287</td>\n",
       "      <td>0.076003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451862</td>\n",
       "      <td>0.183161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074456</td>\n",
       "      <td>0.190027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866611</td>\n",
       "      <td>0.416613</td>\n",
       "      <td>0.025965</td>\n",
       "      <td>0.743440</td>\n",
       "      <td>0.054661</td>\n",
       "      <td>0.769524</td>\n",
       "      <td>1.446789</td>\n",
       "      <td>0.445361</td>\n",
       "      <td>0.033966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.858419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005393</td>\n",
       "      <td>0.021094</td>\n",
       "      <td>0.277284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276732</td>\n",
       "      <td>0.254969</td>\n",
       "      <td>0.551423</td>\n",
       "      <td>0.247976</td>\n",
       "      <td>0.187643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157053</td>\n",
       "      <td>0.254969</td>\n",
       "      <td>1.115224</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.187643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.360247</td>\n",
       "      <td>0.035661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.852650</td>\n",
       "      <td>0.364693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.646550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.867769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.270580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.101321</td>\n",
       "      <td>4.461850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46559</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654668</td>\n",
       "      <td>0.221343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.722984</td>\n",
       "      <td>0.118735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46560</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552214</td>\n",
       "      <td>0.095016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122833</td>\n",
       "      <td>0.504956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46561</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115867</td>\n",
       "      <td>0.214168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335576</td>\n",
       "      <td>0.125906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46562</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.082445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070579</td>\n",
       "      <td>0.608728</td>\n",
       "      <td>0.727974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.096430</td>\n",
       "      <td>0.279903</td>\n",
       "      <td>...</td>\n",
       "      <td>1.106855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.757327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116640</td>\n",
       "      <td>0.851179</td>\n",
       "      <td>0.690499</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46563</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115867</td>\n",
       "      <td>0.214168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125926</td>\n",
       "      <td>0.099023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46564 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NE_0      NE_1      NE_2      NE_3      NE_4      NE_5      NE_6  \\\n",
       "0       0.0  0.214356  0.943176  0.196256  0.607267  0.361846  1.557223   \n",
       "1       0.0  0.866611  0.416613  0.025965  0.743440  0.054661  0.769524   \n",
       "2       0.0  0.276732  0.254969  0.551423  0.247976  0.187643  0.000000   \n",
       "3       0.0  0.157053  0.254969  1.115224  0.248506  0.187643  0.000000   \n",
       "4       0.0  0.852650  0.364693  0.000000  3.646550  0.000000  0.000000   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "46559   0.0  0.000000  0.000000  0.000000  0.000000  0.381118  0.000000   \n",
       "46560   0.0  0.000000  0.000000  0.000000  0.000000  0.572999  0.000000   \n",
       "46561   0.0  0.000000  0.000000  0.000000  0.000000  0.522394  0.000000   \n",
       "46562   0.0  1.082445  0.000000  0.070579  0.608728  0.727974  0.000000   \n",
       "46563   0.0  0.000000  0.000000  0.000000  0.000000  0.531046  0.000000   \n",
       "\n",
       "           NE_7      NE_8      NE_9  ...     NE_90  NE_91     NE_92     NE_93  \\\n",
       "0      1.574131  0.209287  0.076003  ...  0.000000    0.0  0.451862  0.183161   \n",
       "1      1.446789  0.445361  0.033966  ...  0.091498    0.0  0.858419  0.000000   \n",
       "2      0.000000  0.000000  0.000000  ...  0.543029    0.0  0.000000  0.030060   \n",
       "3      0.000000  0.000000  0.036924  ...  0.443598    0.0  0.360247  0.035661   \n",
       "4      6.867769  0.000000  1.270580  ...  0.246011    0.0  1.101321  4.461850   \n",
       "...         ...       ...       ...  ...       ...    ...       ...       ...   \n",
       "46559  0.000000  0.654668  0.221343  ...  0.000000    0.0  0.000000  0.000000   \n",
       "46560  0.000000  0.552214  0.095016  ...  0.000000    0.0  0.000000  0.000000   \n",
       "46561  0.000000  0.115867  0.214168  ...  0.142971    0.0  0.000000  0.378454   \n",
       "46562  0.000000  1.096430  0.279903  ...  1.106855    0.0  0.000000  0.757327   \n",
       "46563  0.000000  0.115867  0.214168  ...  0.142971    0.0  0.000000  0.492652   \n",
       "\n",
       "          NE_94     NE_95     NE_96     NE_97     NE_98  NE_99  \n",
       "0      0.000000  0.074456  0.190027  0.000000  0.000000    0.0  \n",
       "1      0.005393  0.021094  0.277284  0.000000  0.016341    0.0  \n",
       "2      0.000000  0.000000  0.000000  0.139384  0.000000    0.0  \n",
       "3      0.000000  0.000000  0.000000  0.139384  0.000000    0.0  \n",
       "4      0.000000  0.000000  0.000000  0.600567  0.000000    0.0  \n",
       "...         ...       ...       ...       ...       ...    ...  \n",
       "46559  0.000000  0.000000  0.722984  0.118735  0.000000    0.0  \n",
       "46560  0.000000  0.122833  0.504956  0.000000  0.000000    0.0  \n",
       "46561  0.000000  0.000000  0.335576  0.125906  0.000000    0.0  \n",
       "46562  0.000000  0.000000  0.116640  0.851179  0.690499    0.0  \n",
       "46563  0.000000  0.000000  0.125926  0.099023  0.000000    0.0  \n",
       "\n",
       "[46564 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(node_emb_train)\n",
    "print(node_emb_test)\n",
    "\n",
    "np_node_emb = np.concatenate((node_emb_train.cpu().detach().numpy(), \n",
    "                            node_emb_test.cpu().detach().numpy()))\n",
    "\n",
    "print(np_node_emb)\n",
    "\n",
    "# Create embeddings pandas DataFrame \n",
    "node_emb_pd = pd.DataFrame(np_node_emb) \n",
    "node_emb_pd.columns = [f\"NE_{i}\" for i in range(np_node_emb.shape[1])]\n",
    "display(node_emb_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[232438397 232029206 232344069 ... 158375075 147478192 158375402]\n"
     ]
    }
   ],
   "source": [
    "data = ellipticdr.dataset_.copy()\n",
    "txIds = data[(data[\"class\"] != -1)][\"txId\"].values\n",
    "node_emb_pd.insert(0, \"txId\", txIds)\n",
    "print(txIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>NE_0</th>\n",
       "      <th>NE_1</th>\n",
       "      <th>NE_2</th>\n",
       "      <th>NE_3</th>\n",
       "      <th>NE_4</th>\n",
       "      <th>NE_5</th>\n",
       "      <th>NE_6</th>\n",
       "      <th>NE_7</th>\n",
       "      <th>NE_8</th>\n",
       "      <th>...</th>\n",
       "      <th>NE_90</th>\n",
       "      <th>NE_91</th>\n",
       "      <th>NE_92</th>\n",
       "      <th>NE_93</th>\n",
       "      <th>NE_94</th>\n",
       "      <th>NE_95</th>\n",
       "      <th>NE_96</th>\n",
       "      <th>NE_97</th>\n",
       "      <th>NE_98</th>\n",
       "      <th>NE_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232438397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214356</td>\n",
       "      <td>0.943176</td>\n",
       "      <td>0.196256</td>\n",
       "      <td>0.607267</td>\n",
       "      <td>0.361846</td>\n",
       "      <td>1.557223</td>\n",
       "      <td>1.574131</td>\n",
       "      <td>0.209287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451862</td>\n",
       "      <td>0.183161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074456</td>\n",
       "      <td>0.190027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232029206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866611</td>\n",
       "      <td>0.416613</td>\n",
       "      <td>0.025965</td>\n",
       "      <td>0.743440</td>\n",
       "      <td>0.054661</td>\n",
       "      <td>0.769524</td>\n",
       "      <td>1.446789</td>\n",
       "      <td>0.445361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.858419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005393</td>\n",
       "      <td>0.021094</td>\n",
       "      <td>0.277284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232344069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276732</td>\n",
       "      <td>0.254969</td>\n",
       "      <td>0.551423</td>\n",
       "      <td>0.247976</td>\n",
       "      <td>0.187643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27553029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157053</td>\n",
       "      <td>0.254969</td>\n",
       "      <td>1.115224</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.187643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.360247</td>\n",
       "      <td>0.035661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3881097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.852650</td>\n",
       "      <td>0.364693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.646550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.867769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.101321</td>\n",
       "      <td>4.461850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46559</th>\n",
       "      <td>80329479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.722984</td>\n",
       "      <td>0.118735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46560</th>\n",
       "      <td>158406298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122833</td>\n",
       "      <td>0.504956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46561</th>\n",
       "      <td>158375075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335576</td>\n",
       "      <td>0.125906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46562</th>\n",
       "      <td>147478192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.082445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070579</td>\n",
       "      <td>0.608728</td>\n",
       "      <td>0.727974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.096430</td>\n",
       "      <td>...</td>\n",
       "      <td>1.106855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.757327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116640</td>\n",
       "      <td>0.851179</td>\n",
       "      <td>0.690499</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46563</th>\n",
       "      <td>158375402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125926</td>\n",
       "      <td>0.099023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46564 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            txId  NE_0      NE_1      NE_2      NE_3      NE_4      NE_5  \\\n",
       "0      232438397   0.0  0.214356  0.943176  0.196256  0.607267  0.361846   \n",
       "1      232029206   0.0  0.866611  0.416613  0.025965  0.743440  0.054661   \n",
       "2      232344069   0.0  0.276732  0.254969  0.551423  0.247976  0.187643   \n",
       "3       27553029   0.0  0.157053  0.254969  1.115224  0.248506  0.187643   \n",
       "4        3881097   0.0  0.852650  0.364693  0.000000  3.646550  0.000000   \n",
       "...          ...   ...       ...       ...       ...       ...       ...   \n",
       "46559   80329479   0.0  0.000000  0.000000  0.000000  0.000000  0.381118   \n",
       "46560  158406298   0.0  0.000000  0.000000  0.000000  0.000000  0.572999   \n",
       "46561  158375075   0.0  0.000000  0.000000  0.000000  0.000000  0.522394   \n",
       "46562  147478192   0.0  1.082445  0.000000  0.070579  0.608728  0.727974   \n",
       "46563  158375402   0.0  0.000000  0.000000  0.000000  0.000000  0.531046   \n",
       "\n",
       "           NE_6      NE_7      NE_8  ...     NE_90  NE_91     NE_92     NE_93  \\\n",
       "0      1.557223  1.574131  0.209287  ...  0.000000    0.0  0.451862  0.183161   \n",
       "1      0.769524  1.446789  0.445361  ...  0.091498    0.0  0.858419  0.000000   \n",
       "2      0.000000  0.000000  0.000000  ...  0.543029    0.0  0.000000  0.030060   \n",
       "3      0.000000  0.000000  0.000000  ...  0.443598    0.0  0.360247  0.035661   \n",
       "4      0.000000  6.867769  0.000000  ...  0.246011    0.0  1.101321  4.461850   \n",
       "...         ...       ...       ...  ...       ...    ...       ...       ...   \n",
       "46559  0.000000  0.000000  0.654668  ...  0.000000    0.0  0.000000  0.000000   \n",
       "46560  0.000000  0.000000  0.552214  ...  0.000000    0.0  0.000000  0.000000   \n",
       "46561  0.000000  0.000000  0.115867  ...  0.142971    0.0  0.000000  0.378454   \n",
       "46562  0.000000  0.000000  1.096430  ...  1.106855    0.0  0.000000  0.757327   \n",
       "46563  0.000000  0.000000  0.115867  ...  0.142971    0.0  0.000000  0.492652   \n",
       "\n",
       "          NE_94     NE_95     NE_96     NE_97     NE_98  NE_99  \n",
       "0      0.000000  0.074456  0.190027  0.000000  0.000000    0.0  \n",
       "1      0.005393  0.021094  0.277284  0.000000  0.016341    0.0  \n",
       "2      0.000000  0.000000  0.000000  0.139384  0.000000    0.0  \n",
       "3      0.000000  0.000000  0.000000  0.139384  0.000000    0.0  \n",
       "4      0.000000  0.000000  0.000000  0.600567  0.000000    0.0  \n",
       "...         ...       ...       ...       ...       ...    ...  \n",
       "46559  0.000000  0.000000  0.722984  0.118735  0.000000    0.0  \n",
       "46560  0.000000  0.122833  0.504956  0.000000  0.000000    0.0  \n",
       "46561  0.000000  0.000000  0.335576  0.125906  0.000000    0.0  \n",
       "46562  0.000000  0.000000  0.116640  0.851179  0.690499    0.0  \n",
       "46563  0.000000  0.000000  0.125926  0.099023  0.000000    0.0  \n",
       "\n",
       "[46564 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(node_emb_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_emb_pd.to_csv(\"elliptic_embs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('btc-classifier': conda)",
   "language": "python",
   "name": "python37664bitbtcclassifiercondaf328939486114fc0aeb107830f867d66"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
