{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import cryptoaml.datareader as cdr\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipticdr = cdr.get_data(\"elliptic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN model \n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_features, \n",
    "                 out_features, \n",
    "                 bias=True):\n",
    "        \n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 nfeat, \n",
    "                 nhid, \n",
    "                 nclass, \n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        # https://github.com/tkipf/pygcn/issues/26#issuecomment-435801483\n",
    "        # \"In this case itâ€™s best to simply take the embeddings just before doing \n",
    "        # the last linear projection to the softmax logits. \n",
    "        # In other words, if the last layer is softmax(A*H*W), \n",
    "        # take either the embedding H directly or A*H.\"\n",
    "       \n",
    "        # extract node embeddings (A*H)\n",
    "        self.node_embeddings = torch.mm(adj, x)\n",
    "        \n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def load_elliptic(datareader):\n",
    "    \n",
    "    # get all labels  \n",
    "    data = datareader.dataset_.copy()\n",
    "    labelled_data = data[(data[\"class\"] != -1)]\n",
    "    \n",
    "    # get features \n",
    "    feature_cols = [\"txId\"] + datareader.feature_cols_AF_\n",
    "    labelled_features = labelled_data[feature_cols].copy()\n",
    "    labelled_features.set_index(\"txId\", inplace=True) \n",
    "    features = sp.csr_matrix(labelled_features.values, dtype=np.float32)    \n",
    "    \n",
    "    # build edges \n",
    "    tx_ids = labelled_features.index\n",
    "    idx_map = {j: i for i, j in enumerate(tx_ids)}\n",
    "    edges_unordered = datareader.edges_.copy() \n",
    "    edges_unordered = edges_unordered[(edges_unordered[\"txId2\"].isin(set(tx_ids))) & \n",
    "                               (edges_unordered[\"txId1\"].isin(set(tx_ids)))].values\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)    \n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                    shape=(labelled_features.shape[0], labelled_features.shape[0]),\n",
    "                    dtype=np.float32)\n",
    "    \n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    \n",
    "    # get idx for train and test \n",
    "    total_train = labelled_data[labelled_data[\"ts\"] <= 34].shape[0]\n",
    "    idx_train = range(total_train)\n",
    "    idx_train = torch.LongTensor(idx_train)   \n",
    "    total_test = labelled_data[labelled_data[\"ts\"] > 34].shape[0]\n",
    "    idx_test = range(total_train, total_train+total_test)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    # change data to torch tensors \n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labelled_data[\"class\"].values)\n",
    "    \n",
    "    return adj, features, labels, idx_train, idx_test\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def precision(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return precision_score(labels, preds, average=\"binary\")\n",
    "\n",
    "def recall(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return recall_score(labels, preds, average=\"binary\")\n",
    "\n",
    "def fscore_micro(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return f1_score(labels, preds, average=\"micro\")\n",
    "\n",
    "def fscore(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return f1_score(labels, preds, average=\"binary\")\n",
    "\n",
    "def confusion(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setup \n",
    "\n",
    "# build graph data \n",
    "adj, features, labels, idx_train, idx_test = load_elliptic(ellipticdr)\n",
    "\n",
    "# model \n",
    "n_classes = 2\n",
    "n_features = 166\n",
    "n_hidden = 100\n",
    "dropout = 0.5\n",
    "gcn = GCN(nfeat=n_features,\n",
    "          nhid=n_hidden,\n",
    "          nclass=n_classes,\n",
    "          dropout=dropout)\n",
    "\n",
    "# optimizer \n",
    "learning_rate = 0.001\n",
    "gcn_params = gcn.parameters()\n",
    "optimizer = optim.Adam(gcn_params,\n",
    "                       lr=learning_rate)\n",
    "weight_ratio = torch.FloatTensor([0.3, 0.7])\n",
    "loss = nn.CrossEntropyLoss(weight=weight_ratio)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     gcn.cuda()\n",
    "#     features = features.cuda()\n",
    "#     adj = adj.cuda()\n",
    "#     labels = labels.cuda()\n",
    "#     idx_train = idx_train.cuda()\n",
    "#     idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9781 f1_train: 0.1001 time: 0.2333s\n",
      "Epoch: 0002 loss_train: 1.6039 f1_train: 0.1449 time: 0.2463s\n",
      "Epoch: 0003 loss_train: 1.4025 f1_train: 0.1989 time: 0.2331s\n",
      "Epoch: 0004 loss_train: 1.2137 f1_train: 0.2480 time: 0.2505s\n",
      "Epoch: 0005 loss_train: 1.1218 f1_train: 0.2907 time: 0.3513s\n",
      "Epoch: 0006 loss_train: 1.0397 f1_train: 0.3398 time: 0.2986s\n",
      "Epoch: 0007 loss_train: 0.9964 f1_train: 0.3623 time: 0.2270s\n",
      "Epoch: 0008 loss_train: 0.9194 f1_train: 0.4015 time: 0.2804s\n",
      "Epoch: 0009 loss_train: 0.9251 f1_train: 0.4046 time: 0.1772s\n",
      "Epoch: 0010 loss_train: 0.9189 f1_train: 0.4185 time: 0.2256s\n",
      "Epoch: 0011 loss_train: 0.8784 f1_train: 0.4371 time: 0.2223s\n",
      "Epoch: 0012 loss_train: 0.8309 f1_train: 0.4567 time: 0.2419s\n",
      "Epoch: 0013 loss_train: 0.8481 f1_train: 0.4581 time: 0.3530s\n",
      "Epoch: 0014 loss_train: 0.8174 f1_train: 0.4703 time: 0.2185s\n",
      "Epoch: 0015 loss_train: 0.7955 f1_train: 0.4841 time: 0.2909s\n",
      "Epoch: 0016 loss_train: 0.8044 f1_train: 0.4856 time: 0.2967s\n",
      "Epoch: 0017 loss_train: 0.7418 f1_train: 0.4952 time: 0.1755s\n",
      "Epoch: 0018 loss_train: 0.7648 f1_train: 0.4959 time: 0.1837s\n",
      "Epoch: 0019 loss_train: 0.7217 f1_train: 0.5039 time: 0.1757s\n",
      "Epoch: 0020 loss_train: 0.7234 f1_train: 0.5052 time: 0.1832s\n",
      "Epoch: 0021 loss_train: 0.6932 f1_train: 0.5162 time: 0.1845s\n",
      "Epoch: 0022 loss_train: 0.6763 f1_train: 0.5184 time: 0.1934s\n",
      "Epoch: 0023 loss_train: 0.6632 f1_train: 0.5295 time: 0.1774s\n",
      "Epoch: 0024 loss_train: 0.6495 f1_train: 0.5325 time: 0.1981s\n",
      "Epoch: 0025 loss_train: 0.6516 f1_train: 0.5375 time: 0.1765s\n",
      "Epoch: 0026 loss_train: 0.6735 f1_train: 0.5240 time: 0.1802s\n",
      "Epoch: 0027 loss_train: 0.6309 f1_train: 0.5428 time: 0.1750s\n",
      "Epoch: 0028 loss_train: 0.6244 f1_train: 0.5463 time: 0.2004s\n",
      "Epoch: 0029 loss_train: 0.6218 f1_train: 0.5437 time: 0.1819s\n",
      "Epoch: 0030 loss_train: 0.5832 f1_train: 0.5538 time: 0.1813s\n",
      "Epoch: 0031 loss_train: 0.5661 f1_train: 0.5622 time: 0.1769s\n",
      "Epoch: 0032 loss_train: 0.5748 f1_train: 0.5701 time: 0.1806s\n",
      "Epoch: 0033 loss_train: 0.5523 f1_train: 0.5689 time: 0.1751s\n",
      "Epoch: 0034 loss_train: 0.5699 f1_train: 0.5556 time: 0.1810s\n",
      "Epoch: 0035 loss_train: 0.5643 f1_train: 0.5658 time: 0.1750s\n",
      "Epoch: 0036 loss_train: 0.5316 f1_train: 0.5705 time: 0.1798s\n",
      "Epoch: 0037 loss_train: 0.5186 f1_train: 0.5847 time: 0.1760s\n",
      "Epoch: 0038 loss_train: 0.5237 f1_train: 0.5830 time: 0.1835s\n",
      "Epoch: 0039 loss_train: 0.5053 f1_train: 0.5834 time: 0.1751s\n",
      "Epoch: 0040 loss_train: 0.4869 f1_train: 0.5919 time: 0.1847s\n",
      "Epoch: 0041 loss_train: 0.4922 f1_train: 0.5848 time: 0.1744s\n",
      "Epoch: 0042 loss_train: 0.4874 f1_train: 0.5940 time: 0.1814s\n",
      "Epoch: 0043 loss_train: 0.4879 f1_train: 0.5883 time: 0.1928s\n",
      "Epoch: 0044 loss_train: 0.4760 f1_train: 0.5877 time: 0.1760s\n",
      "Epoch: 0045 loss_train: 0.4537 f1_train: 0.5992 time: 0.1752s\n",
      "Epoch: 0046 loss_train: 0.4633 f1_train: 0.5972 time: 0.2044s\n",
      "Epoch: 0047 loss_train: 0.4537 f1_train: 0.6036 time: 0.1784s\n",
      "Epoch: 0048 loss_train: 0.4447 f1_train: 0.6035 time: 0.1871s\n",
      "Epoch: 0049 loss_train: 0.4348 f1_train: 0.6146 time: 0.1829s\n",
      "Epoch: 0050 loss_train: 0.4413 f1_train: 0.6121 time: 0.1794s\n",
      "Epoch: 0051 loss_train: 0.4396 f1_train: 0.6098 time: 0.1727s\n",
      "Epoch: 0052 loss_train: 0.4064 f1_train: 0.6183 time: 0.1937s\n",
      "Epoch: 0053 loss_train: 0.4006 f1_train: 0.6282 time: 0.1749s\n",
      "Epoch: 0054 loss_train: 0.4114 f1_train: 0.6185 time: 0.1822s\n",
      "Epoch: 0055 loss_train: 0.4017 f1_train: 0.6320 time: 0.1741s\n",
      "Epoch: 0056 loss_train: 0.3928 f1_train: 0.6332 time: 0.1829s\n",
      "Epoch: 0057 loss_train: 0.3866 f1_train: 0.6299 time: 0.1724s\n",
      "Epoch: 0058 loss_train: 0.4015 f1_train: 0.6242 time: 0.1830s\n",
      "Epoch: 0059 loss_train: 0.3778 f1_train: 0.6334 time: 0.1731s\n",
      "Epoch: 0060 loss_train: 0.3705 f1_train: 0.6432 time: 0.1836s\n",
      "Epoch: 0061 loss_train: 0.3713 f1_train: 0.6422 time: 0.1752s\n",
      "Epoch: 0062 loss_train: 0.3618 f1_train: 0.6454 time: 0.1855s\n",
      "Epoch: 0063 loss_train: 0.3728 f1_train: 0.6377 time: 0.1758s\n",
      "Epoch: 0064 loss_train: 0.3545 f1_train: 0.6496 time: 0.1823s\n",
      "Epoch: 0065 loss_train: 0.3487 f1_train: 0.6525 time: 0.1743s\n",
      "Epoch: 0066 loss_train: 0.3470 f1_train: 0.6612 time: 0.1827s\n",
      "Epoch: 0067 loss_train: 0.3363 f1_train: 0.6615 time: 0.1759s\n",
      "Epoch: 0068 loss_train: 0.3247 f1_train: 0.6741 time: 0.1896s\n",
      "Epoch: 0069 loss_train: 0.3352 f1_train: 0.6594 time: 0.1843s\n",
      "Epoch: 0070 loss_train: 0.3312 f1_train: 0.6654 time: 0.1837s\n",
      "Epoch: 0071 loss_train: 0.3160 f1_train: 0.6729 time: 0.1762s\n",
      "Epoch: 0072 loss_train: 0.3186 f1_train: 0.6810 time: 0.1911s\n",
      "Epoch: 0073 loss_train: 0.3181 f1_train: 0.6702 time: 0.1733s\n",
      "Epoch: 0074 loss_train: 0.3150 f1_train: 0.6772 time: 0.1851s\n",
      "Epoch: 0075 loss_train: 0.3003 f1_train: 0.6801 time: 0.1744s\n",
      "Epoch: 0076 loss_train: 0.2938 f1_train: 0.6782 time: 0.1836s\n",
      "Epoch: 0077 loss_train: 0.2924 f1_train: 0.6926 time: 0.1754s\n",
      "Epoch: 0078 loss_train: 0.2945 f1_train: 0.6915 time: 0.1872s\n",
      "Epoch: 0079 loss_train: 0.2852 f1_train: 0.6898 time: 0.1812s\n",
      "Epoch: 0080 loss_train: 0.2848 f1_train: 0.6976 time: 0.1901s\n",
      "Epoch: 0081 loss_train: 0.2908 f1_train: 0.6841 time: 0.1753s\n",
      "Epoch: 0082 loss_train: 0.2860 f1_train: 0.6930 time: 0.1910s\n",
      "Epoch: 0083 loss_train: 0.2811 f1_train: 0.6887 time: 0.1824s\n",
      "Epoch: 0084 loss_train: 0.2765 f1_train: 0.6951 time: 0.1909s\n",
      "Epoch: 0085 loss_train: 0.2733 f1_train: 0.7012 time: 0.1740s\n",
      "Epoch: 0086 loss_train: 0.2670 f1_train: 0.6983 time: 0.1827s\n",
      "Epoch: 0087 loss_train: 0.2606 f1_train: 0.7088 time: 0.1734s\n",
      "Epoch: 0088 loss_train: 0.2647 f1_train: 0.7101 time: 0.1812s\n",
      "Epoch: 0089 loss_train: 0.2668 f1_train: 0.7052 time: 0.1776s\n",
      "Epoch: 0090 loss_train: 0.2533 f1_train: 0.7140 time: 0.1827s\n",
      "Epoch: 0091 loss_train: 0.2566 f1_train: 0.7081 time: 0.1743s\n",
      "Epoch: 0092 loss_train: 0.2552 f1_train: 0.7168 time: 0.1822s\n",
      "Epoch: 0093 loss_train: 0.2538 f1_train: 0.7101 time: 0.1737s\n",
      "Epoch: 0094 loss_train: 0.2407 f1_train: 0.7269 time: 0.1855s\n",
      "Epoch: 0095 loss_train: 0.2497 f1_train: 0.7190 time: 0.1745s\n",
      "Epoch: 0096 loss_train: 0.2431 f1_train: 0.7276 time: 0.1828s\n",
      "Epoch: 0097 loss_train: 0.2419 f1_train: 0.7251 time: 0.1737s\n",
      "Epoch: 0098 loss_train: 0.2450 f1_train: 0.7178 time: 0.1962s\n",
      "Epoch: 0099 loss_train: 0.2361 f1_train: 0.7305 time: 0.1782s\n",
      "Epoch: 0100 loss_train: 0.2411 f1_train: 0.7210 time: 0.2003s\n",
      "Epoch: 0101 loss_train: 0.2307 f1_train: 0.7366 time: 0.1791s\n",
      "Epoch: 0102 loss_train: 0.2318 f1_train: 0.7222 time: 0.1806s\n",
      "Epoch: 0103 loss_train: 0.2305 f1_train: 0.7304 time: 0.1749s\n",
      "Epoch: 0104 loss_train: 0.2245 f1_train: 0.7352 time: 0.1980s\n",
      "Epoch: 0105 loss_train: 0.2322 f1_train: 0.7321 time: 0.1844s\n",
      "Epoch: 0106 loss_train: 0.2221 f1_train: 0.7369 time: 0.1862s\n",
      "Epoch: 0107 loss_train: 0.2227 f1_train: 0.7415 time: 0.1756s\n",
      "Epoch: 0108 loss_train: 0.2220 f1_train: 0.7403 time: 0.1863s\n",
      "Epoch: 0109 loss_train: 0.2225 f1_train: 0.7428 time: 0.1753s\n",
      "Epoch: 0110 loss_train: 0.2224 f1_train: 0.7367 time: 0.1855s\n",
      "Epoch: 0111 loss_train: 0.2106 f1_train: 0.7476 time: 0.1754s\n",
      "Epoch: 0112 loss_train: 0.2100 f1_train: 0.7492 time: 0.1841s\n",
      "Epoch: 0113 loss_train: 0.2097 f1_train: 0.7501 time: 0.1757s\n",
      "Epoch: 0114 loss_train: 0.2103 f1_train: 0.7507 time: 0.1831s\n",
      "Epoch: 0115 loss_train: 0.2102 f1_train: 0.7467 time: 0.1757s\n",
      "Epoch: 0116 loss_train: 0.2087 f1_train: 0.7491 time: 0.1822s\n",
      "Epoch: 0117 loss_train: 0.2066 f1_train: 0.7512 time: 0.1807s\n",
      "Epoch: 0118 loss_train: 0.2043 f1_train: 0.7565 time: 0.1818s\n",
      "Epoch: 0119 loss_train: 0.2037 f1_train: 0.7613 time: 0.1741s\n",
      "Epoch: 0120 loss_train: 0.2010 f1_train: 0.7585 time: 0.1835s\n",
      "Epoch: 0121 loss_train: 0.1993 f1_train: 0.7611 time: 0.1744s\n",
      "Epoch: 0122 loss_train: 0.1986 f1_train: 0.7576 time: 0.1820s\n",
      "Epoch: 0123 loss_train: 0.1944 f1_train: 0.7604 time: 0.1969s\n",
      "Epoch: 0124 loss_train: 0.1988 f1_train: 0.7563 time: 0.1759s\n",
      "Epoch: 0125 loss_train: 0.1924 f1_train: 0.7666 time: 0.1842s\n",
      "Epoch: 0126 loss_train: 0.1902 f1_train: 0.7721 time: 0.1814s\n",
      "Epoch: 0127 loss_train: 0.1941 f1_train: 0.7671 time: 0.1747s\n",
      "Epoch: 0128 loss_train: 0.1886 f1_train: 0.7701 time: 0.1974s\n",
      "Epoch: 0129 loss_train: 0.1863 f1_train: 0.7705 time: 0.1760s\n",
      "Epoch: 0130 loss_train: 0.1880 f1_train: 0.7713 time: 0.1849s\n",
      "Epoch: 0131 loss_train: 0.1857 f1_train: 0.7723 time: 0.1757s\n",
      "Epoch: 0132 loss_train: 0.1860 f1_train: 0.7721 time: 0.1912s\n",
      "Epoch: 0133 loss_train: 0.1853 f1_train: 0.7747 time: 0.1784s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0134 loss_train: 0.1829 f1_train: 0.7720 time: 0.1844s\n",
      "Epoch: 0135 loss_train: 0.1849 f1_train: 0.7727 time: 0.1874s\n",
      "Epoch: 0136 loss_train: 0.1822 f1_train: 0.7840 time: 0.1753s\n",
      "Epoch: 0137 loss_train: 0.1831 f1_train: 0.7836 time: 0.1839s\n",
      "Epoch: 0138 loss_train: 0.1839 f1_train: 0.7758 time: 0.1761s\n",
      "Epoch: 0139 loss_train: 0.1826 f1_train: 0.7778 time: 0.1792s\n",
      "Epoch: 0140 loss_train: 0.1809 f1_train: 0.7774 time: 0.1794s\n",
      "Epoch: 0141 loss_train: 0.1844 f1_train: 0.7814 time: 0.1925s\n",
      "Epoch: 0142 loss_train: 0.1785 f1_train: 0.7841 time: 0.1787s\n",
      "Epoch: 0143 loss_train: 0.1744 f1_train: 0.7881 time: 0.1747s\n",
      "Epoch: 0144 loss_train: 0.1756 f1_train: 0.7851 time: 0.1794s\n",
      "Epoch: 0145 loss_train: 0.1761 f1_train: 0.7855 time: 0.1762s\n",
      "Epoch: 0146 loss_train: 0.1706 f1_train: 0.7911 time: 0.1784s\n",
      "Epoch: 0147 loss_train: 0.1789 f1_train: 0.7871 time: 0.1837s\n",
      "Epoch: 0148 loss_train: 0.1725 f1_train: 0.7925 time: 0.1845s\n",
      "Epoch: 0149 loss_train: 0.1752 f1_train: 0.7845 time: 0.1794s\n",
      "Epoch: 0150 loss_train: 0.1707 f1_train: 0.7879 time: 0.1775s\n",
      "Epoch: 0151 loss_train: 0.1740 f1_train: 0.7903 time: 0.1748s\n",
      "Epoch: 0152 loss_train: 0.1687 f1_train: 0.7920 time: 0.1906s\n",
      "Epoch: 0153 loss_train: 0.1701 f1_train: 0.7888 time: 0.1802s\n",
      "Epoch: 0154 loss_train: 0.1714 f1_train: 0.7860 time: 0.1842s\n",
      "Epoch: 0155 loss_train: 0.1678 f1_train: 0.7926 time: 0.1737s\n",
      "Epoch: 0156 loss_train: 0.1682 f1_train: 0.7959 time: 0.1801s\n",
      "Epoch: 0157 loss_train: 0.1687 f1_train: 0.7868 time: 0.1768s\n",
      "Epoch: 0158 loss_train: 0.1649 f1_train: 0.8006 time: 0.1801s\n",
      "Epoch: 0159 loss_train: 0.1687 f1_train: 0.7937 time: 0.1935s\n",
      "Epoch: 0160 loss_train: 0.1645 f1_train: 0.8010 time: 0.1800s\n",
      "Epoch: 0161 loss_train: 0.1657 f1_train: 0.8020 time: 0.1874s\n",
      "Epoch: 0162 loss_train: 0.1647 f1_train: 0.8017 time: 0.1754s\n",
      "Epoch: 0163 loss_train: 0.1635 f1_train: 0.7990 time: 0.1751s\n",
      "Epoch: 0164 loss_train: 0.1584 f1_train: 0.8053 time: 0.1799s\n",
      "Epoch: 0165 loss_train: 0.1612 f1_train: 0.8022 time: 0.1759s\n",
      "Epoch: 0166 loss_train: 0.1675 f1_train: 0.7919 time: 0.1791s\n",
      "Epoch: 0167 loss_train: 0.1613 f1_train: 0.8010 time: 0.1740s\n",
      "Epoch: 0168 loss_train: 0.1579 f1_train: 0.8073 time: 0.1801s\n",
      "Epoch: 0169 loss_train: 0.1615 f1_train: 0.8043 time: 0.1759s\n",
      "Epoch: 0170 loss_train: 0.1585 f1_train: 0.8042 time: 0.1795s\n",
      "Epoch: 0171 loss_train: 0.1604 f1_train: 0.8039 time: 0.1763s\n",
      "Epoch: 0172 loss_train: 0.1634 f1_train: 0.7981 time: 0.1790s\n",
      "Epoch: 0173 loss_train: 0.1540 f1_train: 0.8082 time: 0.1766s\n",
      "Epoch: 0174 loss_train: 0.1575 f1_train: 0.8044 time: 0.1795s\n",
      "Epoch: 0175 loss_train: 0.1563 f1_train: 0.8021 time: 0.1736s\n",
      "Epoch: 0176 loss_train: 0.1608 f1_train: 0.7989 time: 0.1805s\n",
      "Epoch: 0177 loss_train: 0.1586 f1_train: 0.8043 time: 0.1754s\n",
      "Epoch: 0178 loss_train: 0.1563 f1_train: 0.8060 time: 0.1872s\n",
      "Epoch: 0179 loss_train: 0.1558 f1_train: 0.8060 time: 0.1757s\n",
      "Epoch: 0180 loss_train: 0.1547 f1_train: 0.8099 time: 0.1903s\n",
      "Epoch: 0181 loss_train: 0.1557 f1_train: 0.8093 time: 0.1845s\n",
      "Epoch: 0182 loss_train: 0.1532 f1_train: 0.8115 time: 0.1781s\n",
      "Epoch: 0183 loss_train: 0.1533 f1_train: 0.8057 time: 0.1757s\n",
      "Epoch: 0184 loss_train: 0.1538 f1_train: 0.8049 time: 0.1819s\n",
      "Epoch: 0185 loss_train: 0.1515 f1_train: 0.8131 time: 0.1747s\n",
      "Epoch: 0186 loss_train: 0.1530 f1_train: 0.8124 time: 0.1851s\n",
      "Epoch: 0187 loss_train: 0.1539 f1_train: 0.8089 time: 0.1743s\n",
      "Epoch: 0188 loss_train: 0.1516 f1_train: 0.8168 time: 0.1794s\n",
      "Epoch: 0189 loss_train: 0.1512 f1_train: 0.8159 time: 0.1737s\n",
      "Epoch: 0190 loss_train: 0.1481 f1_train: 0.8144 time: 0.1851s\n",
      "Epoch: 0191 loss_train: 0.1510 f1_train: 0.8139 time: 0.1746s\n",
      "Epoch: 0192 loss_train: 0.1508 f1_train: 0.8159 time: 0.1970s\n",
      "Epoch: 0193 loss_train: 0.1484 f1_train: 0.8145 time: 0.1845s\n",
      "Epoch: 0194 loss_train: 0.1477 f1_train: 0.8160 time: 0.1786s\n",
      "Epoch: 0195 loss_train: 0.1476 f1_train: 0.8204 time: 0.1727s\n",
      "Epoch: 0196 loss_train: 0.1502 f1_train: 0.8159 time: 0.1830s\n",
      "Epoch: 0197 loss_train: 0.1492 f1_train: 0.8198 time: 0.1869s\n",
      "Epoch: 0198 loss_train: 0.1500 f1_train: 0.8165 time: 0.1752s\n",
      "Epoch: 0199 loss_train: 0.1503 f1_train: 0.8152 time: 0.1944s\n",
      "Epoch: 0200 loss_train: 0.1479 f1_train: 0.8185 time: 0.1748s\n",
      "Epoch: 0201 loss_train: 0.1450 f1_train: 0.8247 time: 0.1814s\n",
      "Epoch: 0202 loss_train: 0.1463 f1_train: 0.8277 time: 0.1955s\n",
      "Epoch: 0203 loss_train: 0.1437 f1_train: 0.8249 time: 0.1776s\n",
      "Epoch: 0204 loss_train: 0.1457 f1_train: 0.8158 time: 0.1797s\n",
      "Epoch: 0205 loss_train: 0.1472 f1_train: 0.8177 time: 0.1736s\n",
      "Epoch: 0206 loss_train: 0.1449 f1_train: 0.8262 time: 0.1803s\n",
      "Epoch: 0207 loss_train: 0.1453 f1_train: 0.8153 time: 0.1830s\n",
      "Epoch: 0208 loss_train: 0.1449 f1_train: 0.8268 time: 0.1822s\n",
      "Epoch: 0209 loss_train: 0.1452 f1_train: 0.8192 time: 0.1781s\n",
      "Epoch: 0210 loss_train: 0.1450 f1_train: 0.8229 time: 0.1820s\n",
      "Epoch: 0211 loss_train: 0.1422 f1_train: 0.8251 time: 0.1744s\n",
      "Epoch: 0212 loss_train: 0.1386 f1_train: 0.8273 time: 0.1791s\n",
      "Epoch: 0213 loss_train: 0.1428 f1_train: 0.8329 time: 0.1764s\n",
      "Epoch: 0214 loss_train: 0.1451 f1_train: 0.8197 time: 0.1792s\n",
      "Epoch: 0215 loss_train: 0.1435 f1_train: 0.8271 time: 0.2119s\n",
      "Epoch: 0216 loss_train: 0.1416 f1_train: 0.8261 time: 0.2054s\n",
      "Epoch: 0217 loss_train: 0.1416 f1_train: 0.8267 time: 0.1753s\n",
      "Epoch: 0218 loss_train: 0.1404 f1_train: 0.8223 time: 0.1793s\n",
      "Epoch: 0219 loss_train: 0.1403 f1_train: 0.8296 time: 0.1756s\n",
      "Epoch: 0220 loss_train: 0.1399 f1_train: 0.8248 time: 0.2007s\n",
      "Epoch: 0221 loss_train: 0.1419 f1_train: 0.8260 time: 0.1792s\n",
      "Epoch: 0222 loss_train: 0.1395 f1_train: 0.8288 time: 0.1782s\n",
      "Epoch: 0223 loss_train: 0.1421 f1_train: 0.8264 time: 0.1772s\n",
      "Epoch: 0224 loss_train: 0.1374 f1_train: 0.8313 time: 0.1784s\n",
      "Epoch: 0225 loss_train: 0.1401 f1_train: 0.8290 time: 0.1786s\n",
      "Epoch: 0226 loss_train: 0.1412 f1_train: 0.8295 time: 0.1865s\n",
      "Epoch: 0227 loss_train: 0.1429 f1_train: 0.8231 time: 0.1748s\n",
      "Epoch: 0228 loss_train: 0.1425 f1_train: 0.8267 time: 0.1807s\n",
      "Epoch: 0229 loss_train: 0.1386 f1_train: 0.8296 time: 0.1788s\n",
      "Epoch: 0230 loss_train: 0.1370 f1_train: 0.8257 time: 0.1789s\n",
      "Epoch: 0231 loss_train: 0.1384 f1_train: 0.8307 time: 0.1743s\n",
      "Epoch: 0232 loss_train: 0.1368 f1_train: 0.8334 time: 0.1798s\n",
      "Epoch: 0233 loss_train: 0.1384 f1_train: 0.8324 time: 0.1737s\n",
      "Epoch: 0234 loss_train: 0.1369 f1_train: 0.8313 time: 0.1819s\n",
      "Epoch: 0235 loss_train: 0.1361 f1_train: 0.8345 time: 0.1789s\n",
      "Epoch: 0236 loss_train: 0.1371 f1_train: 0.8305 time: 0.1841s\n",
      "Epoch: 0237 loss_train: 0.1349 f1_train: 0.8341 time: 0.1801s\n",
      "Epoch: 0238 loss_train: 0.1363 f1_train: 0.8360 time: 0.1786s\n",
      "Epoch: 0239 loss_train: 0.1362 f1_train: 0.8371 time: 0.1747s\n",
      "Epoch: 0240 loss_train: 0.1356 f1_train: 0.8349 time: 0.2011s\n",
      "Epoch: 0241 loss_train: 0.1379 f1_train: 0.8297 time: 0.1751s\n",
      "Epoch: 0242 loss_train: 0.1346 f1_train: 0.8357 time: 0.1863s\n",
      "Epoch: 0243 loss_train: 0.1355 f1_train: 0.8353 time: 0.1810s\n",
      "Epoch: 0244 loss_train: 0.1352 f1_train: 0.8351 time: 0.1782s\n",
      "Epoch: 0245 loss_train: 0.1352 f1_train: 0.8367 time: 0.1745s\n",
      "Epoch: 0246 loss_train: 0.1344 f1_train: 0.8335 time: 0.1788s\n",
      "Epoch: 0247 loss_train: 0.1335 f1_train: 0.8340 time: 0.1752s\n",
      "Epoch: 0248 loss_train: 0.1340 f1_train: 0.8388 time: 0.1877s\n",
      "Epoch: 0249 loss_train: 0.1346 f1_train: 0.8377 time: 0.1746s\n",
      "Epoch: 0250 loss_train: 0.1343 f1_train: 0.8404 time: 0.1917s\n",
      "Epoch: 0251 loss_train: 0.1341 f1_train: 0.8406 time: 0.1735s\n",
      "Epoch: 0252 loss_train: 0.1343 f1_train: 0.8378 time: 0.1819s\n",
      "Epoch: 0253 loss_train: 0.1309 f1_train: 0.8397 time: 0.1736s\n",
      "Epoch: 0254 loss_train: 0.1338 f1_train: 0.8368 time: 0.1796s\n",
      "Epoch: 0255 loss_train: 0.1323 f1_train: 0.8386 time: 0.1727s\n",
      "Epoch: 0256 loss_train: 0.1317 f1_train: 0.8385 time: 0.1800s\n",
      "Epoch: 0257 loss_train: 0.1293 f1_train: 0.8397 time: 0.1749s\n",
      "Epoch: 0258 loss_train: 0.1267 f1_train: 0.8404 time: 0.1805s\n",
      "Epoch: 0259 loss_train: 0.1306 f1_train: 0.8412 time: 0.1936s\n",
      "Epoch: 0260 loss_train: 0.1323 f1_train: 0.8389 time: 0.1771s\n",
      "Epoch: 0261 loss_train: 0.1299 f1_train: 0.8408 time: 0.1922s\n",
      "Epoch: 0262 loss_train: 0.1309 f1_train: 0.8453 time: 0.1771s\n",
      "Epoch: 0263 loss_train: 0.1310 f1_train: 0.8388 time: 0.1972s\n",
      "Epoch: 0264 loss_train: 0.1301 f1_train: 0.8373 time: 0.1786s\n",
      "Epoch: 0265 loss_train: 0.1301 f1_train: 0.8426 time: 0.1982s\n",
      "Epoch: 0266 loss_train: 0.1348 f1_train: 0.8378 time: 0.1807s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0267 loss_train: 0.1278 f1_train: 0.8441 time: 0.1769s\n",
      "Epoch: 0268 loss_train: 0.1329 f1_train: 0.8416 time: 0.1733s\n",
      "Epoch: 0269 loss_train: 0.1313 f1_train: 0.8405 time: 0.1842s\n",
      "Epoch: 0270 loss_train: 0.1276 f1_train: 0.8434 time: 0.1869s\n",
      "Epoch: 0271 loss_train: 0.1296 f1_train: 0.8367 time: 0.1781s\n",
      "Epoch: 0272 loss_train: 0.1241 f1_train: 0.8437 time: 0.1856s\n",
      "Epoch: 0273 loss_train: 0.1307 f1_train: 0.8346 time: 0.1759s\n",
      "Epoch: 0274 loss_train: 0.1263 f1_train: 0.8463 time: 0.1785s\n",
      "Epoch: 0275 loss_train: 0.1307 f1_train: 0.8363 time: 0.1784s\n",
      "Epoch: 0276 loss_train: 0.1266 f1_train: 0.8509 time: 0.1944s\n",
      "Epoch: 0277 loss_train: 0.1262 f1_train: 0.8497 time: 0.1748s\n",
      "Epoch: 0278 loss_train: 0.1254 f1_train: 0.8472 time: 0.1905s\n",
      "Epoch: 0279 loss_train: 0.1275 f1_train: 0.8421 time: 0.1771s\n",
      "Epoch: 0280 loss_train: 0.1289 f1_train: 0.8457 time: 0.1914s\n",
      "Epoch: 0281 loss_train: 0.1262 f1_train: 0.8422 time: 0.1849s\n",
      "Epoch: 0282 loss_train: 0.1270 f1_train: 0.8444 time: 0.1740s\n",
      "Epoch: 0283 loss_train: 0.1267 f1_train: 0.8422 time: 0.1830s\n",
      "Epoch: 0284 loss_train: 0.1279 f1_train: 0.8437 time: 0.1792s\n",
      "Epoch: 0285 loss_train: 0.1262 f1_train: 0.8438 time: 0.1884s\n",
      "Epoch: 0286 loss_train: 0.1292 f1_train: 0.8457 time: 0.1752s\n",
      "Epoch: 0287 loss_train: 0.1238 f1_train: 0.8460 time: 0.1765s\n",
      "Epoch: 0288 loss_train: 0.1267 f1_train: 0.8487 time: 0.1744s\n",
      "Epoch: 0289 loss_train: 0.1295 f1_train: 0.8469 time: 0.1790s\n",
      "Epoch: 0290 loss_train: 0.1246 f1_train: 0.8509 time: 0.1792s\n",
      "Epoch: 0291 loss_train: 0.1270 f1_train: 0.8452 time: 0.1773s\n",
      "Epoch: 0292 loss_train: 0.1272 f1_train: 0.8446 time: 0.1740s\n",
      "Epoch: 0293 loss_train: 0.1255 f1_train: 0.8463 time: 0.1925s\n",
      "Epoch: 0294 loss_train: 0.1249 f1_train: 0.8475 time: 0.1744s\n",
      "Epoch: 0295 loss_train: 0.1261 f1_train: 0.8474 time: 0.1792s\n",
      "Epoch: 0296 loss_train: 0.1249 f1_train: 0.8463 time: 0.2199s\n",
      "Epoch: 0297 loss_train: 0.1241 f1_train: 0.8526 time: 0.2178s\n",
      "Epoch: 0298 loss_train: 0.1249 f1_train: 0.8436 time: 0.2308s\n",
      "Epoch: 0299 loss_train: 0.1239 f1_train: 0.8483 time: 0.2350s\n",
      "Epoch: 0300 loss_train: 0.1238 f1_train: 0.8477 time: 0.2237s\n",
      "Epoch: 0301 loss_train: 0.1237 f1_train: 0.8519 time: 0.2223s\n",
      "Epoch: 0302 loss_train: 0.1204 f1_train: 0.8521 time: 0.2194s\n",
      "Epoch: 0303 loss_train: 0.1243 f1_train: 0.8561 time: 0.2533s\n",
      "Epoch: 0304 loss_train: 0.1241 f1_train: 0.8491 time: 0.2461s\n",
      "Epoch: 0305 loss_train: 0.1229 f1_train: 0.8498 time: 0.2394s\n",
      "Epoch: 0306 loss_train: 0.1226 f1_train: 0.8472 time: 0.2214s\n",
      "Epoch: 0307 loss_train: 0.1247 f1_train: 0.8473 time: 0.2249s\n",
      "Epoch: 0308 loss_train: 0.1239 f1_train: 0.8494 time: 0.2325s\n",
      "Epoch: 0309 loss_train: 0.1226 f1_train: 0.8532 time: 0.2263s\n",
      "Epoch: 0310 loss_train: 0.1195 f1_train: 0.8515 time: 0.2156s\n",
      "Epoch: 0311 loss_train: 0.1201 f1_train: 0.8537 time: 0.2301s\n",
      "Epoch: 0312 loss_train: 0.1234 f1_train: 0.8486 time: 0.2180s\n",
      "Epoch: 0313 loss_train: 0.1200 f1_train: 0.8548 time: 0.2187s\n",
      "Epoch: 0314 loss_train: 0.1215 f1_train: 0.8482 time: 0.2265s\n",
      "Epoch: 0315 loss_train: 0.1222 f1_train: 0.8517 time: 0.2174s\n",
      "Epoch: 0316 loss_train: 0.1200 f1_train: 0.8561 time: 0.2190s\n",
      "Epoch: 0317 loss_train: 0.1232 f1_train: 0.8441 time: 0.2274s\n",
      "Epoch: 0318 loss_train: 0.1212 f1_train: 0.8569 time: 0.2145s\n",
      "Epoch: 0319 loss_train: 0.1208 f1_train: 0.8531 time: 0.2199s\n",
      "Epoch: 0320 loss_train: 0.1192 f1_train: 0.8518 time: 0.2201s\n",
      "Epoch: 0321 loss_train: 0.1206 f1_train: 0.8576 time: 0.2147s\n",
      "Epoch: 0322 loss_train: 0.1232 f1_train: 0.8513 time: 0.2151s\n",
      "Epoch: 0323 loss_train: 0.1194 f1_train: 0.8536 time: 0.2206s\n",
      "Epoch: 0324 loss_train: 0.1206 f1_train: 0.8497 time: 0.2247s\n",
      "Epoch: 0325 loss_train: 0.1210 f1_train: 0.8493 time: 0.2210s\n",
      "Epoch: 0326 loss_train: 0.1202 f1_train: 0.8560 time: 0.2212s\n",
      "Epoch: 0327 loss_train: 0.1186 f1_train: 0.8560 time: 0.2194s\n",
      "Epoch: 0328 loss_train: 0.1175 f1_train: 0.8520 time: 0.2354s\n",
      "Epoch: 0329 loss_train: 0.1174 f1_train: 0.8547 time: 0.2295s\n",
      "Epoch: 0330 loss_train: 0.1212 f1_train: 0.8533 time: 0.2212s\n",
      "Epoch: 0331 loss_train: 0.1196 f1_train: 0.8512 time: 0.2222s\n",
      "Epoch: 0332 loss_train: 0.1176 f1_train: 0.8575 time: 0.2135s\n",
      "Epoch: 0333 loss_train: 0.1160 f1_train: 0.8591 time: 0.2247s\n",
      "Epoch: 0334 loss_train: 0.1165 f1_train: 0.8565 time: 0.2206s\n",
      "Epoch: 0335 loss_train: 0.1187 f1_train: 0.8546 time: 0.2299s\n",
      "Epoch: 0336 loss_train: 0.1169 f1_train: 0.8576 time: 0.2166s\n",
      "Epoch: 0337 loss_train: 0.1158 f1_train: 0.8559 time: 0.2094s\n",
      "Epoch: 0338 loss_train: 0.1180 f1_train: 0.8557 time: 0.2195s\n",
      "Epoch: 0339 loss_train: 0.1201 f1_train: 0.8537 time: 0.2186s\n",
      "Epoch: 0340 loss_train: 0.1172 f1_train: 0.8545 time: 0.2238s\n",
      "Epoch: 0341 loss_train: 0.1171 f1_train: 0.8559 time: 0.2151s\n",
      "Epoch: 0342 loss_train: 0.1216 f1_train: 0.8513 time: 0.2209s\n",
      "Epoch: 0343 loss_train: 0.1205 f1_train: 0.8523 time: 0.2192s\n",
      "Epoch: 0344 loss_train: 0.1187 f1_train: 0.8559 time: 0.2188s\n",
      "Epoch: 0345 loss_train: 0.1182 f1_train: 0.8533 time: 0.2194s\n",
      "Epoch: 0346 loss_train: 0.1200 f1_train: 0.8527 time: 0.2200s\n",
      "Epoch: 0347 loss_train: 0.1174 f1_train: 0.8582 time: 0.2200s\n",
      "Epoch: 0348 loss_train: 0.1166 f1_train: 0.8585 time: 0.2237s\n",
      "Epoch: 0349 loss_train: 0.1154 f1_train: 0.8590 time: 0.2156s\n",
      "Epoch: 0350 loss_train: 0.1193 f1_train: 0.8538 time: 0.2174s\n",
      "Epoch: 0351 loss_train: 0.1166 f1_train: 0.8567 time: 0.2205s\n",
      "Epoch: 0352 loss_train: 0.1150 f1_train: 0.8579 time: 0.2181s\n",
      "Epoch: 0353 loss_train: 0.1163 f1_train: 0.8557 time: 0.2207s\n",
      "Epoch: 0354 loss_train: 0.1151 f1_train: 0.8584 time: 0.2246s\n",
      "Epoch: 0355 loss_train: 0.1149 f1_train: 0.8567 time: 0.2130s\n",
      "Epoch: 0356 loss_train: 0.1194 f1_train: 0.8510 time: 0.2196s\n",
      "Epoch: 0357 loss_train: 0.1152 f1_train: 0.8594 time: 0.2202s\n",
      "Epoch: 0358 loss_train: 0.1155 f1_train: 0.8593 time: 0.2226s\n",
      "Epoch: 0359 loss_train: 0.1145 f1_train: 0.8580 time: 0.2126s\n",
      "Epoch: 0360 loss_train: 0.1147 f1_train: 0.8631 time: 0.2191s\n",
      "Epoch: 0361 loss_train: 0.1158 f1_train: 0.8536 time: 0.2187s\n",
      "Epoch: 0362 loss_train: 0.1142 f1_train: 0.8588 time: 0.2201s\n",
      "Epoch: 0363 loss_train: 0.1153 f1_train: 0.8605 time: 0.2174s\n",
      "Epoch: 0364 loss_train: 0.1154 f1_train: 0.8595 time: 0.2212s\n",
      "Epoch: 0365 loss_train: 0.1136 f1_train: 0.8617 time: 0.2131s\n",
      "Epoch: 0366 loss_train: 0.1155 f1_train: 0.8599 time: 0.2166s\n",
      "Epoch: 0367 loss_train: 0.1146 f1_train: 0.8605 time: 0.2213s\n",
      "Epoch: 0368 loss_train: 0.1141 f1_train: 0.8577 time: 0.2236s\n",
      "Epoch: 0369 loss_train: 0.1132 f1_train: 0.8611 time: 0.2174s\n",
      "Epoch: 0370 loss_train: 0.1126 f1_train: 0.8610 time: 0.2128s\n",
      "Epoch: 0371 loss_train: 0.1143 f1_train: 0.8587 time: 0.2288s\n",
      "Epoch: 0372 loss_train: 0.1112 f1_train: 0.8628 time: 0.2221s\n",
      "Epoch: 0373 loss_train: 0.1125 f1_train: 0.8583 time: 0.2300s\n",
      "Epoch: 0374 loss_train: 0.1127 f1_train: 0.8621 time: 0.2155s\n",
      "Epoch: 0375 loss_train: 0.1126 f1_train: 0.8576 time: 0.2271s\n",
      "Epoch: 0376 loss_train: 0.1155 f1_train: 0.8551 time: 0.2157s\n",
      "Epoch: 0377 loss_train: 0.1131 f1_train: 0.8580 time: 0.2330s\n",
      "Epoch: 0378 loss_train: 0.1146 f1_train: 0.8595 time: 0.2229s\n",
      "Epoch: 0379 loss_train: 0.1135 f1_train: 0.8626 time: 0.2185s\n",
      "Epoch: 0380 loss_train: 0.1155 f1_train: 0.8569 time: 0.2230s\n",
      "Epoch: 0381 loss_train: 0.1128 f1_train: 0.8638 time: 0.2227s\n",
      "Epoch: 0382 loss_train: 0.1098 f1_train: 0.8645 time: 0.2304s\n",
      "Epoch: 0383 loss_train: 0.1105 f1_train: 0.8626 time: 0.2268s\n",
      "Epoch: 0384 loss_train: 0.1127 f1_train: 0.8613 time: 0.2214s\n",
      "Epoch: 0385 loss_train: 0.1140 f1_train: 0.8590 time: 0.2188s\n",
      "Epoch: 0386 loss_train: 0.1140 f1_train: 0.8568 time: 0.2231s\n",
      "Epoch: 0387 loss_train: 0.1114 f1_train: 0.8616 time: 0.2315s\n",
      "Epoch: 0388 loss_train: 0.1096 f1_train: 0.8662 time: 0.2177s\n",
      "Epoch: 0389 loss_train: 0.1104 f1_train: 0.8620 time: 0.2303s\n",
      "Epoch: 0390 loss_train: 0.1108 f1_train: 0.8671 time: 0.2213s\n",
      "Epoch: 0391 loss_train: 0.1132 f1_train: 0.8608 time: 0.2106s\n",
      "Epoch: 0392 loss_train: 0.1122 f1_train: 0.8590 time: 0.2133s\n",
      "Epoch: 0393 loss_train: 0.1095 f1_train: 0.8688 time: 0.2245s\n",
      "Epoch: 0394 loss_train: 0.1091 f1_train: 0.8668 time: 0.2243s\n",
      "Epoch: 0395 loss_train: 0.1103 f1_train: 0.8664 time: 0.2177s\n",
      "Epoch: 0396 loss_train: 0.1090 f1_train: 0.8641 time: 0.2158s\n",
      "Epoch: 0397 loss_train: 0.1123 f1_train: 0.8588 time: 0.2171s\n",
      "Epoch: 0398 loss_train: 0.1079 f1_train: 0.8653 time: 0.2130s\n",
      "Epoch: 0399 loss_train: 0.1102 f1_train: 0.8672 time: 0.2255s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0400 loss_train: 0.1092 f1_train: 0.8626 time: 0.2176s\n",
      "Epoch: 0401 loss_train: 0.1125 f1_train: 0.8602 time: 0.2192s\n",
      "Epoch: 0402 loss_train: 0.1084 f1_train: 0.8678 time: 0.2204s\n",
      "Epoch: 0403 loss_train: 0.1095 f1_train: 0.8615 time: 0.2227s\n",
      "Epoch: 0404 loss_train: 0.1086 f1_train: 0.8634 time: 0.2241s\n",
      "Epoch: 0405 loss_train: 0.1096 f1_train: 0.8629 time: 0.2224s\n",
      "Epoch: 0406 loss_train: 0.1089 f1_train: 0.8649 time: 0.2219s\n",
      "Epoch: 0407 loss_train: 0.1108 f1_train: 0.8645 time: 0.2216s\n",
      "Epoch: 0408 loss_train: 0.1092 f1_train: 0.8670 time: 0.2262s\n",
      "Epoch: 0409 loss_train: 0.1102 f1_train: 0.8643 time: 0.2133s\n",
      "Epoch: 0410 loss_train: 0.1103 f1_train: 0.8627 time: 0.2258s\n",
      "Epoch: 0411 loss_train: 0.1130 f1_train: 0.8578 time: 0.2273s\n",
      "Epoch: 0412 loss_train: 0.1095 f1_train: 0.8658 time: 0.2183s\n",
      "Epoch: 0413 loss_train: 0.1115 f1_train: 0.8619 time: 0.2291s\n",
      "Epoch: 0414 loss_train: 0.1099 f1_train: 0.8636 time: 0.2208s\n",
      "Epoch: 0415 loss_train: 0.1109 f1_train: 0.8691 time: 0.2203s\n",
      "Epoch: 0416 loss_train: 0.1105 f1_train: 0.8595 time: 0.2324s\n",
      "Epoch: 0417 loss_train: 0.1071 f1_train: 0.8691 time: 0.2223s\n",
      "Epoch: 0418 loss_train: 0.1080 f1_train: 0.8679 time: 0.2184s\n",
      "Epoch: 0419 loss_train: 0.1092 f1_train: 0.8647 time: 0.2189s\n",
      "Epoch: 0420 loss_train: 0.1079 f1_train: 0.8700 time: 0.2181s\n",
      "Epoch: 0421 loss_train: 0.1068 f1_train: 0.8672 time: 0.2381s\n",
      "Epoch: 0422 loss_train: 0.1063 f1_train: 0.8716 time: 0.2142s\n",
      "Epoch: 0423 loss_train: 0.1095 f1_train: 0.8662 time: 0.2168s\n",
      "Epoch: 0424 loss_train: 0.1078 f1_train: 0.8670 time: 0.2149s\n",
      "Epoch: 0425 loss_train: 0.1078 f1_train: 0.8697 time: 0.2233s\n",
      "Epoch: 0426 loss_train: 0.1078 f1_train: 0.8691 time: 0.2196s\n",
      "Epoch: 0427 loss_train: 0.1069 f1_train: 0.8647 time: 0.2239s\n",
      "Epoch: 0428 loss_train: 0.1097 f1_train: 0.8672 time: 0.2223s\n",
      "Epoch: 0429 loss_train: 0.1081 f1_train: 0.8696 time: 0.2166s\n",
      "Epoch: 0430 loss_train: 0.1067 f1_train: 0.8665 time: 0.2200s\n",
      "Epoch: 0431 loss_train: 0.1072 f1_train: 0.8661 time: 0.2278s\n",
      "Epoch: 0432 loss_train: 0.1066 f1_train: 0.8698 time: 0.2344s\n",
      "Epoch: 0433 loss_train: 0.1081 f1_train: 0.8693 time: 0.2206s\n",
      "Epoch: 0434 loss_train: 0.1062 f1_train: 0.8653 time: 0.2211s\n",
      "Epoch: 0435 loss_train: 0.1050 f1_train: 0.8694 time: 0.2243s\n",
      "Epoch: 0436 loss_train: 0.1075 f1_train: 0.8669 time: 0.2209s\n",
      "Epoch: 0437 loss_train: 0.1073 f1_train: 0.8673 time: 0.2142s\n",
      "Epoch: 0438 loss_train: 0.1076 f1_train: 0.8662 time: 0.2217s\n",
      "Epoch: 0439 loss_train: 0.1079 f1_train: 0.8641 time: 0.2147s\n",
      "Epoch: 0440 loss_train: 0.1059 f1_train: 0.8660 time: 0.2291s\n",
      "Epoch: 0441 loss_train: 0.1078 f1_train: 0.8648 time: 0.2134s\n",
      "Epoch: 0442 loss_train: 0.1069 f1_train: 0.8680 time: 0.2353s\n",
      "Epoch: 0443 loss_train: 0.1062 f1_train: 0.8722 time: 0.2401s\n",
      "Epoch: 0444 loss_train: 0.1075 f1_train: 0.8663 time: 0.2427s\n",
      "Epoch: 0445 loss_train: 0.1053 f1_train: 0.8691 time: 0.2406s\n",
      "Epoch: 0446 loss_train: 0.1021 f1_train: 0.8732 time: 0.2322s\n",
      "Epoch: 0447 loss_train: 0.1059 f1_train: 0.8682 time: 0.2405s\n",
      "Epoch: 0448 loss_train: 0.1035 f1_train: 0.8682 time: 0.2372s\n",
      "Epoch: 0449 loss_train: 0.1038 f1_train: 0.8686 time: 0.2424s\n",
      "Epoch: 0450 loss_train: 0.1059 f1_train: 0.8665 time: 0.2436s\n",
      "Epoch: 0451 loss_train: 0.1048 f1_train: 0.8686 time: 0.2309s\n",
      "Epoch: 0452 loss_train: 0.1048 f1_train: 0.8729 time: 0.2460s\n",
      "Epoch: 0453 loss_train: 0.1042 f1_train: 0.8708 time: 0.2374s\n",
      "Epoch: 0454 loss_train: 0.1059 f1_train: 0.8683 time: 0.2389s\n",
      "Epoch: 0455 loss_train: 0.1054 f1_train: 0.8702 time: 0.2370s\n",
      "Epoch: 0456 loss_train: 0.1067 f1_train: 0.8715 time: 0.2370s\n",
      "Epoch: 0457 loss_train: 0.1053 f1_train: 0.8665 time: 0.2386s\n",
      "Epoch: 0458 loss_train: 0.1046 f1_train: 0.8705 time: 0.2341s\n",
      "Epoch: 0459 loss_train: 0.1060 f1_train: 0.8688 time: 0.2409s\n",
      "Epoch: 0460 loss_train: 0.1056 f1_train: 0.8664 time: 0.2341s\n",
      "Epoch: 0461 loss_train: 0.1048 f1_train: 0.8709 time: 0.2375s\n",
      "Epoch: 0462 loss_train: 0.1056 f1_train: 0.8684 time: 0.2404s\n",
      "Epoch: 0463 loss_train: 0.1051 f1_train: 0.8677 time: 0.2349s\n",
      "Epoch: 0464 loss_train: 0.1064 f1_train: 0.8684 time: 0.2326s\n",
      "Epoch: 0465 loss_train: 0.1036 f1_train: 0.8705 time: 0.2321s\n",
      "Epoch: 0466 loss_train: 0.1040 f1_train: 0.8702 time: 0.2299s\n",
      "Epoch: 0467 loss_train: 0.1017 f1_train: 0.8744 time: 0.2431s\n",
      "Epoch: 0468 loss_train: 0.1032 f1_train: 0.8699 time: 0.2442s\n",
      "Epoch: 0469 loss_train: 0.1031 f1_train: 0.8723 time: 0.2444s\n",
      "Epoch: 0470 loss_train: 0.1037 f1_train: 0.8686 time: 0.2417s\n",
      "Epoch: 0471 loss_train: 0.1044 f1_train: 0.8742 time: 0.2341s\n",
      "Epoch: 0472 loss_train: 0.1057 f1_train: 0.8710 time: 0.2420s\n",
      "Epoch: 0473 loss_train: 0.1049 f1_train: 0.8668 time: 0.2386s\n",
      "Epoch: 0474 loss_train: 0.1042 f1_train: 0.8737 time: 0.2379s\n",
      "Epoch: 0475 loss_train: 0.1032 f1_train: 0.8738 time: 0.2469s\n",
      "Epoch: 0476 loss_train: 0.1028 f1_train: 0.8716 time: 0.2374s\n",
      "Epoch: 0477 loss_train: 0.1049 f1_train: 0.8703 time: 0.2430s\n",
      "Epoch: 0478 loss_train: 0.1046 f1_train: 0.8707 time: 0.2293s\n",
      "Epoch: 0479 loss_train: 0.1015 f1_train: 0.8735 time: 0.2293s\n",
      "Epoch: 0480 loss_train: 0.1032 f1_train: 0.8721 time: 0.2451s\n",
      "Epoch: 0481 loss_train: 0.1011 f1_train: 0.8684 time: 0.2298s\n",
      "Epoch: 0482 loss_train: 0.1033 f1_train: 0.8682 time: 0.2309s\n",
      "Epoch: 0483 loss_train: 0.1016 f1_train: 0.8739 time: 0.2320s\n",
      "Epoch: 0484 loss_train: 0.1023 f1_train: 0.8757 time: 0.2344s\n",
      "Epoch: 0485 loss_train: 0.1020 f1_train: 0.8745 time: 0.2328s\n",
      "Epoch: 0486 loss_train: 0.1023 f1_train: 0.8727 time: 0.2364s\n",
      "Epoch: 0487 loss_train: 0.0999 f1_train: 0.8748 time: 0.2310s\n",
      "Epoch: 0488 loss_train: 0.1050 f1_train: 0.8711 time: 0.2300s\n",
      "Epoch: 0489 loss_train: 0.1015 f1_train: 0.8768 time: 0.2481s\n",
      "Epoch: 0490 loss_train: 0.1020 f1_train: 0.8724 time: 0.2310s\n",
      "Epoch: 0491 loss_train: 0.1033 f1_train: 0.8739 time: 0.2313s\n",
      "Epoch: 0492 loss_train: 0.0997 f1_train: 0.8732 time: 0.2408s\n",
      "Epoch: 0493 loss_train: 0.1020 f1_train: 0.8739 time: 0.2499s\n",
      "Epoch: 0494 loss_train: 0.1009 f1_train: 0.8710 time: 0.2444s\n",
      "Epoch: 0495 loss_train: 0.1020 f1_train: 0.8757 time: 0.2381s\n",
      "Epoch: 0496 loss_train: 0.1017 f1_train: 0.8755 time: 0.2448s\n",
      "Epoch: 0497 loss_train: 0.1050 f1_train: 0.8703 time: 0.2377s\n",
      "Epoch: 0498 loss_train: 0.1040 f1_train: 0.8737 time: 0.2429s\n",
      "Epoch: 0499 loss_train: 0.0999 f1_train: 0.8699 time: 0.2365s\n",
      "Epoch: 0500 loss_train: 0.1011 f1_train: 0.8727 time: 0.2392s\n",
      "Epoch: 0501 loss_train: 0.0980 f1_train: 0.8768 time: 0.2347s\n",
      "Epoch: 0502 loss_train: 0.1008 f1_train: 0.8756 time: 0.2344s\n",
      "Epoch: 0503 loss_train: 0.1039 f1_train: 0.8700 time: 0.2387s\n",
      "Epoch: 0504 loss_train: 0.1009 f1_train: 0.8743 time: 0.2336s\n",
      "Epoch: 0505 loss_train: 0.1017 f1_train: 0.8785 time: 0.2350s\n",
      "Epoch: 0506 loss_train: 0.1025 f1_train: 0.8722 time: 0.2340s\n",
      "Epoch: 0507 loss_train: 0.0982 f1_train: 0.8773 time: 0.2359s\n",
      "Epoch: 0508 loss_train: 0.0979 f1_train: 0.8801 time: 0.2373s\n",
      "Epoch: 0509 loss_train: 0.0995 f1_train: 0.8767 time: 0.2375s\n",
      "Epoch: 0510 loss_train: 0.1004 f1_train: 0.8766 time: 0.2330s\n",
      "Epoch: 0511 loss_train: 0.1006 f1_train: 0.8725 time: 0.2321s\n",
      "Epoch: 0512 loss_train: 0.0984 f1_train: 0.8782 time: 0.2361s\n",
      "Epoch: 0513 loss_train: 0.0994 f1_train: 0.8778 time: 0.2337s\n",
      "Epoch: 0514 loss_train: 0.0991 f1_train: 0.8795 time: 0.2340s\n",
      "Epoch: 0515 loss_train: 0.0985 f1_train: 0.8747 time: 0.2307s\n",
      "Epoch: 0516 loss_train: 0.0994 f1_train: 0.8777 time: 0.2294s\n",
      "Epoch: 0517 loss_train: 0.1005 f1_train: 0.8781 time: 0.2323s\n",
      "Epoch: 0518 loss_train: 0.1007 f1_train: 0.8714 time: 0.2337s\n",
      "Epoch: 0519 loss_train: 0.0991 f1_train: 0.8742 time: 0.2362s\n",
      "Epoch: 0520 loss_train: 0.0977 f1_train: 0.8780 time: 0.2355s\n",
      "Epoch: 0521 loss_train: 0.1003 f1_train: 0.8754 time: 0.2348s\n",
      "Epoch: 0522 loss_train: 0.0990 f1_train: 0.8768 time: 0.2402s\n",
      "Epoch: 0523 loss_train: 0.0971 f1_train: 0.8759 time: 0.2427s\n",
      "Epoch: 0524 loss_train: 0.0966 f1_train: 0.8723 time: 0.2405s\n",
      "Epoch: 0525 loss_train: 0.0992 f1_train: 0.8760 time: 0.2494s\n",
      "Epoch: 0526 loss_train: 0.0992 f1_train: 0.8702 time: 0.2385s\n",
      "Epoch: 0527 loss_train: 0.1004 f1_train: 0.8737 time: 0.2392s\n",
      "Epoch: 0528 loss_train: 0.1007 f1_train: 0.8709 time: 0.2368s\n",
      "Epoch: 0529 loss_train: 0.0977 f1_train: 0.8775 time: 0.2337s\n",
      "Epoch: 0530 loss_train: 0.0986 f1_train: 0.8741 time: 0.2344s\n",
      "Epoch: 0531 loss_train: 0.0969 f1_train: 0.8760 time: 0.2373s\n",
      "Epoch: 0532 loss_train: 0.0977 f1_train: 0.8760 time: 0.2343s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0533 loss_train: 0.0973 f1_train: 0.8779 time: 0.2375s\n",
      "Epoch: 0534 loss_train: 0.0988 f1_train: 0.8774 time: 0.2387s\n",
      "Epoch: 0535 loss_train: 0.0966 f1_train: 0.8795 time: 0.2373s\n",
      "Epoch: 0536 loss_train: 0.1011 f1_train: 0.8810 time: 0.2543s\n",
      "Epoch: 0537 loss_train: 0.0961 f1_train: 0.8797 time: 0.2461s\n",
      "Epoch: 0538 loss_train: 0.0959 f1_train: 0.8800 time: 0.2365s\n",
      "Epoch: 0539 loss_train: 0.0946 f1_train: 0.8803 time: 0.2387s\n",
      "Epoch: 0540 loss_train: 0.0990 f1_train: 0.8772 time: 0.2382s\n",
      "Epoch: 0541 loss_train: 0.0975 f1_train: 0.8793 time: 0.2387s\n",
      "Epoch: 0542 loss_train: 0.0988 f1_train: 0.8814 time: 0.2359s\n",
      "Epoch: 0543 loss_train: 0.0986 f1_train: 0.8751 time: 0.2729s\n",
      "Epoch: 0544 loss_train: 0.0978 f1_train: 0.8739 time: 0.1850s\n",
      "Epoch: 0545 loss_train: 0.0972 f1_train: 0.8808 time: 0.1953s\n",
      "Epoch: 0546 loss_train: 0.0978 f1_train: 0.8773 time: 0.2501s\n",
      "Epoch: 0547 loss_train: 0.0977 f1_train: 0.8805 time: 0.2021s\n",
      "Epoch: 0548 loss_train: 0.0976 f1_train: 0.8753 time: 0.2086s\n",
      "Epoch: 0549 loss_train: 0.0968 f1_train: 0.8786 time: 0.1939s\n",
      "Epoch: 0550 loss_train: 0.0964 f1_train: 0.8799 time: 0.1874s\n",
      "Epoch: 0551 loss_train: 0.0999 f1_train: 0.8742 time: 0.1861s\n",
      "Epoch: 0552 loss_train: 0.0984 f1_train: 0.8764 time: 0.1939s\n",
      "Epoch: 0553 loss_train: 0.0968 f1_train: 0.8792 time: 0.1856s\n",
      "Epoch: 0554 loss_train: 0.0984 f1_train: 0.8801 time: 0.1862s\n",
      "Epoch: 0555 loss_train: 0.0961 f1_train: 0.8749 time: 0.1859s\n",
      "Epoch: 0556 loss_train: 0.0967 f1_train: 0.8810 time: 0.1879s\n",
      "Epoch: 0557 loss_train: 0.0998 f1_train: 0.8768 time: 0.1949s\n",
      "Epoch: 0558 loss_train: 0.0963 f1_train: 0.8792 time: 0.1898s\n",
      "Epoch: 0559 loss_train: 0.0980 f1_train: 0.8821 time: 0.1854s\n",
      "Epoch: 0560 loss_train: 0.0975 f1_train: 0.8811 time: 0.1849s\n",
      "Epoch: 0561 loss_train: 0.0953 f1_train: 0.8800 time: 0.1858s\n",
      "Epoch: 0562 loss_train: 0.0984 f1_train: 0.8765 time: 0.1857s\n",
      "Epoch: 0563 loss_train: 0.0951 f1_train: 0.8785 time: 0.1870s\n",
      "Epoch: 0564 loss_train: 0.0988 f1_train: 0.8793 time: 0.1844s\n",
      "Epoch: 0565 loss_train: 0.0950 f1_train: 0.8798 time: 0.1856s\n",
      "Epoch: 0566 loss_train: 0.0936 f1_train: 0.8803 time: 0.1864s\n",
      "Epoch: 0567 loss_train: 0.0956 f1_train: 0.8790 time: 0.1862s\n",
      "Epoch: 0568 loss_train: 0.0960 f1_train: 0.8819 time: 0.1923s\n",
      "Epoch: 0569 loss_train: 0.0954 f1_train: 0.8774 time: 0.1868s\n",
      "Epoch: 0570 loss_train: 0.0966 f1_train: 0.8802 time: 0.1850s\n",
      "Epoch: 0571 loss_train: 0.0979 f1_train: 0.8796 time: 0.1941s\n",
      "Epoch: 0572 loss_train: 0.0961 f1_train: 0.8770 time: 0.1881s\n",
      "Epoch: 0573 loss_train: 0.0942 f1_train: 0.8823 time: 0.1905s\n",
      "Epoch: 0574 loss_train: 0.0954 f1_train: 0.8772 time: 0.1965s\n",
      "Epoch: 0575 loss_train: 0.0946 f1_train: 0.8802 time: 0.1847s\n",
      "Epoch: 0576 loss_train: 0.0943 f1_train: 0.8819 time: 0.2039s\n",
      "Epoch: 0577 loss_train: 0.0933 f1_train: 0.8801 time: 0.2006s\n",
      "Epoch: 0578 loss_train: 0.0954 f1_train: 0.8782 time: 0.1926s\n",
      "Epoch: 0579 loss_train: 0.0941 f1_train: 0.8835 time: 0.1997s\n",
      "Epoch: 0580 loss_train: 0.0934 f1_train: 0.8857 time: 0.1903s\n",
      "Epoch: 0581 loss_train: 0.0982 f1_train: 0.8754 time: 0.1878s\n",
      "Epoch: 0582 loss_train: 0.0950 f1_train: 0.8809 time: 0.1857s\n",
      "Epoch: 0583 loss_train: 0.0925 f1_train: 0.8855 time: 0.1906s\n",
      "Epoch: 0584 loss_train: 0.0967 f1_train: 0.8782 time: 0.1873s\n",
      "Epoch: 0585 loss_train: 0.0937 f1_train: 0.8833 time: 0.1912s\n",
      "Epoch: 0586 loss_train: 0.0950 f1_train: 0.8846 time: 0.1854s\n",
      "Epoch: 0587 loss_train: 0.0915 f1_train: 0.8854 time: 0.1859s\n",
      "Epoch: 0588 loss_train: 0.0956 f1_train: 0.8805 time: 0.1997s\n",
      "Epoch: 0589 loss_train: 0.0936 f1_train: 0.8832 time: 0.1848s\n",
      "Epoch: 0590 loss_train: 0.0942 f1_train: 0.8841 time: 0.1862s\n",
      "Epoch: 0591 loss_train: 0.0944 f1_train: 0.8763 time: 0.1867s\n",
      "Epoch: 0592 loss_train: 0.0945 f1_train: 0.8823 time: 0.1955s\n",
      "Epoch: 0593 loss_train: 0.0930 f1_train: 0.8806 time: 0.1856s\n",
      "Epoch: 0594 loss_train: 0.0910 f1_train: 0.8869 time: 0.1879s\n",
      "Epoch: 0595 loss_train: 0.0947 f1_train: 0.8820 time: 0.1943s\n",
      "Epoch: 0596 loss_train: 0.0925 f1_train: 0.8857 time: 0.1853s\n",
      "Epoch: 0597 loss_train: 0.0935 f1_train: 0.8817 time: 0.1855s\n",
      "Epoch: 0598 loss_train: 0.0943 f1_train: 0.8796 time: 0.1855s\n",
      "Epoch: 0599 loss_train: 0.0950 f1_train: 0.8820 time: 0.2030s\n",
      "Epoch: 0600 loss_train: 0.0929 f1_train: 0.8846 time: 0.1915s\n",
      "Epoch: 0601 loss_train: 0.0941 f1_train: 0.8811 time: 0.1857s\n",
      "Epoch: 0602 loss_train: 0.0927 f1_train: 0.8861 time: 0.1851s\n",
      "Epoch: 0603 loss_train: 0.0935 f1_train: 0.8838 time: 0.1862s\n",
      "Epoch: 0604 loss_train: 0.0930 f1_train: 0.8833 time: 0.1877s\n",
      "Epoch: 0605 loss_train: 0.0943 f1_train: 0.8848 time: 0.1863s\n",
      "Epoch: 0606 loss_train: 0.0920 f1_train: 0.8853 time: 0.1856s\n",
      "Epoch: 0607 loss_train: 0.0942 f1_train: 0.8824 time: 0.1841s\n",
      "Epoch: 0608 loss_train: 0.0942 f1_train: 0.8795 time: 0.1845s\n",
      "Epoch: 0609 loss_train: 0.0946 f1_train: 0.8815 time: 0.1924s\n",
      "Epoch: 0610 loss_train: 0.0930 f1_train: 0.8834 time: 0.1923s\n",
      "Epoch: 0611 loss_train: 0.0942 f1_train: 0.8831 time: 0.1864s\n",
      "Epoch: 0612 loss_train: 0.0927 f1_train: 0.8824 time: 0.1865s\n",
      "Epoch: 0613 loss_train: 0.0924 f1_train: 0.8865 time: 0.2048s\n",
      "Epoch: 0614 loss_train: 0.0917 f1_train: 0.8837 time: 0.1862s\n",
      "Epoch: 0615 loss_train: 0.0916 f1_train: 0.8849 time: 0.1991s\n",
      "Epoch: 0616 loss_train: 0.0931 f1_train: 0.8855 time: 0.1887s\n",
      "Epoch: 0617 loss_train: 0.0915 f1_train: 0.8856 time: 0.1854s\n",
      "Epoch: 0618 loss_train: 0.0935 f1_train: 0.8838 time: 0.1926s\n",
      "Epoch: 0619 loss_train: 0.0941 f1_train: 0.8817 time: 0.1878s\n",
      "Epoch: 0620 loss_train: 0.0894 f1_train: 0.8901 time: 0.1953s\n",
      "Epoch: 0621 loss_train: 0.0910 f1_train: 0.8872 time: 0.1846s\n",
      "Epoch: 0622 loss_train: 0.0921 f1_train: 0.8822 time: 0.1879s\n",
      "Epoch: 0623 loss_train: 0.0953 f1_train: 0.8793 time: 0.1949s\n",
      "Epoch: 0624 loss_train: 0.0890 f1_train: 0.8893 time: 0.1851s\n",
      "Epoch: 0625 loss_train: 0.0922 f1_train: 0.8834 time: 0.1904s\n",
      "Epoch: 0626 loss_train: 0.0923 f1_train: 0.8822 time: 0.1951s\n",
      "Epoch: 0627 loss_train: 0.0928 f1_train: 0.8833 time: 0.1868s\n",
      "Epoch: 0628 loss_train: 0.0920 f1_train: 0.8843 time: 0.1868s\n",
      "Epoch: 0629 loss_train: 0.0904 f1_train: 0.8867 time: 0.1883s\n",
      "Epoch: 0630 loss_train: 0.0949 f1_train: 0.8799 time: 0.1907s\n",
      "Epoch: 0631 loss_train: 0.0929 f1_train: 0.8844 time: 0.1854s\n",
      "Epoch: 0632 loss_train: 0.0925 f1_train: 0.8817 time: 0.1878s\n",
      "Epoch: 0633 loss_train: 0.0911 f1_train: 0.8837 time: 0.1863s\n",
      "Epoch: 0634 loss_train: 0.0918 f1_train: 0.8850 time: 0.1903s\n",
      "Epoch: 0635 loss_train: 0.0910 f1_train: 0.8835 time: 0.1911s\n",
      "Epoch: 0636 loss_train: 0.0895 f1_train: 0.8829 time: 0.1913s\n",
      "Epoch: 0637 loss_train: 0.0942 f1_train: 0.8760 time: 0.1854s\n",
      "Epoch: 0638 loss_train: 0.0896 f1_train: 0.8841 time: 0.1862s\n",
      "Epoch: 0639 loss_train: 0.0929 f1_train: 0.8834 time: 0.1848s\n",
      "Epoch: 0640 loss_train: 0.0917 f1_train: 0.8866 time: 0.1956s\n",
      "Epoch: 0641 loss_train: 0.0894 f1_train: 0.8916 time: 0.1863s\n",
      "Epoch: 0642 loss_train: 0.0906 f1_train: 0.8879 time: 0.1847s\n",
      "Epoch: 0643 loss_train: 0.0890 f1_train: 0.8891 time: 0.1930s\n",
      "Epoch: 0644 loss_train: 0.0896 f1_train: 0.8887 time: 0.1865s\n",
      "Epoch: 0645 loss_train: 0.0881 f1_train: 0.8880 time: 0.1892s\n",
      "Epoch: 0646 loss_train: 0.0912 f1_train: 0.8824 time: 0.1890s\n",
      "Epoch: 0647 loss_train: 0.0914 f1_train: 0.8847 time: 0.1868s\n",
      "Epoch: 0648 loss_train: 0.0905 f1_train: 0.8859 time: 0.1877s\n",
      "Epoch: 0649 loss_train: 0.0889 f1_train: 0.8874 time: 0.1890s\n",
      "Epoch: 0650 loss_train: 0.0887 f1_train: 0.8833 time: 0.1861s\n",
      "Epoch: 0651 loss_train: 0.0905 f1_train: 0.8869 time: 0.1953s\n",
      "Epoch: 0652 loss_train: 0.0910 f1_train: 0.8878 time: 0.1873s\n",
      "Epoch: 0653 loss_train: 0.0881 f1_train: 0.8929 time: 0.2078s\n",
      "Epoch: 0654 loss_train: 0.0910 f1_train: 0.8868 time: 0.1873s\n",
      "Epoch: 0655 loss_train: 0.0890 f1_train: 0.8841 time: 0.1944s\n",
      "Epoch: 0656 loss_train: 0.0892 f1_train: 0.8876 time: 0.1912s\n",
      "Epoch: 0657 loss_train: 0.0902 f1_train: 0.8868 time: 0.1866s\n",
      "Epoch: 0658 loss_train: 0.0896 f1_train: 0.8868 time: 0.1839s\n",
      "Epoch: 0659 loss_train: 0.0871 f1_train: 0.8881 time: 0.2304s\n",
      "Epoch: 0660 loss_train: 0.0923 f1_train: 0.8791 time: 0.2464s\n",
      "Epoch: 0661 loss_train: 0.0919 f1_train: 0.8859 time: 0.2471s\n",
      "Epoch: 0662 loss_train: 0.0899 f1_train: 0.8878 time: 0.2359s\n",
      "Epoch: 0663 loss_train: 0.0875 f1_train: 0.8879 time: 0.2374s\n",
      "Epoch: 0664 loss_train: 0.0885 f1_train: 0.8898 time: 0.2337s\n",
      "Epoch: 0665 loss_train: 0.0925 f1_train: 0.8878 time: 0.2472s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0666 loss_train: 0.0872 f1_train: 0.8867 time: 0.2410s\n",
      "Epoch: 0667 loss_train: 0.0900 f1_train: 0.8891 time: 0.2438s\n",
      "Epoch: 0668 loss_train: 0.0896 f1_train: 0.8920 time: 0.2538s\n",
      "Epoch: 0669 loss_train: 0.0910 f1_train: 0.8842 time: 0.2436s\n",
      "Epoch: 0670 loss_train: 0.0895 f1_train: 0.8869 time: 0.2433s\n",
      "Epoch: 0671 loss_train: 0.0908 f1_train: 0.8859 time: 0.2494s\n",
      "Epoch: 0672 loss_train: 0.0893 f1_train: 0.8925 time: 0.2473s\n",
      "Epoch: 0673 loss_train: 0.0882 f1_train: 0.8877 time: 0.2464s\n",
      "Epoch: 0674 loss_train: 0.0882 f1_train: 0.8920 time: 0.2540s\n",
      "Epoch: 0675 loss_train: 0.0881 f1_train: 0.8880 time: 0.2416s\n",
      "Epoch: 0676 loss_train: 0.0896 f1_train: 0.8866 time: 0.2366s\n",
      "Epoch: 0677 loss_train: 0.0899 f1_train: 0.8858 time: 0.2518s\n",
      "Epoch: 0678 loss_train: 0.0906 f1_train: 0.8828 time: 0.2334s\n",
      "Epoch: 0679 loss_train: 0.0877 f1_train: 0.8892 time: 0.2434s\n",
      "Epoch: 0680 loss_train: 0.0872 f1_train: 0.8918 time: 0.2568s\n",
      "Epoch: 0681 loss_train: 0.0870 f1_train: 0.8851 time: 0.2419s\n",
      "Epoch: 0682 loss_train: 0.0874 f1_train: 0.8881 time: 0.2453s\n",
      "Epoch: 0683 loss_train: 0.0882 f1_train: 0.8912 time: 0.2547s\n",
      "Epoch: 0684 loss_train: 0.0910 f1_train: 0.8839 time: 0.2405s\n",
      "Epoch: 0685 loss_train: 0.0909 f1_train: 0.8871 time: 0.2455s\n",
      "Epoch: 0686 loss_train: 0.0890 f1_train: 0.8884 time: 0.2414s\n",
      "Epoch: 0687 loss_train: 0.0898 f1_train: 0.8869 time: 0.2457s\n",
      "Epoch: 0688 loss_train: 0.0886 f1_train: 0.8876 time: 0.2463s\n",
      "Epoch: 0689 loss_train: 0.0891 f1_train: 0.8844 time: 0.2512s\n",
      "Epoch: 0690 loss_train: 0.0866 f1_train: 0.8901 time: 0.2495s\n",
      "Epoch: 0691 loss_train: 0.0869 f1_train: 0.8887 time: 0.2479s\n",
      "Epoch: 0692 loss_train: 0.0827 f1_train: 0.8965 time: 0.2527s\n",
      "Epoch: 0693 loss_train: 0.0865 f1_train: 0.8866 time: 0.2406s\n",
      "Epoch: 0694 loss_train: 0.0875 f1_train: 0.8884 time: 0.2389s\n",
      "Epoch: 0695 loss_train: 0.0864 f1_train: 0.8881 time: 0.2340s\n",
      "Epoch: 0696 loss_train: 0.0888 f1_train: 0.8907 time: 0.2388s\n",
      "Epoch: 0697 loss_train: 0.0891 f1_train: 0.8879 time: 0.2461s\n",
      "Epoch: 0698 loss_train: 0.0872 f1_train: 0.8892 time: 0.2425s\n",
      "Epoch: 0699 loss_train: 0.0881 f1_train: 0.8879 time: 0.2437s\n",
      "Epoch: 0700 loss_train: 0.0876 f1_train: 0.8909 time: 0.2492s\n",
      "Epoch: 0701 loss_train: 0.0867 f1_train: 0.8874 time: 0.2449s\n",
      "Epoch: 0702 loss_train: 0.0889 f1_train: 0.8895 time: 0.2451s\n",
      "Epoch: 0703 loss_train: 0.0866 f1_train: 0.8862 time: 0.2381s\n",
      "Epoch: 0704 loss_train: 0.0893 f1_train: 0.8835 time: 0.2390s\n",
      "Epoch: 0705 loss_train: 0.0871 f1_train: 0.8867 time: 0.2446s\n",
      "Epoch: 0706 loss_train: 0.0883 f1_train: 0.8884 time: 0.2355s\n",
      "Epoch: 0707 loss_train: 0.0873 f1_train: 0.8915 time: 0.2368s\n",
      "Epoch: 0708 loss_train: 0.0888 f1_train: 0.8917 time: 0.2605s\n",
      "Epoch: 0709 loss_train: 0.0878 f1_train: 0.8920 time: 0.2321s\n",
      "Epoch: 0710 loss_train: 0.0887 f1_train: 0.8864 time: 0.2503s\n",
      "Epoch: 0711 loss_train: 0.0864 f1_train: 0.8910 time: 0.2489s\n",
      "Epoch: 0712 loss_train: 0.0864 f1_train: 0.8944 time: 0.2372s\n",
      "Epoch: 0713 loss_train: 0.0861 f1_train: 0.8919 time: 0.2376s\n",
      "Epoch: 0714 loss_train: 0.0859 f1_train: 0.8943 time: 0.2429s\n",
      "Epoch: 0715 loss_train: 0.0880 f1_train: 0.8841 time: 0.2383s\n",
      "Epoch: 0716 loss_train: 0.0847 f1_train: 0.8898 time: 0.2368s\n",
      "Epoch: 0717 loss_train: 0.0864 f1_train: 0.8869 time: 0.2477s\n",
      "Epoch: 0718 loss_train: 0.0893 f1_train: 0.8864 time: 0.2424s\n",
      "Epoch: 0719 loss_train: 0.0851 f1_train: 0.8911 time: 0.2227s\n",
      "Epoch: 0720 loss_train: 0.0867 f1_train: 0.8902 time: 0.2408s\n",
      "Epoch: 0721 loss_train: 0.0846 f1_train: 0.8915 time: 0.2425s\n",
      "Epoch: 0722 loss_train: 0.0888 f1_train: 0.8871 time: 0.2359s\n",
      "Epoch: 0723 loss_train: 0.0868 f1_train: 0.8899 time: 0.2367s\n",
      "Epoch: 0724 loss_train: 0.0846 f1_train: 0.8954 time: 0.2384s\n",
      "Epoch: 0725 loss_train: 0.0849 f1_train: 0.8930 time: 0.2389s\n",
      "Epoch: 0726 loss_train: 0.0870 f1_train: 0.8892 time: 0.2379s\n",
      "Epoch: 0727 loss_train: 0.0850 f1_train: 0.8910 time: 0.2455s\n",
      "Epoch: 0728 loss_train: 0.0846 f1_train: 0.8924 time: 0.2405s\n",
      "Epoch: 0729 loss_train: 0.0863 f1_train: 0.8881 time: 0.2341s\n",
      "Epoch: 0730 loss_train: 0.0852 f1_train: 0.8891 time: 0.2448s\n",
      "Epoch: 0731 loss_train: 0.0837 f1_train: 0.8932 time: 0.2393s\n",
      "Epoch: 0732 loss_train: 0.0866 f1_train: 0.8908 time: 0.2348s\n",
      "Epoch: 0733 loss_train: 0.0869 f1_train: 0.8921 time: 0.2497s\n",
      "Epoch: 0734 loss_train: 0.0855 f1_train: 0.8901 time: 0.2434s\n",
      "Epoch: 0735 loss_train: 0.0863 f1_train: 0.8925 time: 0.2423s\n",
      "Epoch: 0736 loss_train: 0.0889 f1_train: 0.8906 time: 0.2363s\n",
      "Epoch: 0737 loss_train: 0.0844 f1_train: 0.8962 time: 0.2365s\n",
      "Epoch: 0738 loss_train: 0.0845 f1_train: 0.8906 time: 0.2435s\n",
      "Epoch: 0739 loss_train: 0.0850 f1_train: 0.8906 time: 0.2432s\n",
      "Epoch: 0740 loss_train: 0.0831 f1_train: 0.8979 time: 0.2277s\n",
      "Epoch: 0741 loss_train: 0.0860 f1_train: 0.8874 time: 0.1944s\n",
      "Epoch: 0742 loss_train: 0.0874 f1_train: 0.8900 time: 0.1880s\n",
      "Epoch: 0743 loss_train: 0.0840 f1_train: 0.8915 time: 0.1901s\n",
      "Epoch: 0744 loss_train: 0.0858 f1_train: 0.8892 time: 0.1868s\n",
      "Epoch: 0745 loss_train: 0.0842 f1_train: 0.8917 time: 0.1848s\n",
      "Epoch: 0746 loss_train: 0.0859 f1_train: 0.8928 time: 0.1851s\n",
      "Epoch: 0747 loss_train: 0.0865 f1_train: 0.8936 time: 0.1911s\n",
      "Epoch: 0748 loss_train: 0.0859 f1_train: 0.8911 time: 0.1924s\n",
      "Epoch: 0749 loss_train: 0.0831 f1_train: 0.8984 time: 0.1914s\n",
      "Epoch: 0750 loss_train: 0.0831 f1_train: 0.8962 time: 0.1866s\n",
      "Epoch: 0751 loss_train: 0.0849 f1_train: 0.8903 time: 0.1879s\n",
      "Epoch: 0752 loss_train: 0.0847 f1_train: 0.8915 time: 0.1862s\n",
      "Epoch: 0753 loss_train: 0.0818 f1_train: 0.8959 time: 0.1882s\n",
      "Epoch: 0754 loss_train: 0.0856 f1_train: 0.8946 time: 0.1858s\n",
      "Epoch: 0755 loss_train: 0.0853 f1_train: 0.8937 time: 0.1852s\n",
      "Epoch: 0756 loss_train: 0.0847 f1_train: 0.8921 time: 0.1885s\n",
      "Epoch: 0757 loss_train: 0.0850 f1_train: 0.8942 time: 0.1972s\n",
      "Epoch: 0758 loss_train: 0.0841 f1_train: 0.8934 time: 0.1962s\n",
      "Epoch: 0759 loss_train: 0.0831 f1_train: 0.8911 time: 0.1987s\n",
      "Epoch: 0760 loss_train: 0.0858 f1_train: 0.8928 time: 0.1867s\n",
      "Epoch: 0761 loss_train: 0.0851 f1_train: 0.8919 time: 0.1889s\n",
      "Epoch: 0762 loss_train: 0.0815 f1_train: 0.8970 time: 0.1864s\n",
      "Epoch: 0763 loss_train: 0.0837 f1_train: 0.8961 time: 0.1948s\n",
      "Epoch: 0764 loss_train: 0.0832 f1_train: 0.8949 time: 0.1858s\n",
      "Epoch: 0765 loss_train: 0.0844 f1_train: 0.8920 time: 0.1898s\n",
      "Epoch: 0766 loss_train: 0.0849 f1_train: 0.8963 time: 0.2223s\n",
      "Epoch: 0767 loss_train: 0.0857 f1_train: 0.8950 time: 0.2463s\n",
      "Epoch: 0768 loss_train: 0.0853 f1_train: 0.8897 time: 0.2521s\n",
      "Epoch: 0769 loss_train: 0.0831 f1_train: 0.8953 time: 0.2585s\n",
      "Epoch: 0770 loss_train: 0.0847 f1_train: 0.8929 time: 0.2507s\n",
      "Epoch: 0771 loss_train: 0.0824 f1_train: 0.8949 time: 0.2486s\n",
      "Epoch: 0772 loss_train: 0.0829 f1_train: 0.8956 time: 0.2540s\n",
      "Epoch: 0773 loss_train: 0.0842 f1_train: 0.8929 time: 0.2530s\n",
      "Epoch: 0774 loss_train: 0.0849 f1_train: 0.8899 time: 0.2510s\n",
      "Epoch: 0775 loss_train: 0.0859 f1_train: 0.8939 time: 0.2533s\n",
      "Epoch: 0776 loss_train: 0.0825 f1_train: 0.8908 time: 0.2505s\n",
      "Epoch: 0777 loss_train: 0.0818 f1_train: 0.8942 time: 0.2441s\n",
      "Epoch: 0778 loss_train: 0.0831 f1_train: 0.8933 time: 0.2671s\n",
      "Epoch: 0779 loss_train: 0.0832 f1_train: 0.8881 time: 0.2533s\n",
      "Epoch: 0780 loss_train: 0.0846 f1_train: 0.8919 time: 0.2387s\n",
      "Epoch: 0781 loss_train: 0.0831 f1_train: 0.8992 time: 0.2385s\n",
      "Epoch: 0782 loss_train: 0.0860 f1_train: 0.8919 time: 0.2401s\n",
      "Epoch: 0783 loss_train: 0.0840 f1_train: 0.8965 time: 0.2457s\n",
      "Epoch: 0784 loss_train: 0.0838 f1_train: 0.8958 time: 0.2397s\n",
      "Epoch: 0785 loss_train: 0.0850 f1_train: 0.8917 time: 0.2447s\n",
      "Epoch: 0786 loss_train: 0.0828 f1_train: 0.8950 time: 0.2701s\n",
      "Epoch: 0787 loss_train: 0.0846 f1_train: 0.8953 time: 0.2458s\n",
      "Epoch: 0788 loss_train: 0.0828 f1_train: 0.8963 time: 0.2601s\n",
      "Epoch: 0789 loss_train: 0.0829 f1_train: 0.8934 time: 0.2521s\n",
      "Epoch: 0790 loss_train: 0.0838 f1_train: 0.8918 time: 0.2605s\n",
      "Epoch: 0791 loss_train: 0.0822 f1_train: 0.8958 time: 0.2495s\n",
      "Epoch: 0792 loss_train: 0.0833 f1_train: 0.8950 time: 0.2653s\n",
      "Epoch: 0793 loss_train: 0.0849 f1_train: 0.8916 time: 0.2557s\n",
      "Epoch: 0794 loss_train: 0.0844 f1_train: 0.8930 time: 0.2487s\n",
      "Epoch: 0795 loss_train: 0.0835 f1_train: 0.8939 time: 0.2502s\n",
      "Epoch: 0796 loss_train: 0.0816 f1_train: 0.8929 time: 0.2743s\n",
      "Epoch: 0797 loss_train: 0.0829 f1_train: 0.8913 time: 0.2621s\n",
      "Epoch: 0798 loss_train: 0.0847 f1_train: 0.8949 time: 0.2432s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0799 loss_train: 0.0839 f1_train: 0.8908 time: 0.2475s\n",
      "Epoch: 0800 loss_train: 0.0827 f1_train: 0.8961 time: 0.2470s\n",
      "Epoch: 0801 loss_train: 0.0824 f1_train: 0.8956 time: 0.2492s\n",
      "Epoch: 0802 loss_train: 0.0825 f1_train: 0.8925 time: 0.2481s\n",
      "Epoch: 0803 loss_train: 0.0809 f1_train: 0.8981 time: 0.2524s\n",
      "Epoch: 0804 loss_train: 0.0827 f1_train: 0.8914 time: 0.2423s\n",
      "Epoch: 0805 loss_train: 0.0828 f1_train: 0.8934 time: 0.2445s\n",
      "Epoch: 0806 loss_train: 0.0819 f1_train: 0.8968 time: 0.2494s\n",
      "Epoch: 0807 loss_train: 0.0820 f1_train: 0.8928 time: 0.2483s\n",
      "Epoch: 0808 loss_train: 0.0826 f1_train: 0.8929 time: 0.2636s\n",
      "Epoch: 0809 loss_train: 0.0843 f1_train: 0.8878 time: 0.2445s\n",
      "Epoch: 0810 loss_train: 0.0822 f1_train: 0.8934 time: 0.2624s\n",
      "Epoch: 0811 loss_train: 0.0819 f1_train: 0.8959 time: 0.2580s\n",
      "Epoch: 0812 loss_train: 0.0824 f1_train: 0.8953 time: 0.2524s\n",
      "Epoch: 0813 loss_train: 0.0824 f1_train: 0.8944 time: 0.2755s\n",
      "Epoch: 0814 loss_train: 0.0841 f1_train: 0.8967 time: 0.2481s\n",
      "Epoch: 0815 loss_train: 0.0805 f1_train: 0.8955 time: 0.2714s\n",
      "Epoch: 0816 loss_train: 0.0817 f1_train: 0.8970 time: 0.2567s\n",
      "Epoch: 0817 loss_train: 0.0840 f1_train: 0.8951 time: 0.2500s\n",
      "Epoch: 0818 loss_train: 0.0813 f1_train: 0.8975 time: 0.2517s\n",
      "Epoch: 0819 loss_train: 0.0790 f1_train: 0.8993 time: 0.2697s\n",
      "Epoch: 0820 loss_train: 0.0825 f1_train: 0.8958 time: 0.2491s\n",
      "Epoch: 0821 loss_train: 0.0829 f1_train: 0.8911 time: 0.2552s\n",
      "Epoch: 0822 loss_train: 0.0818 f1_train: 0.8947 time: 0.2535s\n",
      "Epoch: 0823 loss_train: 0.0822 f1_train: 0.8927 time: 0.2614s\n",
      "Epoch: 0824 loss_train: 0.0799 f1_train: 0.8959 time: 0.2512s\n",
      "Epoch: 0825 loss_train: 0.0831 f1_train: 0.8908 time: 0.2409s\n",
      "Epoch: 0826 loss_train: 0.0787 f1_train: 0.8991 time: 0.2580s\n",
      "Epoch: 0827 loss_train: 0.0801 f1_train: 0.8964 time: 0.2458s\n",
      "Epoch: 0828 loss_train: 0.0828 f1_train: 0.8942 time: 0.2501s\n",
      "Epoch: 0829 loss_train: 0.0819 f1_train: 0.8992 time: 0.2533s\n",
      "Epoch: 0830 loss_train: 0.0801 f1_train: 0.8956 time: 0.2550s\n",
      "Epoch: 0831 loss_train: 0.0807 f1_train: 0.8950 time: 0.2519s\n",
      "Epoch: 0832 loss_train: 0.0822 f1_train: 0.8967 time: 0.2617s\n",
      "Epoch: 0833 loss_train: 0.0808 f1_train: 0.8966 time: 0.2612s\n",
      "Epoch: 0834 loss_train: 0.0808 f1_train: 0.8987 time: 0.2469s\n",
      "Epoch: 0835 loss_train: 0.0807 f1_train: 0.8996 time: 0.2566s\n",
      "Epoch: 0836 loss_train: 0.0792 f1_train: 0.8978 time: 0.2478s\n",
      "Epoch: 0837 loss_train: 0.0810 f1_train: 0.8952 time: 0.2661s\n",
      "Epoch: 0838 loss_train: 0.0812 f1_train: 0.8983 time: 0.2475s\n",
      "Epoch: 0839 loss_train: 0.0814 f1_train: 0.8999 time: 0.2594s\n",
      "Epoch: 0840 loss_train: 0.0805 f1_train: 0.8937 time: 0.2554s\n",
      "Epoch: 0841 loss_train: 0.0804 f1_train: 0.8953 time: 0.2638s\n",
      "Epoch: 0842 loss_train: 0.0806 f1_train: 0.8955 time: 0.2561s\n",
      "Epoch: 0843 loss_train: 0.0789 f1_train: 0.9012 time: 0.2750s\n",
      "Epoch: 0844 loss_train: 0.0811 f1_train: 0.8956 time: 0.2637s\n",
      "Epoch: 0845 loss_train: 0.0789 f1_train: 0.8977 time: 0.2515s\n",
      "Epoch: 0846 loss_train: 0.0797 f1_train: 0.8999 time: 0.2473s\n",
      "Epoch: 0847 loss_train: 0.0810 f1_train: 0.8928 time: 0.2625s\n",
      "Epoch: 0848 loss_train: 0.0812 f1_train: 0.8944 time: 0.2629s\n",
      "Epoch: 0849 loss_train: 0.0819 f1_train: 0.9005 time: 0.2528s\n",
      "Epoch: 0850 loss_train: 0.0819 f1_train: 0.8914 time: 0.2555s\n",
      "Epoch: 0851 loss_train: 0.0803 f1_train: 0.8960 time: 0.2745s\n",
      "Epoch: 0852 loss_train: 0.0812 f1_train: 0.8940 time: 0.2595s\n",
      "Epoch: 0853 loss_train: 0.0794 f1_train: 0.8995 time: 0.2533s\n",
      "Epoch: 0854 loss_train: 0.0783 f1_train: 0.8985 time: 0.2586s\n",
      "Epoch: 0855 loss_train: 0.0792 f1_train: 0.9000 time: 0.2617s\n",
      "Epoch: 0856 loss_train: 0.0792 f1_train: 0.9004 time: 0.2542s\n",
      "Epoch: 0857 loss_train: 0.0813 f1_train: 0.8993 time: 0.2569s\n",
      "Epoch: 0858 loss_train: 0.0827 f1_train: 0.8928 time: 0.2519s\n",
      "Epoch: 0859 loss_train: 0.0818 f1_train: 0.8935 time: 0.2516s\n",
      "Epoch: 0860 loss_train: 0.0804 f1_train: 0.8980 time: 0.2629s\n",
      "Epoch: 0861 loss_train: 0.0800 f1_train: 0.8999 time: 0.2578s\n",
      "Epoch: 0862 loss_train: 0.0809 f1_train: 0.8963 time: 0.2542s\n",
      "Epoch: 0863 loss_train: 0.0811 f1_train: 0.8964 time: 0.2590s\n",
      "Epoch: 0864 loss_train: 0.0787 f1_train: 0.9008 time: 0.2484s\n",
      "Epoch: 0865 loss_train: 0.0782 f1_train: 0.9008 time: 0.2515s\n",
      "Epoch: 0866 loss_train: 0.0800 f1_train: 0.8971 time: 0.2563s\n",
      "Epoch: 0867 loss_train: 0.0793 f1_train: 0.8966 time: 0.2570s\n",
      "Epoch: 0868 loss_train: 0.0804 f1_train: 0.8954 time: 0.2657s\n",
      "Epoch: 0869 loss_train: 0.0773 f1_train: 0.9000 time: 0.2522s\n",
      "Epoch: 0870 loss_train: 0.0819 f1_train: 0.8957 time: 0.2503s\n",
      "Epoch: 0871 loss_train: 0.0788 f1_train: 0.8988 time: 0.2509s\n",
      "Epoch: 0872 loss_train: 0.0795 f1_train: 0.8982 time: 0.2573s\n",
      "Epoch: 0873 loss_train: 0.0771 f1_train: 0.9017 time: 0.2532s\n",
      "Epoch: 0874 loss_train: 0.0798 f1_train: 0.9012 time: 0.2630s\n",
      "Epoch: 0875 loss_train: 0.0786 f1_train: 0.9029 time: 0.2606s\n",
      "Epoch: 0876 loss_train: 0.0805 f1_train: 0.8993 time: 0.2512s\n",
      "Epoch: 0877 loss_train: 0.0792 f1_train: 0.8980 time: 0.2556s\n",
      "Epoch: 0878 loss_train: 0.0817 f1_train: 0.8984 time: 0.2536s\n",
      "Epoch: 0879 loss_train: 0.0803 f1_train: 0.8980 time: 0.2564s\n",
      "Epoch: 0880 loss_train: 0.0790 f1_train: 0.8996 time: 0.2512s\n",
      "Epoch: 0881 loss_train: 0.0778 f1_train: 0.9008 time: 0.2529s\n",
      "Epoch: 0882 loss_train: 0.0772 f1_train: 0.9013 time: 0.2668s\n",
      "Epoch: 0883 loss_train: 0.0784 f1_train: 0.9017 time: 0.2768s\n",
      "Epoch: 0884 loss_train: 0.0781 f1_train: 0.9013 time: 0.2577s\n",
      "Epoch: 0885 loss_train: 0.0778 f1_train: 0.9012 time: 0.2580s\n",
      "Epoch: 0886 loss_train: 0.0784 f1_train: 0.9010 time: 0.2547s\n",
      "Epoch: 0887 loss_train: 0.0775 f1_train: 0.9033 time: 0.2525s\n",
      "Epoch: 0888 loss_train: 0.0797 f1_train: 0.8963 time: 0.2529s\n",
      "Epoch: 0889 loss_train: 0.0787 f1_train: 0.9005 time: 0.2528s\n",
      "Epoch: 0890 loss_train: 0.0763 f1_train: 0.9003 time: 0.2562s\n",
      "Epoch: 0891 loss_train: 0.0771 f1_train: 0.9021 time: 0.2552s\n",
      "Epoch: 0892 loss_train: 0.0814 f1_train: 0.8988 time: 0.2766s\n",
      "Epoch: 0893 loss_train: 0.0777 f1_train: 0.9024 time: 0.2559s\n",
      "Epoch: 0894 loss_train: 0.0805 f1_train: 0.8967 time: 0.2619s\n",
      "Epoch: 0895 loss_train: 0.0763 f1_train: 0.9004 time: 0.2608s\n",
      "Epoch: 0896 loss_train: 0.0770 f1_train: 0.8956 time: 0.2572s\n",
      "Epoch: 0897 loss_train: 0.0778 f1_train: 0.9003 time: 0.2609s\n",
      "Epoch: 0898 loss_train: 0.0811 f1_train: 0.8993 time: 0.2631s\n",
      "Epoch: 0899 loss_train: 0.0775 f1_train: 0.9036 time: 0.2558s\n",
      "Epoch: 0900 loss_train: 0.0771 f1_train: 0.9008 time: 0.2578s\n",
      "Epoch: 0901 loss_train: 0.0773 f1_train: 0.9026 time: 0.2668s\n",
      "Epoch: 0902 loss_train: 0.0763 f1_train: 0.9023 time: 0.2620s\n",
      "Epoch: 0903 loss_train: 0.0750 f1_train: 0.9040 time: 0.2608s\n",
      "Epoch: 0904 loss_train: 0.0782 f1_train: 0.8995 time: 0.2539s\n",
      "Epoch: 0905 loss_train: 0.0798 f1_train: 0.8968 time: 0.2666s\n",
      "Epoch: 0906 loss_train: 0.0781 f1_train: 0.9002 time: 0.2676s\n",
      "Epoch: 0907 loss_train: 0.0806 f1_train: 0.8993 time: 0.2529s\n",
      "Epoch: 0908 loss_train: 0.0789 f1_train: 0.8989 time: 0.2615s\n",
      "Epoch: 0909 loss_train: 0.0772 f1_train: 0.9028 time: 0.2644s\n",
      "Epoch: 0910 loss_train: 0.0780 f1_train: 0.9004 time: 0.2612s\n",
      "Epoch: 0911 loss_train: 0.0789 f1_train: 0.8989 time: 0.2487s\n",
      "Epoch: 0912 loss_train: 0.0788 f1_train: 0.9019 time: 0.2536s\n",
      "Epoch: 0913 loss_train: 0.0794 f1_train: 0.8993 time: 0.2622s\n",
      "Epoch: 0914 loss_train: 0.0780 f1_train: 0.9002 time: 0.2531s\n",
      "Epoch: 0915 loss_train: 0.0783 f1_train: 0.9002 time: 0.2598s\n",
      "Epoch: 0916 loss_train: 0.0779 f1_train: 0.8987 time: 0.2580s\n",
      "Epoch: 0917 loss_train: 0.0781 f1_train: 0.9010 time: 0.2607s\n",
      "Epoch: 0918 loss_train: 0.0789 f1_train: 0.8986 time: 0.2574s\n",
      "Epoch: 0919 loss_train: 0.0774 f1_train: 0.9018 time: 0.2570s\n",
      "Epoch: 0920 loss_train: 0.0781 f1_train: 0.8966 time: 0.2536s\n",
      "Epoch: 0921 loss_train: 0.0798 f1_train: 0.8983 time: 0.2543s\n",
      "Epoch: 0922 loss_train: 0.0786 f1_train: 0.8955 time: 0.2505s\n",
      "Epoch: 0923 loss_train: 0.0794 f1_train: 0.8950 time: 0.2522s\n",
      "Epoch: 0924 loss_train: 0.0760 f1_train: 0.9010 time: 0.2655s\n",
      "Epoch: 0925 loss_train: 0.0785 f1_train: 0.9003 time: 0.2531s\n",
      "Epoch: 0926 loss_train: 0.0775 f1_train: 0.8963 time: 0.2500s\n",
      "Epoch: 0927 loss_train: 0.0777 f1_train: 0.9002 time: 0.2486s\n",
      "Epoch: 0928 loss_train: 0.0802 f1_train: 0.9000 time: 0.2414s\n",
      "Epoch: 0929 loss_train: 0.0785 f1_train: 0.8989 time: 0.2491s\n",
      "Epoch: 0930 loss_train: 0.0769 f1_train: 0.9010 time: 0.2629s\n",
      "Epoch: 0931 loss_train: 0.0814 f1_train: 0.8978 time: 0.2466s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0932 loss_train: 0.0777 f1_train: 0.8959 time: 0.2683s\n",
      "Epoch: 0933 loss_train: 0.0793 f1_train: 0.8987 time: 0.2569s\n",
      "Epoch: 0934 loss_train: 0.0749 f1_train: 0.9023 time: 0.2651s\n",
      "Epoch: 0935 loss_train: 0.0773 f1_train: 0.8999 time: 0.2460s\n",
      "Epoch: 0936 loss_train: 0.0765 f1_train: 0.9012 time: 0.2684s\n",
      "Epoch: 0937 loss_train: 0.0760 f1_train: 0.9011 time: 0.2576s\n",
      "Epoch: 0938 loss_train: 0.0765 f1_train: 0.9031 time: 0.2808s\n",
      "Epoch: 0939 loss_train: 0.0744 f1_train: 0.9025 time: 0.2654s\n",
      "Epoch: 0940 loss_train: 0.0755 f1_train: 0.9033 time: 0.2847s\n",
      "Epoch: 0941 loss_train: 0.0763 f1_train: 0.9050 time: 0.2480s\n",
      "Epoch: 0942 loss_train: 0.0764 f1_train: 0.9000 time: 0.2625s\n",
      "Epoch: 0943 loss_train: 0.0779 f1_train: 0.9013 time: 0.2540s\n",
      "Epoch: 0944 loss_train: 0.0764 f1_train: 0.9030 time: 0.2514s\n",
      "Epoch: 0945 loss_train: 0.0760 f1_train: 0.9050 time: 0.2582s\n",
      "Epoch: 0946 loss_train: 0.0758 f1_train: 0.9039 time: 0.2505s\n",
      "Epoch: 0947 loss_train: 0.0757 f1_train: 0.8996 time: 0.2498s\n",
      "Epoch: 0948 loss_train: 0.0757 f1_train: 0.9043 time: 0.2524s\n",
      "Epoch: 0949 loss_train: 0.0786 f1_train: 0.9010 time: 0.2482s\n",
      "Epoch: 0950 loss_train: 0.0732 f1_train: 0.9045 time: 0.2500s\n",
      "Epoch: 0951 loss_train: 0.0751 f1_train: 0.9021 time: 0.2560s\n",
      "Epoch: 0952 loss_train: 0.0758 f1_train: 0.9057 time: 0.2608s\n",
      "Epoch: 0953 loss_train: 0.0754 f1_train: 0.9048 time: 0.2602s\n",
      "Epoch: 0954 loss_train: 0.0771 f1_train: 0.8996 time: 0.2632s\n",
      "Epoch: 0955 loss_train: 0.0760 f1_train: 0.9090 time: 0.2601s\n",
      "Epoch: 0956 loss_train: 0.0773 f1_train: 0.8981 time: 0.2557s\n",
      "Epoch: 0957 loss_train: 0.0761 f1_train: 0.9014 time: 0.2586s\n",
      "Epoch: 0958 loss_train: 0.0746 f1_train: 0.9054 time: 0.2389s\n",
      "Epoch: 0959 loss_train: 0.0781 f1_train: 0.8971 time: 0.2562s\n",
      "Epoch: 0960 loss_train: 0.0794 f1_train: 0.9019 time: 0.2665s\n",
      "Epoch: 0961 loss_train: 0.0777 f1_train: 0.8979 time: 0.2554s\n",
      "Epoch: 0962 loss_train: 0.0764 f1_train: 0.9026 time: 0.2653s\n",
      "Epoch: 0963 loss_train: 0.0735 f1_train: 0.9046 time: 0.2634s\n",
      "Epoch: 0964 loss_train: 0.0744 f1_train: 0.9070 time: 0.2523s\n",
      "Epoch: 0965 loss_train: 0.0767 f1_train: 0.9031 time: 0.2489s\n",
      "Epoch: 0966 loss_train: 0.0753 f1_train: 0.9023 time: 0.2515s\n",
      "Epoch: 0967 loss_train: 0.0745 f1_train: 0.9057 time: 0.2646s\n",
      "Epoch: 0968 loss_train: 0.0732 f1_train: 0.9059 time: 0.2610s\n",
      "Epoch: 0969 loss_train: 0.0773 f1_train: 0.8993 time: 0.2628s\n",
      "Epoch: 0970 loss_train: 0.0756 f1_train: 0.9027 time: 0.2716s\n",
      "Epoch: 0971 loss_train: 0.0757 f1_train: 0.9054 time: 0.2772s\n",
      "Epoch: 0972 loss_train: 0.0740 f1_train: 0.9041 time: 0.2653s\n",
      "Epoch: 0973 loss_train: 0.0757 f1_train: 0.9017 time: 0.2596s\n",
      "Epoch: 0974 loss_train: 0.0734 f1_train: 0.9002 time: 0.2629s\n",
      "Epoch: 0975 loss_train: 0.0753 f1_train: 0.9025 time: 0.2610s\n",
      "Epoch: 0976 loss_train: 0.0745 f1_train: 0.9021 time: 0.2661s\n",
      "Epoch: 0977 loss_train: 0.0769 f1_train: 0.9035 time: 0.2758s\n",
      "Epoch: 0978 loss_train: 0.0749 f1_train: 0.9087 time: 0.2646s\n",
      "Epoch: 0979 loss_train: 0.0749 f1_train: 0.9045 time: 0.2600s\n",
      "Epoch: 0980 loss_train: 0.0729 f1_train: 0.9086 time: 0.2679s\n",
      "Epoch: 0981 loss_train: 0.0755 f1_train: 0.9018 time: 0.2550s\n",
      "Epoch: 0982 loss_train: 0.0737 f1_train: 0.9023 time: 0.2548s\n",
      "Epoch: 0983 loss_train: 0.0758 f1_train: 0.9002 time: 0.2472s\n",
      "Epoch: 0984 loss_train: 0.0782 f1_train: 0.8994 time: 0.2521s\n",
      "Epoch: 0985 loss_train: 0.0740 f1_train: 0.9028 time: 0.2535s\n",
      "Epoch: 0986 loss_train: 0.0751 f1_train: 0.9046 time: 0.2498s\n",
      "Epoch: 0987 loss_train: 0.0734 f1_train: 0.9055 time: 0.2795s\n",
      "Epoch: 0988 loss_train: 0.0747 f1_train: 0.9021 time: 0.2622s\n",
      "Epoch: 0989 loss_train: 0.0758 f1_train: 0.9024 time: 0.2645s\n",
      "Epoch: 0990 loss_train: 0.0741 f1_train: 0.9031 time: 0.2514s\n",
      "Epoch: 0991 loss_train: 0.0753 f1_train: 0.9051 time: 0.2533s\n",
      "Epoch: 0992 loss_train: 0.0752 f1_train: 0.9070 time: 0.2517s\n",
      "Epoch: 0993 loss_train: 0.0742 f1_train: 0.9046 time: 0.2571s\n",
      "Epoch: 0994 loss_train: 0.0754 f1_train: 0.9060 time: 0.2575s\n",
      "Epoch: 0995 loss_train: 0.0741 f1_train: 0.9060 time: 0.2678s\n",
      "Epoch: 0996 loss_train: 0.0744 f1_train: 0.9030 time: 0.2779s\n",
      "Epoch: 0997 loss_train: 0.0728 f1_train: 0.9049 time: 0.2743s\n",
      "Epoch: 0998 loss_train: 0.0753 f1_train: 0.9022 time: 0.2549s\n",
      "Epoch: 0999 loss_train: 0.0740 f1_train: 0.9068 time: 0.2497s\n",
      "Epoch: 1000 loss_train: 0.0765 f1_train: 0.9026 time: 0.2672s\n",
      "Optimization Finished!\n",
      "torch.Size([29894, 100])\n"
     ]
    }
   ],
   "source": [
    "# setup training \n",
    "epochs = 1000\n",
    "best_loss = 1\n",
    "node_emb_train = None \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    gcn.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = gcn(features, adj)\n",
    "    loss_train = loss(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    f1_train = fscore(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epoch_finish = time.time() - epoch_start\n",
    "\n",
    "    if best_loss >= loss_train.item():\n",
    "        node_emb_train = gcn.node_embeddings[idx_train]\n",
    "\n",
    "    print(\n",
    "        \"Epoch: {:04d}\".format(epoch+1),\n",
    "        \"loss_train: {:.4f}\".format(loss_train.item()),\n",
    "        \"f1_train: {:.4f}\".format(f1_train),\n",
    "        \"time: {:.4f}s\".format(epoch_finish)\n",
    "    )\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(node_emb_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.4403 precision= 0.8236 recall= 0.5217 f1_test= 0.6388 f1_micro= 0.9617 confusion= [[15466   121]\n",
      " [  518   565]]\n"
     ]
    }
   ],
   "source": [
    "node_emb_test = None \n",
    "gcn.eval()\n",
    "output = gcn(features, adj)\n",
    "loss_test = loss(output[idx_test], labels[idx_test])\n",
    "\n",
    "\n",
    "precision_score = precision(output[idx_test], labels[idx_test])\n",
    "recall_score = recall(output[idx_test], labels[idx_test])\n",
    "f1_test = fscore(output[idx_test], labels[idx_test])\n",
    "f1_micro = fscore_micro(output[idx_test], labels[idx_test])\n",
    "confusion_score = confusion(output[idx_test], labels[idx_test])\n",
    "\n",
    "node_emb_test = gcn.node_embeddings[idx_test]\n",
    "print(\n",
    "    \"Test set results:\",\n",
    "    \"loss= {:.4f}\".format(loss_test.item()),\n",
    "    \"precision= {:.4f}\".format(precision_score),\n",
    "    \"recall= {:.4f}\".format(recall_score),\n",
    "    \"f1_test= {:.4f}\".format(f1_test),\n",
    "    \"f1_micro= {:.4f}\".format(f1_micro),\n",
    "    \"confusion= {}\".format(confusion_score)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0144, 0.0000, 0.0000,  ..., 0.3601, 0.0100, 0.0362],\n",
      "        [0.0145, 0.0641, 0.0000,  ..., 0.1262, 0.0000, 0.0000],\n",
      "        [0.6548, 0.0000, 0.3025,  ..., 0.0000, 0.2569, 0.1477],\n",
      "        ...,\n",
      "        [1.8311, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0885, 0.0000, 2.3265,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.5055, 0.6984, 1.6701,  ..., 0.0589, 0.0000, 0.0000]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "tensor([[1.2898, 0.0000, 1.8571,  ..., 2.6637, 0.0000, 0.0000],\n",
      "        [1.9974, 1.3258, 1.9122,  ..., 0.9287, 0.8299, 0.0000],\n",
      "        [2.0133, 1.4777, 1.7579,  ..., 0.9109, 0.6846, 0.0000],\n",
      "        ...,\n",
      "        [0.1266, 0.2627, 0.5814,  ..., 0.0997, 0.0000, 0.0000],\n",
      "        [1.1080, 0.7954, 1.6985,  ..., 1.0842, 0.6903, 0.0000],\n",
      "        [0.1266, 0.4636, 0.7815,  ..., 0.0997, 0.0000, 0.0000]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "[[0.01441987 0.         0.         ... 0.3600765  0.01003565 0.0362152 ]\n",
      " [0.01452639 0.06412103 0.         ... 0.1262463  0.         0.        ]\n",
      " [0.6547941  0.         0.30248463 ... 0.         0.25693938 0.14774007]\n",
      " ...\n",
      " [0.12663977 0.26267192 0.58144605 ... 0.09966677 0.         0.        ]\n",
      " [1.1079907  0.7953696  1.69855    ... 1.0842178  0.6903064  0.        ]\n",
      " [0.12663977 0.4636023  0.7815089  ... 0.09966677 0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NE_0</th>\n",
       "      <th>NE_1</th>\n",
       "      <th>NE_2</th>\n",
       "      <th>NE_3</th>\n",
       "      <th>NE_4</th>\n",
       "      <th>NE_5</th>\n",
       "      <th>NE_6</th>\n",
       "      <th>NE_7</th>\n",
       "      <th>NE_8</th>\n",
       "      <th>NE_9</th>\n",
       "      <th>...</th>\n",
       "      <th>NE_90</th>\n",
       "      <th>NE_91</th>\n",
       "      <th>NE_92</th>\n",
       "      <th>NE_93</th>\n",
       "      <th>NE_94</th>\n",
       "      <th>NE_95</th>\n",
       "      <th>NE_96</th>\n",
       "      <th>NE_97</th>\n",
       "      <th>NE_98</th>\n",
       "      <th>NE_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420473</td>\n",
       "      <td>0.342348</td>\n",
       "      <td>1.147490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.109308</td>\n",
       "      <td>0.028354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232963</td>\n",
       "      <td>0.137486</td>\n",
       "      <td>0.135894</td>\n",
       "      <td>0.089023</td>\n",
       "      <td>0.950082</td>\n",
       "      <td>1.042759</td>\n",
       "      <td>0.150506</td>\n",
       "      <td>0.360076</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.036215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014526</td>\n",
       "      <td>0.064121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509672</td>\n",
       "      <td>0.080512</td>\n",
       "      <td>0.452515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132689</td>\n",
       "      <td>0.043171</td>\n",
       "      <td>0.080764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439266</td>\n",
       "      <td>0.804838</td>\n",
       "      <td>0.192160</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>1.576531</td>\n",
       "      <td>0.285096</td>\n",
       "      <td>0.206587</td>\n",
       "      <td>0.126246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.654794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>0.098832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080809</td>\n",
       "      <td>0.352127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.048357</td>\n",
       "      <td>0.086639</td>\n",
       "      <td>0.354941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256939</td>\n",
       "      <td>0.147740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.988228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.048357</td>\n",
       "      <td>0.086639</td>\n",
       "      <td>0.354941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256939</td>\n",
       "      <td>0.237226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.024098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.065698</td>\n",
       "      <td>0.595967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.914960</td>\n",
       "      <td>0.991711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.853590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.902895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46559</th>\n",
       "      <td>0.296902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913962</td>\n",
       "      <td>1.412212</td>\n",
       "      <td>3.264163</td>\n",
       "      <td>2.379227</td>\n",
       "      <td>0.816699</td>\n",
       "      <td>0.140808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605920</td>\n",
       "      <td>0.312746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46560</th>\n",
       "      <td>0.296902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583974</td>\n",
       "      <td>0.749661</td>\n",
       "      <td>3.091289</td>\n",
       "      <td>2.040888</td>\n",
       "      <td>0.938242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142135</td>\n",
       "      <td>0.312746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46561</th>\n",
       "      <td>0.126640</td>\n",
       "      <td>0.262672</td>\n",
       "      <td>0.581446</td>\n",
       "      <td>0.642942</td>\n",
       "      <td>3.248808</td>\n",
       "      <td>1.313747</td>\n",
       "      <td>0.346274</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46562</th>\n",
       "      <td>1.107991</td>\n",
       "      <td>0.795370</td>\n",
       "      <td>1.698550</td>\n",
       "      <td>0.523229</td>\n",
       "      <td>1.684893</td>\n",
       "      <td>1.138802</td>\n",
       "      <td>0.792776</td>\n",
       "      <td>0.818324</td>\n",
       "      <td>0.538285</td>\n",
       "      <td>1.661696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.033117</td>\n",
       "      <td>1.487325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.084218</td>\n",
       "      <td>0.690306</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46563</th>\n",
       "      <td>0.126640</td>\n",
       "      <td>0.463602</td>\n",
       "      <td>0.781509</td>\n",
       "      <td>0.519113</td>\n",
       "      <td>3.366935</td>\n",
       "      <td>1.119092</td>\n",
       "      <td>0.481031</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46564 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           NE_0      NE_1      NE_2      NE_3      NE_4      NE_5      NE_6  \\\n",
       "0      0.014420  0.000000  0.000000  0.420473  0.342348  1.147490  0.000000   \n",
       "1      0.014526  0.064121  0.000000  0.509672  0.080512  0.452515  0.000000   \n",
       "2      0.654794  0.000000  0.302485  0.000000  0.000000  0.014492  0.098832   \n",
       "3      0.988228  0.000000  0.357932  0.000000  0.000000  0.000000  0.098832   \n",
       "4      1.024098  0.000000  0.000000  1.065698  0.595967  0.000000  0.000000   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "46559  0.296902  0.000000  0.913962  1.412212  3.264163  2.379227  0.816699   \n",
       "46560  0.296902  0.000000  0.583974  0.749661  3.091289  2.040888  0.938242   \n",
       "46561  0.126640  0.262672  0.581446  0.642942  3.248808  1.313747  0.346274   \n",
       "46562  1.107991  0.795370  1.698550  0.523229  1.684893  1.138802  0.792776   \n",
       "46563  0.126640  0.463602  0.781509  0.519113  3.366935  1.119092  0.481031   \n",
       "\n",
       "           NE_7      NE_8      NE_9  ...     NE_90     NE_91     NE_92  \\\n",
       "0      0.001206  0.109308  0.028354  ...  0.232963  0.137486  0.135894   \n",
       "1      0.132689  0.043171  0.080764  ...  0.439266  0.804838  0.192160   \n",
       "2      0.000000  0.000000  0.000000  ...  0.080809  0.352127  0.000000   \n",
       "3      0.000000  0.000000  0.000000  ...  0.000000  0.352127  0.000000   \n",
       "4      0.479150  0.000000  0.000000  ...  0.000000  0.914960  0.991711   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "46559  0.140808  0.000000  0.510657  ...  0.153997  0.000000  0.605920   \n",
       "46560  0.000000  0.000000  0.093894  ...  0.226862  0.000000  0.142135   \n",
       "46561  0.010555  0.000000  0.103225  ...  0.922574  0.000000  0.003342   \n",
       "46562  0.818324  0.538285  1.661696  ...  0.108614  0.000000  1.033117   \n",
       "46563  0.010555  0.000000  0.103225  ...  0.716783  0.000000  0.003342   \n",
       "\n",
       "          NE_93     NE_94     NE_95     NE_96     NE_97     NE_98     NE_99  \n",
       "0      0.089023  0.950082  1.042759  0.150506  0.360076  0.010036  0.036215  \n",
       "1      0.942009  1.576531  0.285096  0.206587  0.126246  0.000000  0.000000  \n",
       "2      0.000000  1.048357  0.086639  0.354941  0.000000  0.256939  0.147740  \n",
       "3      0.000000  1.048357  0.086639  0.354941  0.000000  0.256939  0.237226  \n",
       "4      0.000000  2.853590  0.000000  2.902895  0.000000  0.000000  0.000000  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "46559  0.312746  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "46560  0.312746  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "46561  0.000000  0.000000  0.000000  0.000000  0.099667  0.000000  0.000000  \n",
       "46562  1.487325  0.000000  0.517031  0.000000  1.084218  0.690306  0.000000  \n",
       "46563  0.000000  0.000000  0.000000  0.000000  0.099667  0.000000  0.000000  \n",
       "\n",
       "[46564 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(node_emb_train)\n",
    "print(node_emb_test)\n",
    "\n",
    "np_node_emb = np.concatenate((node_emb_train.cpu().detach().numpy(), \n",
    "                            node_emb_test.cpu().detach().numpy()))\n",
    "\n",
    "print(np_node_emb)\n",
    "\n",
    "# Create embeddings pandas DataFrame \n",
    "node_emb_pd = pd.DataFrame(np_node_emb) \n",
    "node_emb_pd.columns = [f\"NE_{i}\" for i in range(np_node_emb.shape[1])]\n",
    "display(node_emb_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[232438397 232029206 232344069 ... 158375075 147478192 158375402]\n"
     ]
    }
   ],
   "source": [
    "data = ellipticdr.dataset_.copy()\n",
    "txIds = data[(data[\"class\"] != -1)][\"txId\"].values\n",
    "node_emb_pd.insert(0, \"txId\", txIds)\n",
    "print(txIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>NE_0</th>\n",
       "      <th>NE_1</th>\n",
       "      <th>NE_2</th>\n",
       "      <th>NE_3</th>\n",
       "      <th>NE_4</th>\n",
       "      <th>NE_5</th>\n",
       "      <th>NE_6</th>\n",
       "      <th>NE_7</th>\n",
       "      <th>NE_8</th>\n",
       "      <th>...</th>\n",
       "      <th>NE_90</th>\n",
       "      <th>NE_91</th>\n",
       "      <th>NE_92</th>\n",
       "      <th>NE_93</th>\n",
       "      <th>NE_94</th>\n",
       "      <th>NE_95</th>\n",
       "      <th>NE_96</th>\n",
       "      <th>NE_97</th>\n",
       "      <th>NE_98</th>\n",
       "      <th>NE_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232438397</td>\n",
       "      <td>0.014420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420473</td>\n",
       "      <td>0.342348</td>\n",
       "      <td>1.147490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.109308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232963</td>\n",
       "      <td>0.137486</td>\n",
       "      <td>0.135894</td>\n",
       "      <td>0.089023</td>\n",
       "      <td>0.950082</td>\n",
       "      <td>1.042759</td>\n",
       "      <td>0.150506</td>\n",
       "      <td>0.360076</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.036215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232029206</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>0.064121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509672</td>\n",
       "      <td>0.080512</td>\n",
       "      <td>0.452515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132689</td>\n",
       "      <td>0.043171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439266</td>\n",
       "      <td>0.804838</td>\n",
       "      <td>0.192160</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>1.576531</td>\n",
       "      <td>0.285096</td>\n",
       "      <td>0.206587</td>\n",
       "      <td>0.126246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232344069</td>\n",
       "      <td>0.654794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>0.098832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080809</td>\n",
       "      <td>0.352127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.048357</td>\n",
       "      <td>0.086639</td>\n",
       "      <td>0.354941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256939</td>\n",
       "      <td>0.147740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27553029</td>\n",
       "      <td>0.988228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.048357</td>\n",
       "      <td>0.086639</td>\n",
       "      <td>0.354941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256939</td>\n",
       "      <td>0.237226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3881097</td>\n",
       "      <td>1.024098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.065698</td>\n",
       "      <td>0.595967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.914960</td>\n",
       "      <td>0.991711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.853590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.902895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46559</th>\n",
       "      <td>80329479</td>\n",
       "      <td>0.296902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913962</td>\n",
       "      <td>1.412212</td>\n",
       "      <td>3.264163</td>\n",
       "      <td>2.379227</td>\n",
       "      <td>0.816699</td>\n",
       "      <td>0.140808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605920</td>\n",
       "      <td>0.312746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46560</th>\n",
       "      <td>158406298</td>\n",
       "      <td>0.296902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583974</td>\n",
       "      <td>0.749661</td>\n",
       "      <td>3.091289</td>\n",
       "      <td>2.040888</td>\n",
       "      <td>0.938242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142135</td>\n",
       "      <td>0.312746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46561</th>\n",
       "      <td>158375075</td>\n",
       "      <td>0.126640</td>\n",
       "      <td>0.262672</td>\n",
       "      <td>0.581446</td>\n",
       "      <td>0.642942</td>\n",
       "      <td>3.248808</td>\n",
       "      <td>1.313747</td>\n",
       "      <td>0.346274</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46562</th>\n",
       "      <td>147478192</td>\n",
       "      <td>1.107991</td>\n",
       "      <td>0.795370</td>\n",
       "      <td>1.698550</td>\n",
       "      <td>0.523229</td>\n",
       "      <td>1.684893</td>\n",
       "      <td>1.138802</td>\n",
       "      <td>0.792776</td>\n",
       "      <td>0.818324</td>\n",
       "      <td>0.538285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.033117</td>\n",
       "      <td>1.487325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.084218</td>\n",
       "      <td>0.690306</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46563</th>\n",
       "      <td>158375402</td>\n",
       "      <td>0.126640</td>\n",
       "      <td>0.463602</td>\n",
       "      <td>0.781509</td>\n",
       "      <td>0.519113</td>\n",
       "      <td>3.366935</td>\n",
       "      <td>1.119092</td>\n",
       "      <td>0.481031</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46564 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            txId      NE_0      NE_1      NE_2      NE_3      NE_4      NE_5  \\\n",
       "0      232438397  0.014420  0.000000  0.000000  0.420473  0.342348  1.147490   \n",
       "1      232029206  0.014526  0.064121  0.000000  0.509672  0.080512  0.452515   \n",
       "2      232344069  0.654794  0.000000  0.302485  0.000000  0.000000  0.014492   \n",
       "3       27553029  0.988228  0.000000  0.357932  0.000000  0.000000  0.000000   \n",
       "4        3881097  1.024098  0.000000  0.000000  1.065698  0.595967  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "46559   80329479  0.296902  0.000000  0.913962  1.412212  3.264163  2.379227   \n",
       "46560  158406298  0.296902  0.000000  0.583974  0.749661  3.091289  2.040888   \n",
       "46561  158375075  0.126640  0.262672  0.581446  0.642942  3.248808  1.313747   \n",
       "46562  147478192  1.107991  0.795370  1.698550  0.523229  1.684893  1.138802   \n",
       "46563  158375402  0.126640  0.463602  0.781509  0.519113  3.366935  1.119092   \n",
       "\n",
       "           NE_6      NE_7      NE_8  ...     NE_90     NE_91     NE_92  \\\n",
       "0      0.000000  0.001206  0.109308  ...  0.232963  0.137486  0.135894   \n",
       "1      0.000000  0.132689  0.043171  ...  0.439266  0.804838  0.192160   \n",
       "2      0.098832  0.000000  0.000000  ...  0.080809  0.352127  0.000000   \n",
       "3      0.098832  0.000000  0.000000  ...  0.000000  0.352127  0.000000   \n",
       "4      0.000000  0.479150  0.000000  ...  0.000000  0.914960  0.991711   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "46559  0.816699  0.140808  0.000000  ...  0.153997  0.000000  0.605920   \n",
       "46560  0.938242  0.000000  0.000000  ...  0.226862  0.000000  0.142135   \n",
       "46561  0.346274  0.010555  0.000000  ...  0.922574  0.000000  0.003342   \n",
       "46562  0.792776  0.818324  0.538285  ...  0.108614  0.000000  1.033117   \n",
       "46563  0.481031  0.010555  0.000000  ...  0.716783  0.000000  0.003342   \n",
       "\n",
       "          NE_93     NE_94     NE_95     NE_96     NE_97     NE_98     NE_99  \n",
       "0      0.089023  0.950082  1.042759  0.150506  0.360076  0.010036  0.036215  \n",
       "1      0.942009  1.576531  0.285096  0.206587  0.126246  0.000000  0.000000  \n",
       "2      0.000000  1.048357  0.086639  0.354941  0.000000  0.256939  0.147740  \n",
       "3      0.000000  1.048357  0.086639  0.354941  0.000000  0.256939  0.237226  \n",
       "4      0.000000  2.853590  0.000000  2.902895  0.000000  0.000000  0.000000  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "46559  0.312746  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "46560  0.312746  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "46561  0.000000  0.000000  0.000000  0.000000  0.099667  0.000000  0.000000  \n",
       "46562  1.487325  0.000000  0.517031  0.000000  1.084218  0.690306  0.000000  \n",
       "46563  0.000000  0.000000  0.000000  0.000000  0.099667  0.000000  0.000000  \n",
       "\n",
       "[46564 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(node_emb_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_emb_pd.to_csv(\"elliptic_embs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(node_emb_pd[\"NE_99\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('btc-classifier': conda)",
   "language": "python",
   "name": "python37664bitbtcclassifiercondaf328939486114fc0aeb107830f867d66"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
