{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.1\n",
    "# Ethereum Illicit Accounts Dataset \n",
    "## - Boosting Algorithms vs Random Forest \n",
    "<hr>\n",
    "\n",
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### importing dependencies #############################################\n",
    "import matplotlib.pyplot as plt\n",
    "from cryptoaml.utils import read_pickle\n",
    "from IPython.core.display import display, HTML\n",
    "from cryptoaml.metrics import (\n",
    "    results_table, \n",
    "    plot_metric_dist,\n",
    "    plot_feature_imp,\n",
    "    print_model_params, \n",
    "    plot_result_matrices,\n",
    "    display_metrics_stats,\n",
    "    elliptic_time_indexed_results\n",
    ")\n",
    "\n",
    "###### constants ##########################################################\n",
    "N_features          = 15 # for feature importance N top/bottom\n",
    "EXP_RESULT_PATH     = \"persistence/experiment_1.1/results\"\n",
    "BENCHMARK_RESULTS   = \"{}/{}\".format(EXP_RESULT_PATH, \"benchmark_model_results.pkl\")\n",
    "DEFAULT_RESULTS     = \"{}/{}\".format(EXP_RESULT_PATH, \"defaults_models_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='benchmark'></a>\n",
    "## Benchmark Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - Default Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results for benchmark model\n",
    "benchmark_model = \"random_forest\"\n",
    "benchmark_results = read_pickle(BENCHMARK_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  RF Default Hyperparameters - Evaluation Metrics\n",
    "- Precision \n",
    "- Recall \n",
    "- <i>F1</i>\n",
    "- <i>F1 Micro Avg/Accuracy<i/>\n",
    "- <i>AUC</i>\n",
    "    \n",
    "<small>\n",
    "    <i>\n",
    "    Italics text refers to metrics which were used in the benchmark paper.\n",
    "    </i>\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with all the perfomance metrics\n",
    "print(\"Performance metrics for RF benchmark model with default hyperparameters\")\n",
    "benchmark_metrics_df = results_table(benchmark_results)\n",
    "display(benchmark_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF Default Hyperparameters - Evaluation Statistics\n",
    "\n",
    "<i>Note: Since random forest is non-deterministic in nature, we ran the model for 100 iterations and averaged the results. This was done to insure robustness and was also recommended in previous studies.<i/>\n",
    "    \n",
    "Below we show statistics for the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display stats for each metric for benchmark model over 100 runs\n",
    "display_metrics_stats(benchmark_results)\n",
    "\n",
    "# plot f1 score distribution for benchmark model over 100 runs\n",
    "plot_metric_dist(benchmark_results, \"f1\", figsize=(17,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  RF Default Hyperparameters - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrices for benchmark model \n",
    "plot_result_matrices(benchmark_results, figsize=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  RF Default Hyperparameters - Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top/bottom N features for benchmark model \n",
    "plot_feature_imp(benchmark_results, N_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF Default Hyperparameters - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print parameters used to train random_forest\n",
    "print_model_params(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost - Suggested Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Algorithms\n",
    "- [XGBoost: eXtreme Gradient Boosting](https://xgboost.readthedocs.io/en/latest/) \n",
    "- [LightGBM: Light Gradient Boosting Machine](https://lightgbm.readthedocs.io/en/latest/) \n",
    "- [CatBoost](https://catboost.ai/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Algorithms - Default Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results for default models\n",
    "default_results = read_pickle(DEFAULT_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Default Hyperparameters - Evaluation Metrics\n",
    "- Precision \n",
    "- Recall \n",
    "- <i>F1</i>\n",
    "- <i>F1 Micro Avg/Accuracy<i/>\n",
    "- <i>AUC</i>\n",
    "    \n",
    "<small>\n",
    "    <i>\n",
    "    Italics text refers to metrics which were used in the benchmark paper.\n",
    "    </i>\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with all the perfomance metrics\n",
    "print(\"Performance metrics for defaults boosting models\")\n",
    "defaults_metrics_df = results_table(default_results)\n",
    "display(defaults_metrics_df)\n",
    "\n",
    "# sorted by f1 score\n",
    "print(\"Performance metrics for defaults boosting models sorted by f1-score\")\n",
    "display(defaults_metrics_df.sort_values(\"f1\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Default Hyperparameters - Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot confusion matrices for boosting models with default parameters \n",
    "plot_result_matrices(default_results, figsize=(6,16), columns=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Default Hyperparameters - Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot top/bottom N features for boosting models with default parameters \n",
    "plot_feature_imp(default_results, N_features, figsize=(17,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Default Hyperparameters - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print parameters used to train boosting algorithms with default params\n",
    "print_model_params(default_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Algorithms - Tuned Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Tuned Hyperparameters - Evaluation Metrics\n",
    "- Precision \n",
    "- Recall \n",
    "- <i>F1</i>\n",
    "- <i>F1 Micro Avg/Accuracy<i/>\n",
    "- <i>AUC</i>\n",
    "    \n",
    "<small>\n",
    "    <i>\n",
    "    Italics text refers to metrics which were used in the benchmark paper.\n",
    "    </i>\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Tuned Hyperparameters - Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Tuned Hyperparameters - Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Tuned Hyperparameters - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('btc-classifier': conda)",
   "language": "python",
   "name": "python37664bitbtcclassifiercondaf328939486114fc0aeb107830f867d66"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
