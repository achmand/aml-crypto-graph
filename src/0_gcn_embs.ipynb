{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import cryptoaml.datareader as cdr\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipticdr = cdr.get_data(\"elliptic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN model https://github.com/tkipf/pygcn\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_features, \n",
    "                 out_features, \n",
    "                 bias=True):\n",
    "        \n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 nfeat, \n",
    "                 nhid, \n",
    "                 nclass, \n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        # https://github.com/tkipf/pygcn/issues/26#issuecomment-435801483\n",
    "        # \"In this case itâ€™s best to simply take the embeddings just before doing \n",
    "        # the last linear projection to the softmax logits. \n",
    "        # In other words, if the last layer is softmax(A*H*W), \n",
    "        # take either the embedding H directly or A*H.\"\n",
    "       \n",
    "        # extract node embeddings (A*H)\n",
    "        self.node_embeddings = torch.mm(adj, x)\n",
    "        \n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def load_elliptic(datareader):\n",
    "    \n",
    "    # get all labels  \n",
    "    data = datareader.dataset_.copy()\n",
    "    labelled_data = data[(data[\"class\"] != -1)]\n",
    "    \n",
    "    # get features \n",
    "    feature_cols = [\"txId\"] + datareader.feature_cols_AF_\n",
    "    labelled_features = labelled_data[feature_cols].copy()\n",
    "    labelled_features.set_index(\"txId\", inplace=True) \n",
    "    features = sp.csr_matrix(labelled_features.values, dtype=np.float32)    \n",
    "    \n",
    "    # build edges \n",
    "    tx_ids = labelled_features.index\n",
    "    idx_map = {j: i for i, j in enumerate(tx_ids)}\n",
    "    edges_unordered = datareader.edges_.copy() \n",
    "    edges_unordered = edges_unordered[(edges_unordered[\"txId2\"].isin(set(tx_ids))) & \n",
    "                               (edges_unordered[\"txId1\"].isin(set(tx_ids)))].values\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)    \n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                    shape=(labelled_features.shape[0], labelled_features.shape[0]),\n",
    "                    dtype=np.float32)\n",
    "    \n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    \n",
    "    # get idx for train and test \n",
    "    total_train = labelled_data[labelled_data[\"ts\"] <= 34].shape[0]\n",
    "    idx_train = range(total_train)\n",
    "    idx_train = torch.LongTensor(idx_train)   \n",
    "    total_test = labelled_data[labelled_data[\"ts\"] > 34].shape[0]\n",
    "    idx_test = range(total_train, total_train+total_test)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    # change data to torch tensors \n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labelled_data[\"class\"].values)\n",
    "    \n",
    "    return adj, features, labels, idx_train, idx_test\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def precision(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return precision_score(labels, preds, average=\"binary\")\n",
    "\n",
    "def recall(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return recall_score(labels, preds, average=\"binary\")\n",
    "\n",
    "def fscore_micro(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return f1_score(labels, preds, average=\"micro\")\n",
    "\n",
    "def fscore(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return f1_score(labels, preds, average=\"binary\")\n",
    "\n",
    "def confusion(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    return confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setup \n",
    "\n",
    "# build graph data \n",
    "adj, features, labels, idx_train, idx_test = load_elliptic(ellipticdr)\n",
    "\n",
    "# model \n",
    "n_classes = 2\n",
    "n_features = 166\n",
    "n_hidden = 100\n",
    "dropout = 0.5\n",
    "gcn = GCN(nfeat=n_features,\n",
    "          nhid=n_hidden,\n",
    "          nclass=n_classes,\n",
    "          dropout=dropout)\n",
    "\n",
    "# optimizer \n",
    "learning_rate = 0.001\n",
    "gcn_params = gcn.parameters()\n",
    "optimizer = optim.Adam(gcn_params,\n",
    "                       lr=learning_rate)\n",
    "weight_ratio = torch.FloatTensor([0.3, 0.7])\n",
    "loss = nn.CrossEntropyLoss(weight=weight_ratio)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     gcn.cuda()\n",
    "#     features = features.cuda()\n",
    "#     adj = adj.cuda()\n",
    "#     labels = labels.cuda()\n",
    "#     idx_train = idx_train.cuda()\n",
    "#     idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 4.3496 f1_train: 0.1904 time: 0.2460s\n",
      "Epoch: 0002 loss_train: 3.5720 f1_train: 0.1961 time: 0.1948s\n",
      "Epoch: 0003 loss_train: 2.7399 f1_train: 0.2141 time: 0.1935s\n",
      "Epoch: 0004 loss_train: 2.1970 f1_train: 0.2398 time: 0.1759s\n",
      "Epoch: 0005 loss_train: 1.8622 f1_train: 0.2586 time: 0.1879s\n",
      "Epoch: 0006 loss_train: 1.5127 f1_train: 0.2776 time: 0.1792s\n",
      "Epoch: 0007 loss_train: 1.3232 f1_train: 0.3008 time: 0.1986s\n",
      "Epoch: 0008 loss_train: 1.1446 f1_train: 0.3217 time: 0.2272s\n",
      "Epoch: 0009 loss_train: 1.0620 f1_train: 0.3362 time: 0.2237s\n",
      "Epoch: 0010 loss_train: 0.9787 f1_train: 0.3488 time: 0.2229s\n",
      "Epoch: 0011 loss_train: 0.9536 f1_train: 0.3585 time: 0.2264s\n",
      "Epoch: 0012 loss_train: 0.9455 f1_train: 0.3647 time: 0.2168s\n",
      "Epoch: 0013 loss_train: 0.8918 f1_train: 0.3716 time: 0.2226s\n",
      "Epoch: 0014 loss_train: 0.8890 f1_train: 0.3769 time: 0.2118s\n",
      "Epoch: 0015 loss_train: 0.8540 f1_train: 0.3939 time: 0.2122s\n",
      "Epoch: 0016 loss_train: 0.8449 f1_train: 0.3940 time: 0.2110s\n",
      "Epoch: 0017 loss_train: 0.8605 f1_train: 0.3987 time: 0.2360s\n",
      "Epoch: 0018 loss_train: 0.8376 f1_train: 0.4110 time: 0.2135s\n",
      "Epoch: 0019 loss_train: 0.8528 f1_train: 0.4065 time: 0.2351s\n",
      "Epoch: 0020 loss_train: 0.7929 f1_train: 0.4325 time: 0.1760s\n",
      "Epoch: 0021 loss_train: 0.7666 f1_train: 0.4567 time: 0.1837s\n",
      "Epoch: 0022 loss_train: 0.7716 f1_train: 0.4536 time: 0.1759s\n",
      "Epoch: 0023 loss_train: 0.7546 f1_train: 0.4740 time: 0.1929s\n",
      "Epoch: 0024 loss_train: 0.7428 f1_train: 0.4612 time: 0.1812s\n",
      "Epoch: 0025 loss_train: 0.7278 f1_train: 0.4732 time: 0.1899s\n",
      "Epoch: 0026 loss_train: 0.6996 f1_train: 0.4908 time: 0.1811s\n",
      "Epoch: 0027 loss_train: 0.6973 f1_train: 0.4933 time: 0.2121s\n",
      "Epoch: 0028 loss_train: 0.7029 f1_train: 0.5015 time: 0.2292s\n",
      "Epoch: 0029 loss_train: 0.6702 f1_train: 0.5204 time: 0.2108s\n",
      "Epoch: 0030 loss_train: 0.6629 f1_train: 0.5213 time: 0.2285s\n",
      "Epoch: 0031 loss_train: 0.6678 f1_train: 0.5180 time: 0.1984s\n",
      "Epoch: 0032 loss_train: 0.6361 f1_train: 0.5349 time: 0.2292s\n",
      "Epoch: 0033 loss_train: 0.6299 f1_train: 0.5291 time: 0.2260s\n",
      "Epoch: 0034 loss_train: 0.6464 f1_train: 0.5398 time: 0.2126s\n",
      "Epoch: 0035 loss_train: 0.6196 f1_train: 0.5309 time: 0.2298s\n",
      "Epoch: 0036 loss_train: 0.6323 f1_train: 0.5357 time: 0.2689s\n",
      "Epoch: 0037 loss_train: 0.6180 f1_train: 0.5419 time: 0.3389s\n",
      "Epoch: 0038 loss_train: 0.6128 f1_train: 0.5525 time: 0.2393s\n",
      "Epoch: 0039 loss_train: 0.6080 f1_train: 0.5477 time: 0.2341s\n",
      "Epoch: 0040 loss_train: 0.6146 f1_train: 0.5513 time: 0.2225s\n",
      "Epoch: 0041 loss_train: 0.5805 f1_train: 0.5585 time: 0.2684s\n",
      "Epoch: 0042 loss_train: 0.5815 f1_train: 0.5647 time: 0.2223s\n",
      "Epoch: 0043 loss_train: 0.5674 f1_train: 0.5680 time: 0.3143s\n",
      "Epoch: 0044 loss_train: 0.5813 f1_train: 0.5555 time: 0.2799s\n",
      "Epoch: 0045 loss_train: 0.5569 f1_train: 0.5686 time: 0.2655s\n",
      "Epoch: 0046 loss_train: 0.5746 f1_train: 0.5629 time: 0.2624s\n",
      "Epoch: 0047 loss_train: 0.5525 f1_train: 0.5690 time: 0.1894s\n",
      "Epoch: 0048 loss_train: 0.5459 f1_train: 0.5788 time: 0.2077s\n",
      "Epoch: 0049 loss_train: 0.5269 f1_train: 0.5795 time: 0.1824s\n",
      "Epoch: 0050 loss_train: 0.5436 f1_train: 0.5679 time: 0.1962s\n",
      "Epoch: 0051 loss_train: 0.5199 f1_train: 0.5775 time: 0.1980s\n",
      "Epoch: 0052 loss_train: 0.5142 f1_train: 0.5801 time: 0.1927s\n",
      "Epoch: 0053 loss_train: 0.5126 f1_train: 0.5844 time: 0.2164s\n",
      "Epoch: 0054 loss_train: 0.5137 f1_train: 0.5779 time: 0.2200s\n",
      "Epoch: 0055 loss_train: 0.5194 f1_train: 0.5818 time: 0.2246s\n",
      "Epoch: 0056 loss_train: 0.5212 f1_train: 0.5769 time: 0.2289s\n",
      "Epoch: 0057 loss_train: 0.4800 f1_train: 0.5924 time: 0.2392s\n",
      "Epoch: 0058 loss_train: 0.4976 f1_train: 0.5882 time: 0.2197s\n",
      "Epoch: 0059 loss_train: 0.4984 f1_train: 0.5831 time: 0.2169s\n",
      "Epoch: 0060 loss_train: 0.4927 f1_train: 0.5872 time: 0.2245s\n",
      "Epoch: 0061 loss_train: 0.4722 f1_train: 0.5982 time: 0.2344s\n",
      "Epoch: 0062 loss_train: 0.4810 f1_train: 0.5913 time: 0.2414s\n",
      "Epoch: 0063 loss_train: 0.4558 f1_train: 0.6060 time: 0.1828s\n",
      "Epoch: 0064 loss_train: 0.4676 f1_train: 0.6033 time: 0.2397s\n",
      "Epoch: 0065 loss_train: 0.4805 f1_train: 0.5932 time: 0.2243s\n",
      "Epoch: 0066 loss_train: 0.4532 f1_train: 0.6013 time: 0.2173s\n",
      "Epoch: 0067 loss_train: 0.4679 f1_train: 0.5975 time: 0.2197s\n",
      "Epoch: 0068 loss_train: 0.4480 f1_train: 0.6060 time: 0.2415s\n",
      "Epoch: 0069 loss_train: 0.4352 f1_train: 0.6108 time: 0.2202s\n",
      "Epoch: 0070 loss_train: 0.4448 f1_train: 0.6059 time: 0.2389s\n",
      "Epoch: 0071 loss_train: 0.4362 f1_train: 0.6110 time: 0.2134s\n",
      "Epoch: 0072 loss_train: 0.4330 f1_train: 0.6102 time: 0.2161s\n",
      "Epoch: 0073 loss_train: 0.4146 f1_train: 0.6184 time: 0.2295s\n",
      "Epoch: 0074 loss_train: 0.4294 f1_train: 0.6126 time: 0.1953s\n",
      "Epoch: 0075 loss_train: 0.4201 f1_train: 0.6178 time: 0.1858s\n",
      "Epoch: 0076 loss_train: 0.4377 f1_train: 0.6028 time: 0.1840s\n",
      "Epoch: 0077 loss_train: 0.4086 f1_train: 0.6194 time: 0.1898s\n",
      "Epoch: 0078 loss_train: 0.4113 f1_train: 0.6184 time: 0.1775s\n",
      "Epoch: 0079 loss_train: 0.3987 f1_train: 0.6232 time: 0.1937s\n",
      "Epoch: 0080 loss_train: 0.3927 f1_train: 0.6317 time: 0.1909s\n",
      "Epoch: 0081 loss_train: 0.4155 f1_train: 0.6170 time: 0.1905s\n",
      "Epoch: 0082 loss_train: 0.4080 f1_train: 0.6181 time: 0.1750s\n",
      "Epoch: 0083 loss_train: 0.4007 f1_train: 0.6327 time: 0.1906s\n",
      "Epoch: 0084 loss_train: 0.4030 f1_train: 0.6301 time: 0.1795s\n",
      "Epoch: 0085 loss_train: 0.3903 f1_train: 0.6375 time: 0.1895s\n",
      "Epoch: 0086 loss_train: 0.3829 f1_train: 0.6355 time: 0.1743s\n",
      "Epoch: 0087 loss_train: 0.3833 f1_train: 0.6311 time: 0.1869s\n",
      "Epoch: 0088 loss_train: 0.3791 f1_train: 0.6380 time: 0.1847s\n",
      "Epoch: 0089 loss_train: 0.3796 f1_train: 0.6377 time: 0.1850s\n",
      "Epoch: 0090 loss_train: 0.3635 f1_train: 0.6445 time: 0.1751s\n",
      "Epoch: 0091 loss_train: 0.3624 f1_train: 0.6404 time: 0.1979s\n",
      "Epoch: 0092 loss_train: 0.3687 f1_train: 0.6423 time: 0.1743s\n",
      "Epoch: 0093 loss_train: 0.3616 f1_train: 0.6509 time: 0.2060s\n",
      "Epoch: 0094 loss_train: 0.3627 f1_train: 0.6413 time: 0.1763s\n",
      "Epoch: 0095 loss_train: 0.3572 f1_train: 0.6460 time: 0.2155s\n",
      "Epoch: 0096 loss_train: 0.3544 f1_train: 0.6481 time: 0.1958s\n",
      "Epoch: 0097 loss_train: 0.3487 f1_train: 0.6478 time: 0.1814s\n",
      "Epoch: 0098 loss_train: 0.3466 f1_train: 0.6526 time: 0.2260s\n",
      "Epoch: 0099 loss_train: 0.3528 f1_train: 0.6448 time: 0.2749s\n",
      "Epoch: 0100 loss_train: 0.3480 f1_train: 0.6504 time: 0.2427s\n",
      "Epoch: 0101 loss_train: 0.3480 f1_train: 0.6526 time: 0.2164s\n",
      "Epoch: 0102 loss_train: 0.3326 f1_train: 0.6556 time: 0.2164s\n",
      "Epoch: 0103 loss_train: 0.3316 f1_train: 0.6658 time: 0.2269s\n",
      "Epoch: 0104 loss_train: 0.3312 f1_train: 0.6626 time: 0.2551s\n",
      "Epoch: 0105 loss_train: 0.3293 f1_train: 0.6639 time: 0.2267s\n",
      "Epoch: 0106 loss_train: 0.3315 f1_train: 0.6608 time: 0.2188s\n",
      "Epoch: 0107 loss_train: 0.3298 f1_train: 0.6596 time: 0.2216s\n",
      "Epoch: 0108 loss_train: 0.3333 f1_train: 0.6695 time: 0.2089s\n",
      "Epoch: 0109 loss_train: 0.3278 f1_train: 0.6682 time: 0.2616s\n",
      "Epoch: 0110 loss_train: 0.3198 f1_train: 0.6735 time: 0.2181s\n",
      "Epoch: 0111 loss_train: 0.3143 f1_train: 0.6735 time: 0.2412s\n",
      "Epoch: 0112 loss_train: 0.3030 f1_train: 0.6789 time: 0.2195s\n",
      "Epoch: 0113 loss_train: 0.3105 f1_train: 0.6760 time: 0.2284s\n",
      "Epoch: 0114 loss_train: 0.3164 f1_train: 0.6706 time: 0.2196s\n",
      "Epoch: 0115 loss_train: 0.3027 f1_train: 0.6793 time: 0.2184s\n",
      "Epoch: 0116 loss_train: 0.2997 f1_train: 0.6779 time: 0.1778s\n",
      "Epoch: 0117 loss_train: 0.3012 f1_train: 0.6804 time: 0.1944s\n",
      "Epoch: 0118 loss_train: 0.3019 f1_train: 0.6793 time: 0.1838s\n",
      "Epoch: 0119 loss_train: 0.2930 f1_train: 0.6857 time: 0.1870s\n",
      "Epoch: 0120 loss_train: 0.2969 f1_train: 0.6818 time: 0.1804s\n",
      "Epoch: 0121 loss_train: 0.2917 f1_train: 0.6858 time: 0.1869s\n",
      "Epoch: 0122 loss_train: 0.2859 f1_train: 0.6808 time: 0.2186s\n",
      "Epoch: 0123 loss_train: 0.2918 f1_train: 0.6841 time: 0.2216s\n",
      "Epoch: 0124 loss_train: 0.2891 f1_train: 0.6896 time: 0.1924s\n",
      "Epoch: 0125 loss_train: 0.2820 f1_train: 0.6935 time: 0.1926s\n",
      "Epoch: 0126 loss_train: 0.2834 f1_train: 0.6882 time: 0.1775s\n",
      "Epoch: 0127 loss_train: 0.2803 f1_train: 0.6940 time: 0.1878s\n",
      "Epoch: 0128 loss_train: 0.2834 f1_train: 0.6895 time: 0.1883s\n",
      "Epoch: 0129 loss_train: 0.2746 f1_train: 0.6916 time: 0.2026s\n",
      "Epoch: 0130 loss_train: 0.2842 f1_train: 0.6980 time: 0.1834s\n",
      "Epoch: 0131 loss_train: 0.2787 f1_train: 0.6940 time: 0.1998s\n",
      "Epoch: 0132 loss_train: 0.2764 f1_train: 0.6955 time: 0.2040s\n",
      "Epoch: 0133 loss_train: 0.2696 f1_train: 0.7027 time: 0.1804s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0134 loss_train: 0.2632 f1_train: 0.7059 time: 0.2405s\n",
      "Epoch: 0135 loss_train: 0.2710 f1_train: 0.7078 time: 0.2647s\n",
      "Epoch: 0136 loss_train: 0.2702 f1_train: 0.7041 time: 0.2249s\n",
      "Epoch: 0137 loss_train: 0.2541 f1_train: 0.7090 time: 0.2323s\n",
      "Epoch: 0138 loss_train: 0.2672 f1_train: 0.7016 time: 0.2211s\n",
      "Epoch: 0139 loss_train: 0.2564 f1_train: 0.7109 time: 0.2189s\n",
      "Epoch: 0140 loss_train: 0.2587 f1_train: 0.7048 time: 0.2220s\n",
      "Epoch: 0141 loss_train: 0.2577 f1_train: 0.7051 time: 0.2381s\n",
      "Epoch: 0142 loss_train: 0.2552 f1_train: 0.7066 time: 0.2378s\n",
      "Epoch: 0143 loss_train: 0.2555 f1_train: 0.7131 time: 0.2205s\n",
      "Epoch: 0144 loss_train: 0.2566 f1_train: 0.7056 time: 0.2546s\n",
      "Epoch: 0145 loss_train: 0.2558 f1_train: 0.7101 time: 0.2157s\n",
      "Epoch: 0146 loss_train: 0.2523 f1_train: 0.7117 time: 0.2186s\n",
      "Epoch: 0147 loss_train: 0.2489 f1_train: 0.7186 time: 0.2228s\n",
      "Epoch: 0148 loss_train: 0.2396 f1_train: 0.7242 time: 0.2203s\n",
      "Epoch: 0149 loss_train: 0.2517 f1_train: 0.7151 time: 0.2208s\n",
      "Epoch: 0150 loss_train: 0.2479 f1_train: 0.7222 time: 0.2141s\n",
      "Epoch: 0151 loss_train: 0.2548 f1_train: 0.7134 time: 0.2183s\n",
      "Epoch: 0152 loss_train: 0.2493 f1_train: 0.7199 time: 0.2164s\n",
      "Epoch: 0153 loss_train: 0.2454 f1_train: 0.7163 time: 0.2241s\n",
      "Epoch: 0154 loss_train: 0.2422 f1_train: 0.7248 time: 0.2125s\n",
      "Epoch: 0155 loss_train: 0.2427 f1_train: 0.7181 time: 0.2364s\n",
      "Epoch: 0156 loss_train: 0.2334 f1_train: 0.7298 time: 0.2178s\n",
      "Epoch: 0157 loss_train: 0.2384 f1_train: 0.7216 time: 0.2307s\n",
      "Epoch: 0158 loss_train: 0.2293 f1_train: 0.7361 time: 0.2153s\n",
      "Epoch: 0159 loss_train: 0.2376 f1_train: 0.7257 time: 0.2297s\n",
      "Epoch: 0160 loss_train: 0.2291 f1_train: 0.7327 time: 0.2161s\n",
      "Epoch: 0161 loss_train: 0.2404 f1_train: 0.7268 time: 0.2314s\n",
      "Epoch: 0162 loss_train: 0.2327 f1_train: 0.7323 time: 0.2238s\n",
      "Epoch: 0163 loss_train: 0.2297 f1_train: 0.7367 time: 0.2348s\n",
      "Epoch: 0164 loss_train: 0.2288 f1_train: 0.7325 time: 0.1753s\n",
      "Epoch: 0165 loss_train: 0.2272 f1_train: 0.7327 time: 0.2022s\n",
      "Epoch: 0166 loss_train: 0.2337 f1_train: 0.7333 time: 0.1782s\n",
      "Epoch: 0167 loss_train: 0.2263 f1_train: 0.7324 time: 0.2008s\n",
      "Epoch: 0168 loss_train: 0.2244 f1_train: 0.7405 time: 0.1807s\n",
      "Epoch: 0169 loss_train: 0.2256 f1_train: 0.7364 time: 0.2026s\n",
      "Epoch: 0170 loss_train: 0.2254 f1_train: 0.7351 time: 0.1757s\n",
      "Epoch: 0171 loss_train: 0.2166 f1_train: 0.7454 time: 0.2018s\n",
      "Epoch: 0172 loss_train: 0.2194 f1_train: 0.7409 time: 0.1762s\n",
      "Epoch: 0173 loss_train: 0.2161 f1_train: 0.7387 time: 0.2035s\n",
      "Epoch: 0174 loss_train: 0.2140 f1_train: 0.7502 time: 0.1740s\n",
      "Epoch: 0175 loss_train: 0.2200 f1_train: 0.7449 time: 0.1993s\n",
      "Epoch: 0176 loss_train: 0.2107 f1_train: 0.7489 time: 0.1870s\n",
      "Epoch: 0177 loss_train: 0.2093 f1_train: 0.7480 time: 0.1802s\n",
      "Epoch: 0178 loss_train: 0.2092 f1_train: 0.7566 time: 0.1772s\n",
      "Epoch: 0179 loss_train: 0.2160 f1_train: 0.7512 time: 0.1869s\n",
      "Epoch: 0180 loss_train: 0.2091 f1_train: 0.7563 time: 0.1932s\n",
      "Epoch: 0181 loss_train: 0.2137 f1_train: 0.7449 time: 0.2016s\n",
      "Epoch: 0182 loss_train: 0.2153 f1_train: 0.7427 time: 0.1818s\n",
      "Epoch: 0183 loss_train: 0.2127 f1_train: 0.7430 time: 0.1887s\n",
      "Epoch: 0184 loss_train: 0.2049 f1_train: 0.7550 time: 0.1821s\n",
      "Epoch: 0185 loss_train: 0.2091 f1_train: 0.7508 time: 0.1956s\n",
      "Epoch: 0186 loss_train: 0.2129 f1_train: 0.7553 time: 0.1774s\n",
      "Epoch: 0187 loss_train: 0.2092 f1_train: 0.7574 time: 0.1840s\n",
      "Epoch: 0188 loss_train: 0.2097 f1_train: 0.7553 time: 0.1756s\n",
      "Epoch: 0189 loss_train: 0.2054 f1_train: 0.7597 time: 0.1911s\n",
      "Epoch: 0190 loss_train: 0.2064 f1_train: 0.7596 time: 0.2207s\n",
      "Epoch: 0191 loss_train: 0.2064 f1_train: 0.7516 time: 0.1743s\n",
      "Epoch: 0192 loss_train: 0.2041 f1_train: 0.7567 time: 0.1942s\n",
      "Epoch: 0193 loss_train: 0.2031 f1_train: 0.7575 time: 0.2209s\n",
      "Epoch: 0194 loss_train: 0.2041 f1_train: 0.7555 time: 0.1783s\n",
      "Epoch: 0195 loss_train: 0.1993 f1_train: 0.7586 time: 0.1881s\n",
      "Epoch: 0196 loss_train: 0.1990 f1_train: 0.7588 time: 0.1809s\n",
      "Epoch: 0197 loss_train: 0.2023 f1_train: 0.7586 time: 0.2240s\n",
      "Epoch: 0198 loss_train: 0.1965 f1_train: 0.7654 time: 0.2166s\n",
      "Epoch: 0199 loss_train: 0.1984 f1_train: 0.7609 time: 0.1828s\n",
      "Epoch: 0200 loss_train: 0.1931 f1_train: 0.7700 time: 0.1828s\n",
      "Epoch: 0201 loss_train: 0.1921 f1_train: 0.7666 time: 0.1743s\n",
      "Epoch: 0202 loss_train: 0.1915 f1_train: 0.7723 time: 0.2119s\n",
      "Epoch: 0203 loss_train: 0.1960 f1_train: 0.7683 time: 0.1750s\n",
      "Epoch: 0204 loss_train: 0.1994 f1_train: 0.7624 time: 0.1851s\n",
      "Epoch: 0205 loss_train: 0.1994 f1_train: 0.7650 time: 0.1749s\n",
      "Epoch: 0206 loss_train: 0.1892 f1_train: 0.7625 time: 0.1837s\n",
      "Epoch: 0207 loss_train: 0.1936 f1_train: 0.7662 time: 0.1902s\n",
      "Epoch: 0208 loss_train: 0.1930 f1_train: 0.7685 time: 0.1882s\n",
      "Epoch: 0209 loss_train: 0.1908 f1_train: 0.7687 time: 0.3134s\n",
      "Epoch: 0210 loss_train: 0.1897 f1_train: 0.7737 time: 0.2799s\n",
      "Epoch: 0211 loss_train: 0.1876 f1_train: 0.7686 time: 0.2500s\n",
      "Epoch: 0212 loss_train: 0.1915 f1_train: 0.7704 time: 0.1772s\n",
      "Epoch: 0213 loss_train: 0.1869 f1_train: 0.7751 time: 0.1908s\n",
      "Epoch: 0214 loss_train: 0.1869 f1_train: 0.7768 time: 0.2827s\n",
      "Epoch: 0215 loss_train: 0.1938 f1_train: 0.7693 time: 0.2215s\n",
      "Epoch: 0216 loss_train: 0.1898 f1_train: 0.7736 time: 0.2424s\n",
      "Epoch: 0217 loss_train: 0.1849 f1_train: 0.7718 time: 0.2320s\n",
      "Epoch: 0218 loss_train: 0.1906 f1_train: 0.7685 time: 0.2190s\n",
      "Epoch: 0219 loss_train: 0.1842 f1_train: 0.7745 time: 0.2270s\n",
      "Epoch: 0220 loss_train: 0.1833 f1_train: 0.7800 time: 0.2312s\n",
      "Epoch: 0221 loss_train: 0.1846 f1_train: 0.7773 time: 0.2257s\n",
      "Epoch: 0222 loss_train: 0.1927 f1_train: 0.7698 time: 0.2121s\n",
      "Epoch: 0223 loss_train: 0.1795 f1_train: 0.7792 time: 0.2133s\n",
      "Epoch: 0224 loss_train: 0.1818 f1_train: 0.7782 time: 0.2116s\n",
      "Epoch: 0225 loss_train: 0.1805 f1_train: 0.7857 time: 0.2034s\n",
      "Epoch: 0226 loss_train: 0.1828 f1_train: 0.7860 time: 0.2262s\n",
      "Epoch: 0227 loss_train: 0.1861 f1_train: 0.7768 time: 0.2205s\n",
      "Epoch: 0228 loss_train: 0.1869 f1_train: 0.7765 time: 0.2214s\n",
      "Epoch: 0229 loss_train: 0.1801 f1_train: 0.7831 time: 0.2483s\n",
      "Epoch: 0230 loss_train: 0.1800 f1_train: 0.7829 time: 0.2170s\n",
      "Epoch: 0231 loss_train: 0.1809 f1_train: 0.7821 time: 0.2188s\n",
      "Epoch: 0232 loss_train: 0.1797 f1_train: 0.7870 time: 0.2284s\n",
      "Epoch: 0233 loss_train: 0.1785 f1_train: 0.7875 time: 0.2208s\n",
      "Epoch: 0234 loss_train: 0.1777 f1_train: 0.7853 time: 0.2193s\n",
      "Epoch: 0235 loss_train: 0.1777 f1_train: 0.7841 time: 0.2286s\n",
      "Epoch: 0236 loss_train: 0.1756 f1_train: 0.7889 time: 0.2474s\n",
      "Epoch: 0237 loss_train: 0.1757 f1_train: 0.7858 time: 0.2194s\n",
      "Epoch: 0238 loss_train: 0.1802 f1_train: 0.7855 time: 0.2309s\n",
      "Epoch: 0239 loss_train: 0.1740 f1_train: 0.7852 time: 0.2321s\n",
      "Epoch: 0240 loss_train: 0.1754 f1_train: 0.7913 time: 0.2200s\n",
      "Epoch: 0241 loss_train: 0.1743 f1_train: 0.7943 time: 0.2213s\n",
      "Epoch: 0242 loss_train: 0.1770 f1_train: 0.7843 time: 0.1782s\n",
      "Epoch: 0243 loss_train: 0.1761 f1_train: 0.7908 time: 0.2204s\n",
      "Epoch: 0244 loss_train: 0.1750 f1_train: 0.7898 time: 0.2263s\n",
      "Epoch: 0245 loss_train: 0.1758 f1_train: 0.7843 time: 0.2303s\n",
      "Epoch: 0246 loss_train: 0.1755 f1_train: 0.7820 time: 0.2203s\n",
      "Epoch: 0247 loss_train: 0.1704 f1_train: 0.7870 time: 0.2147s\n",
      "Epoch: 0248 loss_train: 0.1756 f1_train: 0.7862 time: 0.2415s\n",
      "Epoch: 0249 loss_train: 0.1725 f1_train: 0.7856 time: 0.2134s\n",
      "Epoch: 0250 loss_train: 0.1748 f1_train: 0.7868 time: 0.2260s\n",
      "Epoch: 0251 loss_train: 0.1717 f1_train: 0.7888 time: 0.2224s\n",
      "Epoch: 0252 loss_train: 0.1669 f1_train: 0.7989 time: 0.2154s\n",
      "Epoch: 0253 loss_train: 0.1685 f1_train: 0.7940 time: 0.2297s\n",
      "Epoch: 0254 loss_train: 0.1769 f1_train: 0.7872 time: 0.2192s\n",
      "Epoch: 0255 loss_train: 0.1676 f1_train: 0.8010 time: 0.2288s\n",
      "Epoch: 0256 loss_train: 0.1665 f1_train: 0.7962 time: 0.2141s\n",
      "Epoch: 0257 loss_train: 0.1685 f1_train: 0.7934 time: 0.2263s\n",
      "Epoch: 0258 loss_train: 0.1663 f1_train: 0.7992 time: 0.2151s\n",
      "Epoch: 0259 loss_train: 0.1709 f1_train: 0.7964 time: 0.2174s\n",
      "Epoch: 0260 loss_train: 0.1678 f1_train: 0.7955 time: 0.2287s\n",
      "Epoch: 0261 loss_train: 0.1701 f1_train: 0.7964 time: 0.2166s\n",
      "Epoch: 0262 loss_train: 0.1664 f1_train: 0.7929 time: 0.2346s\n",
      "Epoch: 0263 loss_train: 0.1681 f1_train: 0.7921 time: 0.2718s\n",
      "Epoch: 0264 loss_train: 0.1650 f1_train: 0.8025 time: 0.2231s\n",
      "Epoch: 0265 loss_train: 0.1691 f1_train: 0.8053 time: 0.2711s\n",
      "Epoch: 0266 loss_train: 0.1672 f1_train: 0.7916 time: 0.2152s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0267 loss_train: 0.1648 f1_train: 0.8024 time: 0.2187s\n",
      "Epoch: 0268 loss_train: 0.1669 f1_train: 0.7980 time: 0.2318s\n",
      "Epoch: 0269 loss_train: 0.1646 f1_train: 0.8037 time: 0.2861s\n",
      "Epoch: 0270 loss_train: 0.1660 f1_train: 0.8011 time: 0.2207s\n",
      "Epoch: 0271 loss_train: 0.1694 f1_train: 0.7963 time: 0.2243s\n",
      "Epoch: 0272 loss_train: 0.1634 f1_train: 0.7962 time: 0.2213s\n",
      "Epoch: 0273 loss_train: 0.1655 f1_train: 0.7981 time: 0.2473s\n",
      "Epoch: 0274 loss_train: 0.1666 f1_train: 0.7962 time: 0.2246s\n",
      "Epoch: 0275 loss_train: 0.1636 f1_train: 0.8046 time: 0.2240s\n",
      "Epoch: 0276 loss_train: 0.1614 f1_train: 0.8035 time: 0.2260s\n",
      "Epoch: 0277 loss_train: 0.1640 f1_train: 0.8025 time: 0.2119s\n",
      "Epoch: 0278 loss_train: 0.1633 f1_train: 0.7995 time: 0.2815s\n",
      "Epoch: 0279 loss_train: 0.1623 f1_train: 0.7996 time: 0.1742s\n",
      "Epoch: 0280 loss_train: 0.1650 f1_train: 0.8100 time: 0.1838s\n",
      "Epoch: 0281 loss_train: 0.1649 f1_train: 0.8036 time: 0.2627s\n",
      "Epoch: 0282 loss_train: 0.1632 f1_train: 0.8085 time: 0.1761s\n",
      "Epoch: 0283 loss_train: 0.1586 f1_train: 0.8086 time: 0.2019s\n",
      "Epoch: 0284 loss_train: 0.1613 f1_train: 0.8035 time: 0.1787s\n",
      "Epoch: 0285 loss_train: 0.1602 f1_train: 0.8058 time: 0.1809s\n",
      "Epoch: 0286 loss_train: 0.1585 f1_train: 0.8030 time: 0.1814s\n",
      "Epoch: 0287 loss_train: 0.1579 f1_train: 0.8031 time: 0.2798s\n",
      "Epoch: 0288 loss_train: 0.1583 f1_train: 0.8064 time: 0.1779s\n",
      "Epoch: 0289 loss_train: 0.1594 f1_train: 0.8105 time: 0.1844s\n",
      "Epoch: 0290 loss_train: 0.1619 f1_train: 0.8034 time: 0.1808s\n",
      "Epoch: 0291 loss_train: 0.1576 f1_train: 0.8052 time: 0.2264s\n",
      "Epoch: 0292 loss_train: 0.1606 f1_train: 0.8011 time: 0.2303s\n",
      "Epoch: 0293 loss_train: 0.1554 f1_train: 0.8119 time: 0.2209s\n",
      "Epoch: 0294 loss_train: 0.1613 f1_train: 0.8050 time: 0.2815s\n",
      "Epoch: 0295 loss_train: 0.1545 f1_train: 0.8106 time: 0.2139s\n",
      "Epoch: 0296 loss_train: 0.1578 f1_train: 0.8045 time: 0.2784s\n",
      "Epoch: 0297 loss_train: 0.1583 f1_train: 0.8107 time: 0.1816s\n",
      "Epoch: 0298 loss_train: 0.1591 f1_train: 0.8065 time: 0.1845s\n",
      "Epoch: 0299 loss_train: 0.1581 f1_train: 0.8137 time: 0.1807s\n",
      "Epoch: 0300 loss_train: 0.1585 f1_train: 0.8096 time: 0.1890s\n",
      "Epoch: 0301 loss_train: 0.1563 f1_train: 0.8078 time: 0.1784s\n",
      "Epoch: 0302 loss_train: 0.1549 f1_train: 0.8098 time: 0.1873s\n",
      "Epoch: 0303 loss_train: 0.1584 f1_train: 0.8108 time: 0.1762s\n",
      "Epoch: 0304 loss_train: 0.1561 f1_train: 0.8096 time: 0.1892s\n",
      "Epoch: 0305 loss_train: 0.1581 f1_train: 0.8046 time: 0.1806s\n",
      "Epoch: 0306 loss_train: 0.1552 f1_train: 0.8164 time: 0.2279s\n",
      "Epoch: 0307 loss_train: 0.1552 f1_train: 0.8109 time: 0.2312s\n",
      "Epoch: 0308 loss_train: 0.1538 f1_train: 0.8169 time: 0.1759s\n",
      "Epoch: 0309 loss_train: 0.1518 f1_train: 0.8182 time: 0.1854s\n",
      "Epoch: 0310 loss_train: 0.1532 f1_train: 0.8176 time: 0.1784s\n",
      "Epoch: 0311 loss_train: 0.1559 f1_train: 0.8082 time: 0.1868s\n",
      "Epoch: 0312 loss_train: 0.1510 f1_train: 0.8111 time: 0.1960s\n",
      "Epoch: 0313 loss_train: 0.1530 f1_train: 0.8185 time: 0.1917s\n",
      "Epoch: 0314 loss_train: 0.1552 f1_train: 0.8109 time: 0.2217s\n",
      "Epoch: 0315 loss_train: 0.1489 f1_train: 0.8180 time: 0.1898s\n",
      "Epoch: 0316 loss_train: 0.1533 f1_train: 0.8116 time: 0.1834s\n",
      "Epoch: 0317 loss_train: 0.1563 f1_train: 0.8105 time: 0.1889s\n",
      "Epoch: 0318 loss_train: 0.1502 f1_train: 0.8142 time: 0.1789s\n",
      "Epoch: 0319 loss_train: 0.1535 f1_train: 0.8150 time: 0.1745s\n",
      "Epoch: 0320 loss_train: 0.1525 f1_train: 0.8123 time: 0.1868s\n",
      "Epoch: 0321 loss_train: 0.1511 f1_train: 0.8174 time: 0.1746s\n",
      "Epoch: 0322 loss_train: 0.1519 f1_train: 0.8152 time: 0.1830s\n",
      "Epoch: 0323 loss_train: 0.1493 f1_train: 0.8178 time: 0.1847s\n",
      "Epoch: 0324 loss_train: 0.1553 f1_train: 0.8156 time: 0.1853s\n",
      "Epoch: 0325 loss_train: 0.1491 f1_train: 0.8193 time: 0.1834s\n",
      "Epoch: 0326 loss_train: 0.1550 f1_train: 0.8175 time: 0.1812s\n",
      "Epoch: 0327 loss_train: 0.1517 f1_train: 0.8126 time: 0.1754s\n",
      "Epoch: 0328 loss_train: 0.1488 f1_train: 0.8187 time: 0.1927s\n",
      "Epoch: 0329 loss_train: 0.1492 f1_train: 0.8195 time: 0.1754s\n",
      "Epoch: 0330 loss_train: 0.1495 f1_train: 0.8183 time: 0.1834s\n",
      "Epoch: 0331 loss_train: 0.1502 f1_train: 0.8171 time: 0.1818s\n",
      "Epoch: 0332 loss_train: 0.1532 f1_train: 0.8151 time: 0.2178s\n",
      "Epoch: 0333 loss_train: 0.1545 f1_train: 0.8086 time: 0.2264s\n",
      "Epoch: 0334 loss_train: 0.1482 f1_train: 0.8166 time: 0.2159s\n",
      "Epoch: 0335 loss_train: 0.1501 f1_train: 0.8118 time: 0.2287s\n",
      "Epoch: 0336 loss_train: 0.1493 f1_train: 0.8215 time: 0.2132s\n",
      "Epoch: 0337 loss_train: 0.1484 f1_train: 0.8183 time: 0.2210s\n",
      "Epoch: 0338 loss_train: 0.1463 f1_train: 0.8252 time: 0.2407s\n",
      "Epoch: 0339 loss_train: 0.1486 f1_train: 0.8204 time: 0.2175s\n",
      "Epoch: 0340 loss_train: 0.1502 f1_train: 0.8197 time: 0.2347s\n",
      "Epoch: 0341 loss_train: 0.1486 f1_train: 0.8241 time: 0.2353s\n",
      "Epoch: 0342 loss_train: 0.1450 f1_train: 0.8284 time: 0.2371s\n",
      "Epoch: 0343 loss_train: 0.1481 f1_train: 0.8216 time: 0.2152s\n",
      "Epoch: 0344 loss_train: 0.1478 f1_train: 0.8230 time: 0.2238s\n",
      "Epoch: 0345 loss_train: 0.1478 f1_train: 0.8209 time: 0.2353s\n",
      "Epoch: 0346 loss_train: 0.1467 f1_train: 0.8208 time: 0.2315s\n",
      "Epoch: 0347 loss_train: 0.1479 f1_train: 0.8180 time: 0.2272s\n",
      "Epoch: 0348 loss_train: 0.1470 f1_train: 0.8179 time: 0.2303s\n",
      "Epoch: 0349 loss_train: 0.1456 f1_train: 0.8227 time: 0.2295s\n",
      "Epoch: 0350 loss_train: 0.1452 f1_train: 0.8220 time: 0.2164s\n",
      "Epoch: 0351 loss_train: 0.1464 f1_train: 0.8234 time: 0.2256s\n",
      "Epoch: 0352 loss_train: 0.1456 f1_train: 0.8199 time: 0.2218s\n",
      "Epoch: 0353 loss_train: 0.1440 f1_train: 0.8223 time: 0.2205s\n",
      "Epoch: 0354 loss_train: 0.1448 f1_train: 0.8234 time: 0.2282s\n",
      "Epoch: 0355 loss_train: 0.1458 f1_train: 0.8187 time: 0.1916s\n",
      "Epoch: 0356 loss_train: 0.1420 f1_train: 0.8234 time: 0.1837s\n",
      "Epoch: 0357 loss_train: 0.1460 f1_train: 0.8238 time: 0.1928s\n",
      "Epoch: 0358 loss_train: 0.1468 f1_train: 0.8251 time: 0.1848s\n",
      "Epoch: 0359 loss_train: 0.1464 f1_train: 0.8240 time: 0.1874s\n",
      "Epoch: 0360 loss_train: 0.1423 f1_train: 0.8278 time: 0.1891s\n",
      "Epoch: 0361 loss_train: 0.1460 f1_train: 0.8231 time: 0.1760s\n",
      "Epoch: 0362 loss_train: 0.1430 f1_train: 0.8219 time: 0.1841s\n",
      "Epoch: 0363 loss_train: 0.1468 f1_train: 0.8216 time: 0.1896s\n",
      "Epoch: 0364 loss_train: 0.1430 f1_train: 0.8295 time: 0.2264s\n",
      "Epoch: 0365 loss_train: 0.1417 f1_train: 0.8279 time: 0.2225s\n",
      "Epoch: 0366 loss_train: 0.1455 f1_train: 0.8255 time: 0.2289s\n",
      "Epoch: 0367 loss_train: 0.1423 f1_train: 0.8293 time: 0.2217s\n",
      "Epoch: 0368 loss_train: 0.1439 f1_train: 0.8200 time: 0.2248s\n",
      "Epoch: 0369 loss_train: 0.1458 f1_train: 0.8222 time: 0.2382s\n",
      "Epoch: 0370 loss_train: 0.1419 f1_train: 0.8275 time: 0.2312s\n",
      "Epoch: 0371 loss_train: 0.1456 f1_train: 0.8230 time: 0.2232s\n",
      "Epoch: 0372 loss_train: 0.1415 f1_train: 0.8290 time: 0.2276s\n",
      "Epoch: 0373 loss_train: 0.1438 f1_train: 0.8250 time: 0.2177s\n",
      "Epoch: 0374 loss_train: 0.1459 f1_train: 0.8290 time: 0.2286s\n",
      "Epoch: 0375 loss_train: 0.1458 f1_train: 0.8260 time: 0.2295s\n",
      "Epoch: 0376 loss_train: 0.1420 f1_train: 0.8286 time: 0.2268s\n",
      "Epoch: 0377 loss_train: 0.1414 f1_train: 0.8255 time: 0.1912s\n",
      "Epoch: 0378 loss_train: 0.1434 f1_train: 0.8249 time: 0.1807s\n",
      "Epoch: 0379 loss_train: 0.1433 f1_train: 0.8267 time: 0.2026s\n",
      "Epoch: 0380 loss_train: 0.1434 f1_train: 0.8266 time: 0.1860s\n",
      "Epoch: 0381 loss_train: 0.1417 f1_train: 0.8301 time: 0.2051s\n",
      "Epoch: 0382 loss_train: 0.1441 f1_train: 0.8255 time: 0.1803s\n",
      "Epoch: 0383 loss_train: 0.1422 f1_train: 0.8265 time: 0.1887s\n",
      "Epoch: 0384 loss_train: 0.1414 f1_train: 0.8337 time: 0.1762s\n",
      "Epoch: 0385 loss_train: 0.1392 f1_train: 0.8312 time: 0.1917s\n",
      "Epoch: 0386 loss_train: 0.1415 f1_train: 0.8306 time: 0.1936s\n",
      "Epoch: 0387 loss_train: 0.1406 f1_train: 0.8313 time: 0.1842s\n",
      "Epoch: 0388 loss_train: 0.1440 f1_train: 0.8242 time: 0.1777s\n",
      "Epoch: 0389 loss_train: 0.1406 f1_train: 0.8279 time: 0.2008s\n",
      "Epoch: 0390 loss_train: 0.1421 f1_train: 0.8284 time: 0.1837s\n",
      "Epoch: 0391 loss_train: 0.1382 f1_train: 0.8302 time: 0.2200s\n",
      "Epoch: 0392 loss_train: 0.1406 f1_train: 0.8246 time: 0.2179s\n",
      "Epoch: 0393 loss_train: 0.1398 f1_train: 0.8296 time: 0.1940s\n",
      "Epoch: 0394 loss_train: 0.1380 f1_train: 0.8337 time: 0.2219s\n",
      "Epoch: 0395 loss_train: 0.1400 f1_train: 0.8313 time: 0.2051s\n",
      "Epoch: 0396 loss_train: 0.1408 f1_train: 0.8284 time: 0.2841s\n",
      "Epoch: 0397 loss_train: 0.1405 f1_train: 0.8318 time: 0.2858s\n",
      "Epoch: 0398 loss_train: 0.1377 f1_train: 0.8306 time: 0.2283s\n",
      "Epoch: 0399 loss_train: 0.1374 f1_train: 0.8322 time: 0.2260s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0400 loss_train: 0.1389 f1_train: 0.8321 time: 0.2670s\n",
      "Epoch: 0401 loss_train: 0.1362 f1_train: 0.8330 time: 0.2494s\n",
      "Epoch: 0402 loss_train: 0.1420 f1_train: 0.8294 time: 0.2256s\n",
      "Epoch: 0403 loss_train: 0.1402 f1_train: 0.8272 time: 0.2578s\n",
      "Epoch: 0404 loss_train: 0.1412 f1_train: 0.8295 time: 0.3055s\n",
      "Epoch: 0405 loss_train: 0.1400 f1_train: 0.8267 time: 0.3246s\n",
      "Epoch: 0406 loss_train: 0.1387 f1_train: 0.8301 time: 0.2608s\n",
      "Epoch: 0407 loss_train: 0.1393 f1_train: 0.8315 time: 0.2784s\n",
      "Epoch: 0408 loss_train: 0.1375 f1_train: 0.8318 time: 0.2702s\n",
      "Epoch: 0409 loss_train: 0.1363 f1_train: 0.8335 time: 0.2649s\n",
      "Epoch: 0410 loss_train: 0.1387 f1_train: 0.8343 time: 0.2892s\n",
      "Epoch: 0411 loss_train: 0.1401 f1_train: 0.8342 time: 0.3564s\n",
      "Epoch: 0412 loss_train: 0.1363 f1_train: 0.8360 time: 0.2988s\n",
      "Epoch: 0413 loss_train: 0.1373 f1_train: 0.8335 time: 0.2981s\n",
      "Epoch: 0414 loss_train: 0.1370 f1_train: 0.8355 time: 0.2831s\n",
      "Epoch: 0415 loss_train: 0.1365 f1_train: 0.8367 time: 0.3121s\n",
      "Epoch: 0416 loss_train: 0.1353 f1_train: 0.8371 time: 0.3649s\n",
      "Epoch: 0417 loss_train: 0.1384 f1_train: 0.8324 time: 0.2915s\n",
      "Epoch: 0418 loss_train: 0.1356 f1_train: 0.8382 time: 0.3596s\n",
      "Epoch: 0419 loss_train: 0.1352 f1_train: 0.8361 time: 0.2768s\n",
      "Epoch: 0420 loss_train: 0.1342 f1_train: 0.8346 time: 0.3612s\n",
      "Epoch: 0421 loss_train: 0.1374 f1_train: 0.8319 time: 0.3619s\n",
      "Epoch: 0422 loss_train: 0.1382 f1_train: 0.8357 time: 0.3263s\n",
      "Epoch: 0423 loss_train: 0.1381 f1_train: 0.8358 time: 0.3111s\n",
      "Epoch: 0424 loss_train: 0.1370 f1_train: 0.8392 time: 0.3738s\n",
      "Epoch: 0425 loss_train: 0.1348 f1_train: 0.8387 time: 0.2649s\n",
      "Epoch: 0426 loss_train: 0.1371 f1_train: 0.8338 time: 0.2475s\n",
      "Epoch: 0427 loss_train: 0.1342 f1_train: 0.8364 time: 0.2913s\n",
      "Epoch: 0428 loss_train: 0.1364 f1_train: 0.8366 time: 0.3807s\n",
      "Epoch: 0429 loss_train: 0.1378 f1_train: 0.8321 time: 0.3142s\n",
      "Epoch: 0430 loss_train: 0.1353 f1_train: 0.8362 time: 0.3315s\n",
      "Epoch: 0431 loss_train: 0.1350 f1_train: 0.8404 time: 0.2925s\n",
      "Epoch: 0432 loss_train: 0.1333 f1_train: 0.8343 time: 0.4090s\n",
      "Epoch: 0433 loss_train: 0.1343 f1_train: 0.8305 time: 0.2902s\n",
      "Epoch: 0434 loss_train: 0.1373 f1_train: 0.8359 time: 0.3820s\n",
      "Epoch: 0435 loss_train: 0.1334 f1_train: 0.8403 time: 0.3228s\n",
      "Epoch: 0436 loss_train: 0.1344 f1_train: 0.8377 time: 0.2007s\n",
      "Epoch: 0437 loss_train: 0.1358 f1_train: 0.8365 time: 0.2407s\n",
      "Epoch: 0438 loss_train: 0.1336 f1_train: 0.8449 time: 0.2277s\n",
      "Epoch: 0439 loss_train: 0.1325 f1_train: 0.8353 time: 0.2231s\n",
      "Epoch: 0440 loss_train: 0.1308 f1_train: 0.8388 time: 0.2408s\n",
      "Epoch: 0441 loss_train: 0.1332 f1_train: 0.8397 time: 0.2676s\n",
      "Epoch: 0442 loss_train: 0.1328 f1_train: 0.8401 time: 0.2455s\n",
      "Epoch: 0443 loss_train: 0.1363 f1_train: 0.8341 time: 0.3187s\n",
      "Epoch: 0444 loss_train: 0.1331 f1_train: 0.8377 time: 0.2669s\n",
      "Epoch: 0445 loss_train: 0.1330 f1_train: 0.8370 time: 0.2662s\n",
      "Epoch: 0446 loss_train: 0.1335 f1_train: 0.8402 time: 0.2530s\n",
      "Epoch: 0447 loss_train: 0.1333 f1_train: 0.8433 time: 0.2931s\n",
      "Epoch: 0448 loss_train: 0.1309 f1_train: 0.8404 time: 0.2403s\n",
      "Epoch: 0449 loss_train: 0.1343 f1_train: 0.8349 time: 0.2394s\n",
      "Epoch: 0450 loss_train: 0.1318 f1_train: 0.8409 time: 0.3074s\n",
      "Epoch: 0451 loss_train: 0.1339 f1_train: 0.8424 time: 0.2471s\n",
      "Epoch: 0452 loss_train: 0.1309 f1_train: 0.8394 time: 0.2254s\n",
      "Epoch: 0453 loss_train: 0.1322 f1_train: 0.8422 time: 0.2452s\n",
      "Epoch: 0454 loss_train: 0.1336 f1_train: 0.8423 time: 0.2249s\n",
      "Epoch: 0455 loss_train: 0.1302 f1_train: 0.8366 time: 0.2101s\n",
      "Epoch: 0456 loss_train: 0.1357 f1_train: 0.8378 time: 0.1902s\n",
      "Epoch: 0457 loss_train: 0.1292 f1_train: 0.8451 time: 0.1907s\n",
      "Epoch: 0458 loss_train: 0.1360 f1_train: 0.8392 time: 0.2346s\n",
      "Epoch: 0459 loss_train: 0.1322 f1_train: 0.8435 time: 0.2569s\n",
      "Epoch: 0460 loss_train: 0.1300 f1_train: 0.8441 time: 0.1890s\n",
      "Epoch: 0461 loss_train: 0.1300 f1_train: 0.8436 time: 0.1908s\n",
      "Epoch: 0462 loss_train: 0.1330 f1_train: 0.8478 time: 0.1957s\n",
      "Epoch: 0463 loss_train: 0.1280 f1_train: 0.8451 time: 0.1908s\n",
      "Epoch: 0464 loss_train: 0.1321 f1_train: 0.8406 time: 0.2445s\n",
      "Epoch: 0465 loss_train: 0.1260 f1_train: 0.8459 time: 0.1898s\n",
      "Epoch: 0466 loss_train: 0.1337 f1_train: 0.8389 time: 0.1939s\n",
      "Epoch: 0467 loss_train: 0.1295 f1_train: 0.8425 time: 0.2039s\n",
      "Epoch: 0468 loss_train: 0.1283 f1_train: 0.8399 time: 0.1913s\n",
      "Epoch: 0469 loss_train: 0.1310 f1_train: 0.8435 time: 0.2156s\n",
      "Epoch: 0470 loss_train: 0.1322 f1_train: 0.8386 time: 0.2321s\n",
      "Epoch: 0471 loss_train: 0.1289 f1_train: 0.8423 time: 0.2326s\n",
      "Epoch: 0472 loss_train: 0.1312 f1_train: 0.8363 time: 0.2395s\n",
      "Epoch: 0473 loss_train: 0.1302 f1_train: 0.8364 time: 0.2423s\n",
      "Epoch: 0474 loss_train: 0.1282 f1_train: 0.8465 time: 0.2393s\n",
      "Epoch: 0475 loss_train: 0.1278 f1_train: 0.8445 time: 0.2811s\n",
      "Epoch: 0476 loss_train: 0.1284 f1_train: 0.8431 time: 0.2857s\n",
      "Epoch: 0477 loss_train: 0.1306 f1_train: 0.8394 time: 0.2903s\n",
      "Epoch: 0478 loss_train: 0.1297 f1_train: 0.8424 time: 0.2497s\n",
      "Epoch: 0479 loss_train: 0.1325 f1_train: 0.8417 time: 0.2402s\n",
      "Epoch: 0480 loss_train: 0.1269 f1_train: 0.8497 time: 0.2538s\n",
      "Epoch: 0481 loss_train: 0.1280 f1_train: 0.8466 time: 0.2438s\n",
      "Epoch: 0482 loss_train: 0.1305 f1_train: 0.8402 time: 0.2533s\n",
      "Epoch: 0483 loss_train: 0.1298 f1_train: 0.8434 time: 0.2711s\n",
      "Epoch: 0484 loss_train: 0.1311 f1_train: 0.8424 time: 0.2586s\n",
      "Epoch: 0485 loss_train: 0.1263 f1_train: 0.8428 time: 0.2951s\n",
      "Epoch: 0486 loss_train: 0.1273 f1_train: 0.8469 time: 0.3045s\n",
      "Epoch: 0487 loss_train: 0.1290 f1_train: 0.8374 time: 0.2809s\n",
      "Epoch: 0488 loss_train: 0.1307 f1_train: 0.8437 time: 0.3083s\n",
      "Epoch: 0489 loss_train: 0.1318 f1_train: 0.8403 time: 0.3011s\n",
      "Epoch: 0490 loss_train: 0.1281 f1_train: 0.8462 time: 0.3515s\n",
      "Epoch: 0491 loss_train: 0.1320 f1_train: 0.8455 time: 0.3650s\n",
      "Epoch: 0492 loss_train: 0.1294 f1_train: 0.8437 time: 0.2664s\n",
      "Epoch: 0493 loss_train: 0.1269 f1_train: 0.8489 time: 0.3402s\n",
      "Epoch: 0494 loss_train: 0.1284 f1_train: 0.8455 time: 0.2476s\n",
      "Epoch: 0495 loss_train: 0.1277 f1_train: 0.8425 time: 0.2486s\n",
      "Epoch: 0496 loss_train: 0.1279 f1_train: 0.8409 time: 0.2820s\n",
      "Epoch: 0497 loss_train: 0.1268 f1_train: 0.8461 time: 0.3121s\n",
      "Epoch: 0498 loss_train: 0.1272 f1_train: 0.8467 time: 0.2418s\n",
      "Epoch: 0499 loss_train: 0.1289 f1_train: 0.8463 time: 0.3299s\n",
      "Epoch: 0500 loss_train: 0.1256 f1_train: 0.8492 time: 0.2975s\n",
      "Epoch: 0501 loss_train: 0.1272 f1_train: 0.8447 time: 0.3135s\n",
      "Epoch: 0502 loss_train: 0.1305 f1_train: 0.8464 time: 0.2800s\n",
      "Epoch: 0503 loss_train: 0.1295 f1_train: 0.8443 time: 0.2681s\n",
      "Epoch: 0504 loss_train: 0.1260 f1_train: 0.8428 time: 0.2813s\n",
      "Epoch: 0505 loss_train: 0.1266 f1_train: 0.8464 time: 0.2660s\n",
      "Epoch: 0506 loss_train: 0.1275 f1_train: 0.8464 time: 0.2785s\n",
      "Epoch: 0507 loss_train: 0.1266 f1_train: 0.8492 time: 0.2752s\n",
      "Epoch: 0508 loss_train: 0.1252 f1_train: 0.8464 time: 0.2756s\n",
      "Epoch: 0509 loss_train: 0.1274 f1_train: 0.8440 time: 0.2391s\n",
      "Epoch: 0510 loss_train: 0.1284 f1_train: 0.8473 time: 0.2834s\n",
      "Epoch: 0511 loss_train: 0.1261 f1_train: 0.8496 time: 0.2839s\n",
      "Epoch: 0512 loss_train: 0.1271 f1_train: 0.8462 time: 0.2541s\n",
      "Epoch: 0513 loss_train: 0.1268 f1_train: 0.8504 time: 0.2496s\n",
      "Epoch: 0514 loss_train: 0.1268 f1_train: 0.8475 time: 0.2413s\n",
      "Epoch: 0515 loss_train: 0.1230 f1_train: 0.8534 time: 0.2478s\n",
      "Epoch: 0516 loss_train: 0.1272 f1_train: 0.8481 time: 0.2393s\n",
      "Epoch: 0517 loss_train: 0.1282 f1_train: 0.8417 time: 0.2576s\n",
      "Epoch: 0518 loss_train: 0.1274 f1_train: 0.8464 time: 0.2551s\n",
      "Epoch: 0519 loss_train: 0.1291 f1_train: 0.8443 time: 0.2401s\n",
      "Epoch: 0520 loss_train: 0.1246 f1_train: 0.8487 time: 0.2429s\n",
      "Epoch: 0521 loss_train: 0.1269 f1_train: 0.8480 time: 0.2452s\n",
      "Epoch: 0522 loss_train: 0.1275 f1_train: 0.8451 time: 0.2443s\n",
      "Epoch: 0523 loss_train: 0.1218 f1_train: 0.8534 time: 0.2650s\n",
      "Epoch: 0524 loss_train: 0.1260 f1_train: 0.8482 time: 0.2378s\n",
      "Epoch: 0525 loss_train: 0.1244 f1_train: 0.8505 time: 0.2351s\n",
      "Epoch: 0526 loss_train: 0.1244 f1_train: 0.8511 time: 0.2366s\n",
      "Epoch: 0527 loss_train: 0.1241 f1_train: 0.8528 time: 0.2353s\n",
      "Epoch: 0528 loss_train: 0.1231 f1_train: 0.8460 time: 0.2373s\n",
      "Epoch: 0529 loss_train: 0.1248 f1_train: 0.8523 time: 0.2369s\n",
      "Epoch: 0530 loss_train: 0.1250 f1_train: 0.8471 time: 0.3098s\n",
      "Epoch: 0531 loss_train: 0.1259 f1_train: 0.8459 time: 0.2490s\n",
      "Epoch: 0532 loss_train: 0.1244 f1_train: 0.8464 time: 0.2392s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0533 loss_train: 0.1224 f1_train: 0.8523 time: 0.2438s\n",
      "Epoch: 0534 loss_train: 0.1218 f1_train: 0.8538 time: 0.2431s\n",
      "Epoch: 0535 loss_train: 0.1239 f1_train: 0.8509 time: 0.2585s\n",
      "Epoch: 0536 loss_train: 0.1228 f1_train: 0.8512 time: 0.2595s\n",
      "Epoch: 0537 loss_train: 0.1243 f1_train: 0.8499 time: 0.2405s\n",
      "Epoch: 0538 loss_train: 0.1232 f1_train: 0.8513 time: 0.2307s\n",
      "Epoch: 0539 loss_train: 0.1216 f1_train: 0.8512 time: 0.2339s\n",
      "Epoch: 0540 loss_train: 0.1222 f1_train: 0.8495 time: 0.2390s\n",
      "Epoch: 0541 loss_train: 0.1219 f1_train: 0.8496 time: 0.2349s\n",
      "Epoch: 0542 loss_train: 0.1223 f1_train: 0.8479 time: 0.2412s\n",
      "Epoch: 0543 loss_train: 0.1226 f1_train: 0.8522 time: 0.2039s\n",
      "Epoch: 0544 loss_train: 0.1218 f1_train: 0.8522 time: 0.1977s\n",
      "Epoch: 0545 loss_train: 0.1264 f1_train: 0.8507 time: 0.1946s\n",
      "Epoch: 0546 loss_train: 0.1208 f1_train: 0.8533 time: 0.1922s\n",
      "Epoch: 0547 loss_train: 0.1246 f1_train: 0.8457 time: 0.1932s\n",
      "Epoch: 0548 loss_train: 0.1227 f1_train: 0.8518 time: 0.2078s\n",
      "Epoch: 0549 loss_train: 0.1226 f1_train: 0.8557 time: 0.2119s\n",
      "Epoch: 0550 loss_train: 0.1220 f1_train: 0.8500 time: 0.2415s\n",
      "Epoch: 0551 loss_train: 0.1221 f1_train: 0.8524 time: 0.2431s\n",
      "Epoch: 0552 loss_train: 0.1236 f1_train: 0.8491 time: 0.2579s\n",
      "Epoch: 0553 loss_train: 0.1228 f1_train: 0.8485 time: 0.2406s\n",
      "Epoch: 0554 loss_train: 0.1236 f1_train: 0.8523 time: 0.1973s\n",
      "Epoch: 0555 loss_train: 0.1211 f1_train: 0.8497 time: 0.1955s\n",
      "Epoch: 0556 loss_train: 0.1229 f1_train: 0.8501 time: 0.2378s\n",
      "Epoch: 0557 loss_train: 0.1227 f1_train: 0.8547 time: 0.2993s\n",
      "Epoch: 0558 loss_train: 0.1204 f1_train: 0.8535 time: 0.2894s\n",
      "Epoch: 0559 loss_train: 0.1195 f1_train: 0.8519 time: 0.2676s\n",
      "Epoch: 0560 loss_train: 0.1217 f1_train: 0.8517 time: 0.2406s\n",
      "Epoch: 0561 loss_train: 0.1204 f1_train: 0.8516 time: 0.3799s\n",
      "Epoch: 0562 loss_train: 0.1208 f1_train: 0.8555 time: 0.3105s\n",
      "Epoch: 0563 loss_train: 0.1203 f1_train: 0.8532 time: 0.3015s\n",
      "Epoch: 0564 loss_train: 0.1256 f1_train: 0.8508 time: 0.2822s\n",
      "Epoch: 0565 loss_train: 0.1215 f1_train: 0.8531 time: 0.2072s\n",
      "Epoch: 0566 loss_train: 0.1238 f1_train: 0.8490 time: 0.2000s\n",
      "Epoch: 0567 loss_train: 0.1221 f1_train: 0.8519 time: 0.1888s\n",
      "Epoch: 0568 loss_train: 0.1229 f1_train: 0.8530 time: 0.2033s\n",
      "Epoch: 0569 loss_train: 0.1216 f1_train: 0.8514 time: 0.2413s\n",
      "Epoch: 0570 loss_train: 0.1196 f1_train: 0.8518 time: 0.1973s\n",
      "Epoch: 0571 loss_train: 0.1216 f1_train: 0.8547 time: 0.1912s\n",
      "Epoch: 0572 loss_train: 0.1204 f1_train: 0.8541 time: 0.1898s\n",
      "Epoch: 0573 loss_train: 0.1188 f1_train: 0.8574 time: 0.2175s\n",
      "Epoch: 0574 loss_train: 0.1225 f1_train: 0.8445 time: 0.2284s\n",
      "Epoch: 0575 loss_train: 0.1211 f1_train: 0.8529 time: 0.2308s\n",
      "Epoch: 0576 loss_train: 0.1221 f1_train: 0.8524 time: 0.2355s\n",
      "Epoch: 0577 loss_train: 0.1212 f1_train: 0.8509 time: 0.2506s\n",
      "Epoch: 0578 loss_train: 0.1182 f1_train: 0.8522 time: 0.2407s\n",
      "Epoch: 0579 loss_train: 0.1205 f1_train: 0.8532 time: 0.2287s\n",
      "Epoch: 0580 loss_train: 0.1237 f1_train: 0.8487 time: 0.2412s\n",
      "Epoch: 0581 loss_train: 0.1190 f1_train: 0.8528 time: 0.2357s\n",
      "Epoch: 0582 loss_train: 0.1224 f1_train: 0.8601 time: 0.2434s\n",
      "Epoch: 0583 loss_train: 0.1206 f1_train: 0.8567 time: 0.2313s\n",
      "Epoch: 0584 loss_train: 0.1212 f1_train: 0.8535 time: 0.2323s\n",
      "Epoch: 0585 loss_train: 0.1196 f1_train: 0.8525 time: 0.2444s\n",
      "Epoch: 0586 loss_train: 0.1224 f1_train: 0.8511 time: 0.2364s\n",
      "Epoch: 0587 loss_train: 0.1210 f1_train: 0.8567 time: 0.2434s\n",
      "Epoch: 0588 loss_train: 0.1161 f1_train: 0.8560 time: 0.2357s\n",
      "Epoch: 0589 loss_train: 0.1163 f1_train: 0.8564 time: 0.2347s\n",
      "Epoch: 0590 loss_train: 0.1202 f1_train: 0.8539 time: 0.2397s\n",
      "Epoch: 0591 loss_train: 0.1190 f1_train: 0.8564 time: 0.2319s\n",
      "Epoch: 0592 loss_train: 0.1224 f1_train: 0.8515 time: 0.2398s\n",
      "Epoch: 0593 loss_train: 0.1201 f1_train: 0.8562 time: 0.2321s\n",
      "Epoch: 0594 loss_train: 0.1203 f1_train: 0.8562 time: 0.2334s\n",
      "Epoch: 0595 loss_train: 0.1217 f1_train: 0.8512 time: 0.2284s\n",
      "Epoch: 0596 loss_train: 0.1221 f1_train: 0.8479 time: 0.2300s\n",
      "Epoch: 0597 loss_train: 0.1190 f1_train: 0.8558 time: 0.2442s\n",
      "Epoch: 0598 loss_train: 0.1179 f1_train: 0.8545 time: 0.2321s\n",
      "Epoch: 0599 loss_train: 0.1191 f1_train: 0.8492 time: 0.2272s\n",
      "Epoch: 0600 loss_train: 0.1178 f1_train: 0.8581 time: 0.2283s\n",
      "Epoch: 0601 loss_train: 0.1160 f1_train: 0.8568 time: 0.2301s\n",
      "Epoch: 0602 loss_train: 0.1186 f1_train: 0.8534 time: 0.2319s\n",
      "Epoch: 0603 loss_train: 0.1176 f1_train: 0.8543 time: 0.3131s\n",
      "Epoch: 0604 loss_train: 0.1176 f1_train: 0.8578 time: 0.2374s\n",
      "Epoch: 0605 loss_train: 0.1176 f1_train: 0.8507 time: 0.2348s\n",
      "Epoch: 0606 loss_train: 0.1157 f1_train: 0.8593 time: 0.2325s\n",
      "Epoch: 0607 loss_train: 0.1233 f1_train: 0.8538 time: 0.2482s\n",
      "Epoch: 0608 loss_train: 0.1194 f1_train: 0.8559 time: 0.2316s\n",
      "Epoch: 0609 loss_train: 0.1195 f1_train: 0.8604 time: 0.2454s\n",
      "Epoch: 0610 loss_train: 0.1170 f1_train: 0.8594 time: 0.2381s\n",
      "Epoch: 0611 loss_train: 0.1178 f1_train: 0.8553 time: 0.2717s\n",
      "Epoch: 0612 loss_train: 0.1196 f1_train: 0.8552 time: 0.2476s\n",
      "Epoch: 0613 loss_train: 0.1162 f1_train: 0.8567 time: 0.2392s\n",
      "Epoch: 0614 loss_train: 0.1160 f1_train: 0.8573 time: 0.2534s\n",
      "Epoch: 0615 loss_train: 0.1179 f1_train: 0.8565 time: 0.2431s\n",
      "Epoch: 0616 loss_train: 0.1160 f1_train: 0.8597 time: 0.2399s\n",
      "Epoch: 0617 loss_train: 0.1177 f1_train: 0.8528 time: 0.2388s\n",
      "Epoch: 0618 loss_train: 0.1179 f1_train: 0.8562 time: 0.2363s\n",
      "Epoch: 0619 loss_train: 0.1183 f1_train: 0.8556 time: 0.2428s\n",
      "Epoch: 0620 loss_train: 0.1179 f1_train: 0.8538 time: 0.2365s\n",
      "Epoch: 0621 loss_train: 0.1207 f1_train: 0.8524 time: 0.2279s\n",
      "Epoch: 0622 loss_train: 0.1189 f1_train: 0.8514 time: 0.2352s\n",
      "Epoch: 0623 loss_train: 0.1196 f1_train: 0.8512 time: 0.2536s\n",
      "Epoch: 0624 loss_train: 0.1194 f1_train: 0.8533 time: 0.2303s\n",
      "Epoch: 0625 loss_train: 0.1171 f1_train: 0.8545 time: 0.3592s\n",
      "Epoch: 0626 loss_train: 0.1172 f1_train: 0.8588 time: 0.2001s\n",
      "Epoch: 0627 loss_train: 0.1169 f1_train: 0.8604 time: 0.1948s\n",
      "Epoch: 0628 loss_train: 0.1158 f1_train: 0.8557 time: 0.1907s\n",
      "Epoch: 0629 loss_train: 0.1157 f1_train: 0.8627 time: 0.3151s\n",
      "Epoch: 0630 loss_train: 0.1164 f1_train: 0.8586 time: 0.2477s\n",
      "Epoch: 0631 loss_train: 0.1167 f1_train: 0.8573 time: 0.2364s\n",
      "Epoch: 0632 loss_train: 0.1174 f1_train: 0.8566 time: 0.2579s\n",
      "Epoch: 0633 loss_train: 0.1176 f1_train: 0.8527 time: 0.2417s\n",
      "Epoch: 0634 loss_train: 0.1163 f1_train: 0.8582 time: 0.2338s\n",
      "Epoch: 0635 loss_train: 0.1181 f1_train: 0.8598 time: 0.2348s\n",
      "Epoch: 0636 loss_train: 0.1149 f1_train: 0.8595 time: 0.2416s\n",
      "Epoch: 0637 loss_train: 0.1169 f1_train: 0.8557 time: 0.2544s\n",
      "Epoch: 0638 loss_train: 0.1149 f1_train: 0.8604 time: 0.2418s\n",
      "Epoch: 0639 loss_train: 0.1144 f1_train: 0.8609 time: 0.2390s\n",
      "Epoch: 0640 loss_train: 0.1194 f1_train: 0.8541 time: 0.2471s\n",
      "Epoch: 0641 loss_train: 0.1149 f1_train: 0.8602 time: 0.2371s\n",
      "Epoch: 0642 loss_train: 0.1161 f1_train: 0.8560 time: 0.2388s\n",
      "Epoch: 0643 loss_train: 0.1177 f1_train: 0.8560 time: 0.2401s\n",
      "Epoch: 0644 loss_train: 0.1169 f1_train: 0.8564 time: 0.2307s\n",
      "Epoch: 0645 loss_train: 0.1161 f1_train: 0.8601 time: 0.2344s\n",
      "Epoch: 0646 loss_train: 0.1144 f1_train: 0.8607 time: 0.2370s\n",
      "Epoch: 0647 loss_train: 0.1148 f1_train: 0.8586 time: 0.2512s\n",
      "Epoch: 0648 loss_train: 0.1180 f1_train: 0.8576 time: 0.2330s\n",
      "Epoch: 0649 loss_train: 0.1173 f1_train: 0.8633 time: 0.2360s\n",
      "Epoch: 0650 loss_train: 0.1176 f1_train: 0.8556 time: 0.2418s\n",
      "Epoch: 0651 loss_train: 0.1161 f1_train: 0.8608 time: 0.2414s\n",
      "Epoch: 0652 loss_train: 0.1154 f1_train: 0.8631 time: 0.2420s\n",
      "Epoch: 0653 loss_train: 0.1155 f1_train: 0.8573 time: 0.2382s\n",
      "Epoch: 0654 loss_train: 0.1138 f1_train: 0.8615 time: 0.2316s\n",
      "Epoch: 0655 loss_train: 0.1143 f1_train: 0.8601 time: 0.2343s\n",
      "Epoch: 0656 loss_train: 0.1167 f1_train: 0.8612 time: 0.2399s\n",
      "Epoch: 0657 loss_train: 0.1149 f1_train: 0.8566 time: 0.2414s\n",
      "Epoch: 0658 loss_train: 0.1143 f1_train: 0.8617 time: 0.2648s\n",
      "Epoch: 0659 loss_train: 0.1153 f1_train: 0.8549 time: 0.2398s\n",
      "Epoch: 0660 loss_train: 0.1144 f1_train: 0.8593 time: 0.2441s\n",
      "Epoch: 0661 loss_train: 0.1166 f1_train: 0.8570 time: 0.2508s\n",
      "Epoch: 0662 loss_train: 0.1177 f1_train: 0.8554 time: 0.2366s\n",
      "Epoch: 0663 loss_train: 0.1141 f1_train: 0.8584 time: 0.2454s\n",
      "Epoch: 0664 loss_train: 0.1151 f1_train: 0.8619 time: 0.2375s\n",
      "Epoch: 0665 loss_train: 0.1172 f1_train: 0.8602 time: 0.2354s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0666 loss_train: 0.1157 f1_train: 0.8570 time: 0.2358s\n",
      "Epoch: 0667 loss_train: 0.1160 f1_train: 0.8625 time: 0.2381s\n",
      "Epoch: 0668 loss_train: 0.1151 f1_train: 0.8617 time: 0.2508s\n",
      "Epoch: 0669 loss_train: 0.1119 f1_train: 0.8610 time: 0.2344s\n",
      "Epoch: 0670 loss_train: 0.1149 f1_train: 0.8595 time: 0.2342s\n",
      "Epoch: 0671 loss_train: 0.1129 f1_train: 0.8621 time: 0.2465s\n",
      "Epoch: 0672 loss_train: 0.1144 f1_train: 0.8591 time: 0.2360s\n",
      "Epoch: 0673 loss_train: 0.1167 f1_train: 0.8572 time: 0.2290s\n",
      "Epoch: 0674 loss_train: 0.1110 f1_train: 0.8629 time: 0.2308s\n",
      "Epoch: 0675 loss_train: 0.1122 f1_train: 0.8602 time: 0.2477s\n",
      "Epoch: 0676 loss_train: 0.1162 f1_train: 0.8594 time: 0.2317s\n",
      "Epoch: 0677 loss_train: 0.1122 f1_train: 0.8553 time: 0.2320s\n",
      "Epoch: 0678 loss_train: 0.1132 f1_train: 0.8618 time: 0.2454s\n",
      "Epoch: 0679 loss_train: 0.1139 f1_train: 0.8609 time: 0.2459s\n",
      "Epoch: 0680 loss_train: 0.1163 f1_train: 0.8593 time: 0.2379s\n",
      "Epoch: 0681 loss_train: 0.1169 f1_train: 0.8547 time: 0.2504s\n",
      "Epoch: 0682 loss_train: 0.1138 f1_train: 0.8633 time: 0.2299s\n",
      "Epoch: 0683 loss_train: 0.1128 f1_train: 0.8608 time: 0.2506s\n",
      "Epoch: 0684 loss_train: 0.1136 f1_train: 0.8612 time: 0.2366s\n",
      "Epoch: 0685 loss_train: 0.1129 f1_train: 0.8624 time: 0.2625s\n",
      "Epoch: 0686 loss_train: 0.1132 f1_train: 0.8624 time: 0.2334s\n",
      "Epoch: 0687 loss_train: 0.1117 f1_train: 0.8606 time: 0.2563s\n",
      "Epoch: 0688 loss_train: 0.1114 f1_train: 0.8593 time: 0.2403s\n",
      "Epoch: 0689 loss_train: 0.1115 f1_train: 0.8618 time: 0.2656s\n",
      "Epoch: 0690 loss_train: 0.1133 f1_train: 0.8621 time: 0.1925s\n",
      "Epoch: 0691 loss_train: 0.1104 f1_train: 0.8637 time: 0.1946s\n",
      "Epoch: 0692 loss_train: 0.1127 f1_train: 0.8603 time: 0.2159s\n",
      "Epoch: 0693 loss_train: 0.1128 f1_train: 0.8566 time: 0.2660s\n",
      "Epoch: 0694 loss_train: 0.1160 f1_train: 0.8561 time: 0.2348s\n",
      "Epoch: 0695 loss_train: 0.1113 f1_train: 0.8620 time: 0.2381s\n",
      "Epoch: 0696 loss_train: 0.1094 f1_train: 0.8653 time: 0.2396s\n",
      "Epoch: 0697 loss_train: 0.1126 f1_train: 0.8607 time: 0.2484s\n",
      "Epoch: 0698 loss_train: 0.1121 f1_train: 0.8595 time: 0.2440s\n",
      "Epoch: 0699 loss_train: 0.1144 f1_train: 0.8582 time: 0.2387s\n",
      "Epoch: 0700 loss_train: 0.1127 f1_train: 0.8590 time: 0.2369s\n",
      "Epoch: 0701 loss_train: 0.1120 f1_train: 0.8607 time: 0.2325s\n",
      "Epoch: 0702 loss_train: 0.1102 f1_train: 0.8648 time: 0.2512s\n",
      "Epoch: 0703 loss_train: 0.1111 f1_train: 0.8643 time: 0.2410s\n",
      "Epoch: 0704 loss_train: 0.1116 f1_train: 0.8609 time: 0.2392s\n",
      "Epoch: 0705 loss_train: 0.1136 f1_train: 0.8620 time: 0.2478s\n",
      "Epoch: 0706 loss_train: 0.1116 f1_train: 0.8652 time: 0.2846s\n",
      "Epoch: 0707 loss_train: 0.1104 f1_train: 0.8620 time: 0.2771s\n",
      "Epoch: 0708 loss_train: 0.1112 f1_train: 0.8656 time: 0.2647s\n",
      "Epoch: 0709 loss_train: 0.1131 f1_train: 0.8597 time: 0.2405s\n",
      "Epoch: 0710 loss_train: 0.1091 f1_train: 0.8595 time: 0.2559s\n",
      "Epoch: 0711 loss_train: 0.1122 f1_train: 0.8580 time: 0.2647s\n",
      "Epoch: 0712 loss_train: 0.1127 f1_train: 0.8593 time: 0.2128s\n",
      "Epoch: 0713 loss_train: 0.1130 f1_train: 0.8606 time: 0.2034s\n",
      "Epoch: 0714 loss_train: 0.1133 f1_train: 0.8582 time: 0.2242s\n",
      "Epoch: 0715 loss_train: 0.1108 f1_train: 0.8678 time: 0.1990s\n",
      "Epoch: 0716 loss_train: 0.1105 f1_train: 0.8626 time: 0.2057s\n",
      "Epoch: 0717 loss_train: 0.1124 f1_train: 0.8632 time: 0.1935s\n",
      "Epoch: 0718 loss_train: 0.1117 f1_train: 0.8615 time: 0.2486s\n",
      "Epoch: 0719 loss_train: 0.1122 f1_train: 0.8621 time: 0.2565s\n",
      "Epoch: 0720 loss_train: 0.1113 f1_train: 0.8651 time: 0.2471s\n",
      "Epoch: 0721 loss_train: 0.1111 f1_train: 0.8617 time: 0.2353s\n",
      "Epoch: 0722 loss_train: 0.1105 f1_train: 0.8656 time: 0.2419s\n",
      "Epoch: 0723 loss_train: 0.1105 f1_train: 0.8613 time: 0.2317s\n",
      "Epoch: 0724 loss_train: 0.1108 f1_train: 0.8638 time: 0.2373s\n",
      "Epoch: 0725 loss_train: 0.1121 f1_train: 0.8644 time: 0.2409s\n",
      "Epoch: 0726 loss_train: 0.1094 f1_train: 0.8620 time: 0.2363s\n",
      "Epoch: 0727 loss_train: 0.1101 f1_train: 0.8653 time: 0.2415s\n",
      "Epoch: 0728 loss_train: 0.1123 f1_train: 0.8632 time: 0.2036s\n",
      "Epoch: 0729 loss_train: 0.1116 f1_train: 0.8655 time: 0.1943s\n",
      "Epoch: 0730 loss_train: 0.1099 f1_train: 0.8632 time: 0.1940s\n",
      "Epoch: 0731 loss_train: 0.1087 f1_train: 0.8665 time: 0.1915s\n",
      "Epoch: 0732 loss_train: 0.1097 f1_train: 0.8633 time: 0.2104s\n",
      "Epoch: 0733 loss_train: 0.1109 f1_train: 0.8701 time: 0.1966s\n",
      "Epoch: 0734 loss_train: 0.1121 f1_train: 0.8625 time: 0.1986s\n",
      "Epoch: 0735 loss_train: 0.1088 f1_train: 0.8666 time: 0.1923s\n",
      "Epoch: 0736 loss_train: 0.1086 f1_train: 0.8670 time: 0.1905s\n",
      "Epoch: 0737 loss_train: 0.1098 f1_train: 0.8639 time: 0.1995s\n",
      "Epoch: 0738 loss_train: 0.1123 f1_train: 0.8606 time: 0.1956s\n",
      "Epoch: 0739 loss_train: 0.1103 f1_train: 0.8640 time: 0.2118s\n",
      "Epoch: 0740 loss_train: 0.1103 f1_train: 0.8657 time: 0.2546s\n",
      "Epoch: 0741 loss_train: 0.1104 f1_train: 0.8638 time: 0.2514s\n",
      "Epoch: 0742 loss_train: 0.1112 f1_train: 0.8633 time: 0.2347s\n",
      "Epoch: 0743 loss_train: 0.1088 f1_train: 0.8633 time: 0.2455s\n",
      "Epoch: 0744 loss_train: 0.1095 f1_train: 0.8681 time: 0.2555s\n",
      "Epoch: 0745 loss_train: 0.1086 f1_train: 0.8650 time: 0.2534s\n",
      "Epoch: 0746 loss_train: 0.1100 f1_train: 0.8664 time: 0.2406s\n",
      "Epoch: 0747 loss_train: 0.1073 f1_train: 0.8649 time: 0.2313s\n",
      "Epoch: 0748 loss_train: 0.1103 f1_train: 0.8677 time: 0.2316s\n",
      "Epoch: 0749 loss_train: 0.1129 f1_train: 0.8653 time: 0.2345s\n",
      "Epoch: 0750 loss_train: 0.1107 f1_train: 0.8630 time: 0.2407s\n",
      "Epoch: 0751 loss_train: 0.1102 f1_train: 0.8613 time: 0.2367s\n",
      "Epoch: 0752 loss_train: 0.1123 f1_train: 0.8615 time: 0.2686s\n",
      "Epoch: 0753 loss_train: 0.1090 f1_train: 0.8629 time: 0.2391s\n",
      "Epoch: 0754 loss_train: 0.1113 f1_train: 0.8619 time: 0.2438s\n",
      "Epoch: 0755 loss_train: 0.1108 f1_train: 0.8637 time: 0.2381s\n",
      "Epoch: 0756 loss_train: 0.1092 f1_train: 0.8627 time: 0.2657s\n",
      "Epoch: 0757 loss_train: 0.1086 f1_train: 0.8612 time: 0.2440s\n",
      "Epoch: 0758 loss_train: 0.1092 f1_train: 0.8680 time: 0.2686s\n",
      "Epoch: 0759 loss_train: 0.1089 f1_train: 0.8628 time: 0.2470s\n",
      "Epoch: 0760 loss_train: 0.1086 f1_train: 0.8670 time: 0.2537s\n",
      "Epoch: 0761 loss_train: 0.1091 f1_train: 0.8659 time: 0.2405s\n",
      "Epoch: 0762 loss_train: 0.1112 f1_train: 0.8615 time: 0.2525s\n",
      "Epoch: 0763 loss_train: 0.1100 f1_train: 0.8623 time: 0.2317s\n",
      "Epoch: 0764 loss_train: 0.1096 f1_train: 0.8640 time: 0.2773s\n",
      "Epoch: 0765 loss_train: 0.1094 f1_train: 0.8655 time: 0.2406s\n",
      "Epoch: 0766 loss_train: 0.1081 f1_train: 0.8633 time: 0.2540s\n",
      "Epoch: 0767 loss_train: 0.1063 f1_train: 0.8644 time: 0.2533s\n",
      "Epoch: 0768 loss_train: 0.1065 f1_train: 0.8721 time: 0.2511s\n",
      "Epoch: 0769 loss_train: 0.1095 f1_train: 0.8693 time: 0.2399s\n",
      "Epoch: 0770 loss_train: 0.1090 f1_train: 0.8638 time: 0.2662s\n",
      "Epoch: 0771 loss_train: 0.1078 f1_train: 0.8721 time: 0.2417s\n",
      "Epoch: 0772 loss_train: 0.1085 f1_train: 0.8635 time: 0.2363s\n",
      "Epoch: 0773 loss_train: 0.1099 f1_train: 0.8646 time: 0.2147s\n",
      "Epoch: 0774 loss_train: 0.1081 f1_train: 0.8646 time: 0.2226s\n",
      "Epoch: 0775 loss_train: 0.1077 f1_train: 0.8740 time: 0.2121s\n",
      "Epoch: 0776 loss_train: 0.1104 f1_train: 0.8640 time: 0.2428s\n",
      "Epoch: 0777 loss_train: 0.1074 f1_train: 0.8662 time: 0.2347s\n",
      "Epoch: 0778 loss_train: 0.1064 f1_train: 0.8715 time: 0.2308s\n",
      "Epoch: 0779 loss_train: 0.1075 f1_train: 0.8710 time: 0.2353s\n",
      "Epoch: 0780 loss_train: 0.1064 f1_train: 0.8684 time: 0.2490s\n",
      "Epoch: 0781 loss_train: 0.1085 f1_train: 0.8685 time: 0.2361s\n",
      "Epoch: 0782 loss_train: 0.1069 f1_train: 0.8666 time: 0.2358s\n",
      "Epoch: 0783 loss_train: 0.1080 f1_train: 0.8696 time: 0.2614s\n",
      "Epoch: 0784 loss_train: 0.1075 f1_train: 0.8687 time: 0.2405s\n",
      "Epoch: 0785 loss_train: 0.1067 f1_train: 0.8673 time: 0.2462s\n",
      "Epoch: 0786 loss_train: 0.1073 f1_train: 0.8670 time: 0.2460s\n",
      "Epoch: 0787 loss_train: 0.1088 f1_train: 0.8686 time: 0.2423s\n",
      "Epoch: 0788 loss_train: 0.1072 f1_train: 0.8660 time: 0.2573s\n",
      "Epoch: 0789 loss_train: 0.1067 f1_train: 0.8652 time: 0.2518s\n",
      "Epoch: 0790 loss_train: 0.1060 f1_train: 0.8662 time: 0.2432s\n",
      "Epoch: 0791 loss_train: 0.1088 f1_train: 0.8709 time: 0.2437s\n",
      "Epoch: 0792 loss_train: 0.1066 f1_train: 0.8667 time: 0.2355s\n",
      "Epoch: 0793 loss_train: 0.1060 f1_train: 0.8681 time: 0.2369s\n",
      "Epoch: 0794 loss_train: 0.1049 f1_train: 0.8682 time: 0.2380s\n",
      "Epoch: 0795 loss_train: 0.1070 f1_train: 0.8678 time: 0.2865s\n",
      "Epoch: 0796 loss_train: 0.1079 f1_train: 0.8652 time: 0.2333s\n",
      "Epoch: 0797 loss_train: 0.1105 f1_train: 0.8629 time: 0.2307s\n",
      "Epoch: 0798 loss_train: 0.1089 f1_train: 0.8622 time: 0.2344s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0799 loss_train: 0.1080 f1_train: 0.8666 time: 0.2292s\n",
      "Epoch: 0800 loss_train: 0.1061 f1_train: 0.8699 time: 0.2310s\n",
      "Epoch: 0801 loss_train: 0.1087 f1_train: 0.8658 time: 0.2427s\n",
      "Epoch: 0802 loss_train: 0.1065 f1_train: 0.8721 time: 0.2447s\n",
      "Epoch: 0803 loss_train: 0.1046 f1_train: 0.8720 time: 0.2423s\n",
      "Epoch: 0804 loss_train: 0.1065 f1_train: 0.8723 time: 0.2424s\n",
      "Epoch: 0805 loss_train: 0.1050 f1_train: 0.8665 time: 0.2381s\n",
      "Epoch: 0806 loss_train: 0.1051 f1_train: 0.8679 time: 0.2447s\n",
      "Epoch: 0807 loss_train: 0.1075 f1_train: 0.8612 time: 0.2449s\n",
      "Epoch: 0808 loss_train: 0.1088 f1_train: 0.8652 time: 0.2516s\n",
      "Epoch: 0809 loss_train: 0.1052 f1_train: 0.8701 time: 0.2341s\n",
      "Epoch: 0810 loss_train: 0.1068 f1_train: 0.8711 time: 0.2328s\n",
      "Epoch: 0811 loss_train: 0.1050 f1_train: 0.8666 time: 0.2509s\n",
      "Epoch: 0812 loss_train: 0.1066 f1_train: 0.8693 time: 0.2475s\n",
      "Epoch: 0813 loss_train: 0.1086 f1_train: 0.8667 time: 0.2454s\n",
      "Epoch: 0814 loss_train: 0.1077 f1_train: 0.8645 time: 0.2548s\n",
      "Epoch: 0815 loss_train: 0.1105 f1_train: 0.8618 time: 0.2462s\n",
      "Epoch: 0816 loss_train: 0.1046 f1_train: 0.8689 time: 0.2532s\n",
      "Epoch: 0817 loss_train: 0.1047 f1_train: 0.8703 time: 0.2366s\n",
      "Epoch: 0818 loss_train: 0.1065 f1_train: 0.8645 time: 0.2427s\n",
      "Epoch: 0819 loss_train: 0.1038 f1_train: 0.8698 time: 0.2463s\n",
      "Epoch: 0820 loss_train: 0.1070 f1_train: 0.8664 time: 0.2606s\n",
      "Epoch: 0821 loss_train: 0.1038 f1_train: 0.8745 time: 0.2350s\n",
      "Epoch: 0822 loss_train: 0.1034 f1_train: 0.8709 time: 0.2328s\n",
      "Epoch: 0823 loss_train: 0.1074 f1_train: 0.8720 time: 0.2526s\n",
      "Epoch: 0824 loss_train: 0.1059 f1_train: 0.8686 time: 0.2351s\n",
      "Epoch: 0825 loss_train: 0.1063 f1_train: 0.8660 time: 0.2335s\n",
      "Epoch: 0826 loss_train: 0.1072 f1_train: 0.8676 time: 0.2343s\n",
      "Epoch: 0827 loss_train: 0.1052 f1_train: 0.8706 time: 0.2646s\n",
      "Epoch: 0828 loss_train: 0.1074 f1_train: 0.8665 time: 0.2417s\n",
      "Epoch: 0829 loss_train: 0.1073 f1_train: 0.8673 time: 0.2352s\n",
      "Epoch: 0830 loss_train: 0.1065 f1_train: 0.8694 time: 0.2313s\n",
      "Epoch: 0831 loss_train: 0.1067 f1_train: 0.8704 time: 0.2441s\n",
      "Epoch: 0832 loss_train: 0.1065 f1_train: 0.8688 time: 0.2317s\n",
      "Epoch: 0833 loss_train: 0.1065 f1_train: 0.8681 time: 0.2599s\n",
      "Epoch: 0834 loss_train: 0.1053 f1_train: 0.8747 time: 0.2315s\n",
      "Epoch: 0835 loss_train: 0.1039 f1_train: 0.8731 time: 0.2489s\n",
      "Epoch: 0836 loss_train: 0.1069 f1_train: 0.8658 time: 0.2474s\n",
      "Epoch: 0837 loss_train: 0.1039 f1_train: 0.8709 time: 0.2475s\n",
      "Epoch: 0838 loss_train: 0.1040 f1_train: 0.8715 time: 0.2327s\n",
      "Epoch: 0839 loss_train: 0.1049 f1_train: 0.8662 time: 0.2703s\n",
      "Epoch: 0840 loss_train: 0.1029 f1_train: 0.8695 time: 0.2395s\n",
      "Epoch: 0841 loss_train: 0.1055 f1_train: 0.8688 time: 0.2640s\n",
      "Epoch: 0842 loss_train: 0.1037 f1_train: 0.8693 time: 0.2352s\n",
      "Epoch: 0843 loss_train: 0.1047 f1_train: 0.8704 time: 0.2642s\n",
      "Epoch: 0844 loss_train: 0.1014 f1_train: 0.8743 time: 0.2594s\n",
      "Epoch: 0845 loss_train: 0.1055 f1_train: 0.8700 time: 0.2925s\n",
      "Epoch: 0846 loss_train: 0.1068 f1_train: 0.8735 time: 0.2153s\n",
      "Epoch: 0847 loss_train: 0.1038 f1_train: 0.8734 time: 0.2185s\n",
      "Epoch: 0848 loss_train: 0.1049 f1_train: 0.8674 time: 0.2655s\n",
      "Epoch: 0849 loss_train: 0.1035 f1_train: 0.8706 time: 0.2511s\n",
      "Epoch: 0850 loss_train: 0.1046 f1_train: 0.8667 time: 0.2447s\n",
      "Epoch: 0851 loss_train: 0.1040 f1_train: 0.8685 time: 0.2596s\n",
      "Epoch: 0852 loss_train: 0.1026 f1_train: 0.8688 time: 0.2344s\n",
      "Epoch: 0853 loss_train: 0.1046 f1_train: 0.8676 time: 0.2496s\n",
      "Epoch: 0854 loss_train: 0.1043 f1_train: 0.8707 time: 0.2383s\n",
      "Epoch: 0855 loss_train: 0.1051 f1_train: 0.8722 time: 0.2439s\n",
      "Epoch: 0856 loss_train: 0.1061 f1_train: 0.8723 time: 0.2407s\n",
      "Epoch: 0857 loss_train: 0.1069 f1_train: 0.8708 time: 0.2609s\n",
      "Epoch: 0858 loss_train: 0.1068 f1_train: 0.8691 time: 0.2454s\n",
      "Epoch: 0859 loss_train: 0.1031 f1_train: 0.8719 time: 0.2432s\n",
      "Epoch: 0860 loss_train: 0.1032 f1_train: 0.8740 time: 0.2290s\n",
      "Epoch: 0861 loss_train: 0.1039 f1_train: 0.8671 time: 0.2345s\n",
      "Epoch: 0862 loss_train: 0.1038 f1_train: 0.8711 time: 0.2355s\n",
      "Epoch: 0863 loss_train: 0.1036 f1_train: 0.8738 time: 0.2361s\n",
      "Epoch: 0864 loss_train: 0.1027 f1_train: 0.8737 time: 0.2178s\n",
      "Epoch: 0865 loss_train: 0.1046 f1_train: 0.8741 time: 0.2581s\n",
      "Epoch: 0866 loss_train: 0.1003 f1_train: 0.8773 time: 0.2503s\n",
      "Epoch: 0867 loss_train: 0.1006 f1_train: 0.8753 time: 0.2331s\n",
      "Epoch: 0868 loss_train: 0.1048 f1_train: 0.8666 time: 0.2419s\n",
      "Epoch: 0869 loss_train: 0.1001 f1_train: 0.8791 time: 0.2309s\n",
      "Epoch: 0870 loss_train: 0.1008 f1_train: 0.8717 time: 0.2516s\n",
      "Epoch: 0871 loss_train: 0.1028 f1_train: 0.8709 time: 0.2423s\n",
      "Epoch: 0872 loss_train: 0.1033 f1_train: 0.8673 time: 0.2363s\n",
      "Epoch: 0873 loss_train: 0.1017 f1_train: 0.8722 time: 0.2401s\n",
      "Epoch: 0874 loss_train: 0.1024 f1_train: 0.8734 time: 0.2369s\n",
      "Epoch: 0875 loss_train: 0.1040 f1_train: 0.8716 time: 0.2442s\n",
      "Epoch: 0876 loss_train: 0.1030 f1_train: 0.8683 time: 0.2635s\n",
      "Epoch: 0877 loss_train: 0.1010 f1_train: 0.8740 time: 0.2415s\n",
      "Epoch: 0878 loss_train: 0.1025 f1_train: 0.8674 time: 0.2495s\n",
      "Epoch: 0879 loss_train: 0.1056 f1_train: 0.8660 time: 0.2376s\n",
      "Epoch: 0880 loss_train: 0.1036 f1_train: 0.8713 time: 0.2332s\n",
      "Epoch: 0881 loss_train: 0.1017 f1_train: 0.8770 time: 0.2453s\n",
      "Epoch: 0882 loss_train: 0.1025 f1_train: 0.8711 time: 0.2844s\n",
      "Epoch: 0883 loss_train: 0.1031 f1_train: 0.8744 time: 0.2376s\n",
      "Epoch: 0884 loss_train: 0.1046 f1_train: 0.8692 time: 0.2391s\n",
      "Epoch: 0885 loss_train: 0.1035 f1_train: 0.8719 time: 0.2418s\n",
      "Epoch: 0886 loss_train: 0.1013 f1_train: 0.8728 time: 0.2451s\n",
      "Epoch: 0887 loss_train: 0.1011 f1_train: 0.8751 time: 0.2400s\n",
      "Epoch: 0888 loss_train: 0.1036 f1_train: 0.8708 time: 0.2606s\n",
      "Epoch: 0889 loss_train: 0.1027 f1_train: 0.8732 time: 0.2321s\n",
      "Epoch: 0890 loss_train: 0.1029 f1_train: 0.8702 time: 0.2334s\n",
      "Epoch: 0891 loss_train: 0.1005 f1_train: 0.8731 time: 0.2301s\n",
      "Epoch: 0892 loss_train: 0.1032 f1_train: 0.8753 time: 0.2288s\n",
      "Epoch: 0893 loss_train: 0.1047 f1_train: 0.8659 time: 0.2401s\n",
      "Epoch: 0894 loss_train: 0.1025 f1_train: 0.8718 time: 0.2287s\n",
      "Epoch: 0895 loss_train: 0.1047 f1_train: 0.8678 time: 0.2371s\n",
      "Epoch: 0896 loss_train: 0.1020 f1_train: 0.8722 time: 0.2413s\n",
      "Epoch: 0897 loss_train: 0.1021 f1_train: 0.8754 time: 0.2356s\n",
      "Epoch: 0898 loss_train: 0.1036 f1_train: 0.8717 time: 0.2358s\n",
      "Epoch: 0899 loss_train: 0.1006 f1_train: 0.8734 time: 0.2378s\n",
      "Epoch: 0900 loss_train: 0.1039 f1_train: 0.8709 time: 0.2385s\n",
      "Epoch: 0901 loss_train: 0.1021 f1_train: 0.8708 time: 0.2467s\n",
      "Epoch: 0902 loss_train: 0.1002 f1_train: 0.8787 time: 0.2746s\n",
      "Epoch: 0903 loss_train: 0.1029 f1_train: 0.8724 time: 0.2442s\n",
      "Epoch: 0904 loss_train: 0.1015 f1_train: 0.8723 time: 0.2499s\n",
      "Epoch: 0905 loss_train: 0.1029 f1_train: 0.8751 time: 0.2569s\n",
      "Epoch: 0906 loss_train: 0.1026 f1_train: 0.8695 time: 0.2379s\n",
      "Epoch: 0907 loss_train: 0.1018 f1_train: 0.8709 time: 0.2613s\n",
      "Epoch: 0908 loss_train: 0.1012 f1_train: 0.8750 time: 0.2388s\n",
      "Epoch: 0909 loss_train: 0.1031 f1_train: 0.8707 time: 0.2478s\n",
      "Epoch: 0910 loss_train: 0.0998 f1_train: 0.8760 time: 0.2454s\n",
      "Epoch: 0911 loss_train: 0.1019 f1_train: 0.8694 time: 0.2424s\n",
      "Epoch: 0912 loss_train: 0.1008 f1_train: 0.8739 time: 0.2359s\n",
      "Epoch: 0913 loss_train: 0.1021 f1_train: 0.8752 time: 0.2375s\n",
      "Epoch: 0914 loss_train: 0.1023 f1_train: 0.8664 time: 0.2399s\n",
      "Epoch: 0915 loss_train: 0.1028 f1_train: 0.8715 time: 0.2477s\n",
      "Epoch: 0916 loss_train: 0.1029 f1_train: 0.8705 time: 0.2503s\n",
      "Epoch: 0917 loss_train: 0.1018 f1_train: 0.8715 time: 0.2511s\n",
      "Epoch: 0918 loss_train: 0.1027 f1_train: 0.8690 time: 0.2372s\n",
      "Epoch: 0919 loss_train: 0.1014 f1_train: 0.8739 time: 0.2427s\n",
      "Epoch: 0920 loss_train: 0.1024 f1_train: 0.8730 time: 0.2533s\n",
      "Epoch: 0921 loss_train: 0.1012 f1_train: 0.8737 time: 0.2322s\n",
      "Epoch: 0922 loss_train: 0.1007 f1_train: 0.8732 time: 0.2368s\n",
      "Epoch: 0923 loss_train: 0.1005 f1_train: 0.8722 time: 0.2438s\n",
      "Epoch: 0924 loss_train: 0.1010 f1_train: 0.8758 time: 0.2429s\n",
      "Epoch: 0925 loss_train: 0.1022 f1_train: 0.8736 time: 0.2466s\n",
      "Epoch: 0926 loss_train: 0.1022 f1_train: 0.8707 time: 0.2624s\n",
      "Epoch: 0927 loss_train: 0.1005 f1_train: 0.8744 time: 0.2361s\n",
      "Epoch: 0928 loss_train: 0.1009 f1_train: 0.8731 time: 0.2447s\n",
      "Epoch: 0929 loss_train: 0.1003 f1_train: 0.8728 time: 0.2416s\n",
      "Epoch: 0930 loss_train: 0.1014 f1_train: 0.8722 time: 0.2549s\n",
      "Epoch: 0931 loss_train: 0.1027 f1_train: 0.8713 time: 0.2451s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0932 loss_train: 0.1007 f1_train: 0.8751 time: 0.2662s\n",
      "Epoch: 0933 loss_train: 0.1018 f1_train: 0.8758 time: 0.2459s\n",
      "Epoch: 0934 loss_train: 0.1015 f1_train: 0.8752 time: 0.2491s\n",
      "Epoch: 0935 loss_train: 0.0993 f1_train: 0.8746 time: 0.2474s\n",
      "Epoch: 0936 loss_train: 0.1009 f1_train: 0.8723 time: 0.2526s\n",
      "Epoch: 0937 loss_train: 0.1022 f1_train: 0.8711 time: 0.2443s\n",
      "Epoch: 0938 loss_train: 0.1023 f1_train: 0.8746 time: 0.2544s\n",
      "Epoch: 0939 loss_train: 0.0996 f1_train: 0.8700 time: 0.2438s\n",
      "Epoch: 0940 loss_train: 0.0976 f1_train: 0.8802 time: 0.2376s\n",
      "Epoch: 0941 loss_train: 0.1004 f1_train: 0.8769 time: 0.2353s\n",
      "Epoch: 0942 loss_train: 0.0978 f1_train: 0.8735 time: 0.2318s\n",
      "Epoch: 0943 loss_train: 0.0994 f1_train: 0.8763 time: 0.2364s\n",
      "Epoch: 0944 loss_train: 0.1022 f1_train: 0.8725 time: 0.2439s\n",
      "Epoch: 0945 loss_train: 0.1009 f1_train: 0.8734 time: 0.2720s\n",
      "Epoch: 0946 loss_train: 0.1013 f1_train: 0.8728 time: 0.2403s\n",
      "Epoch: 0947 loss_train: 0.1005 f1_train: 0.8725 time: 0.2534s\n",
      "Epoch: 0948 loss_train: 0.0982 f1_train: 0.8756 time: 0.2463s\n",
      "Epoch: 0949 loss_train: 0.1009 f1_train: 0.8732 time: 0.2449s\n",
      "Epoch: 0950 loss_train: 0.1006 f1_train: 0.8726 time: 0.2492s\n",
      "Epoch: 0951 loss_train: 0.1013 f1_train: 0.8744 time: 0.2492s\n",
      "Epoch: 0952 loss_train: 0.0989 f1_train: 0.8752 time: 0.2459s\n",
      "Epoch: 0953 loss_train: 0.1011 f1_train: 0.8745 time: 0.2352s\n",
      "Epoch: 0954 loss_train: 0.0993 f1_train: 0.8762 time: 0.2423s\n",
      "Epoch: 0955 loss_train: 0.0989 f1_train: 0.8762 time: 0.2424s\n",
      "Epoch: 0956 loss_train: 0.1010 f1_train: 0.8756 time: 0.2410s\n",
      "Epoch: 0957 loss_train: 0.0992 f1_train: 0.8765 time: 0.2663s\n",
      "Epoch: 0958 loss_train: 0.0978 f1_train: 0.8762 time: 0.1947s\n",
      "Epoch: 0959 loss_train: 0.0973 f1_train: 0.8764 time: 0.1952s\n",
      "Epoch: 0960 loss_train: 0.1002 f1_train: 0.8750 time: 0.1962s\n",
      "Epoch: 0961 loss_train: 0.1019 f1_train: 0.8708 time: 0.1971s\n",
      "Epoch: 0962 loss_train: 0.0995 f1_train: 0.8712 time: 0.1954s\n",
      "Epoch: 0963 loss_train: 0.0985 f1_train: 0.8781 time: 0.2111s\n",
      "Epoch: 0964 loss_train: 0.0994 f1_train: 0.8772 time: 0.1896s\n",
      "Epoch: 0965 loss_train: 0.0974 f1_train: 0.8779 time: 0.2145s\n",
      "Epoch: 0966 loss_train: 0.0974 f1_train: 0.8763 time: 0.1972s\n",
      "Epoch: 0967 loss_train: 0.0979 f1_train: 0.8756 time: 0.1953s\n",
      "Epoch: 0968 loss_train: 0.0984 f1_train: 0.8752 time: 0.1960s\n",
      "Epoch: 0969 loss_train: 0.1010 f1_train: 0.8744 time: 0.1937s\n",
      "Epoch: 0970 loss_train: 0.1016 f1_train: 0.8811 time: 0.2216s\n",
      "Epoch: 0971 loss_train: 0.0983 f1_train: 0.8774 time: 0.1950s\n",
      "Epoch: 0972 loss_train: 0.0998 f1_train: 0.8785 time: 0.2057s\n",
      "Epoch: 0973 loss_train: 0.0995 f1_train: 0.8727 time: 0.1925s\n",
      "Epoch: 0974 loss_train: 0.1000 f1_train: 0.8734 time: 0.2039s\n",
      "Epoch: 0975 loss_train: 0.1000 f1_train: 0.8730 time: 0.1935s\n",
      "Epoch: 0976 loss_train: 0.0987 f1_train: 0.8746 time: 0.1975s\n",
      "Epoch: 0977 loss_train: 0.0972 f1_train: 0.8768 time: 0.2058s\n",
      "Epoch: 0978 loss_train: 0.0984 f1_train: 0.8747 time: 0.1912s\n",
      "Epoch: 0979 loss_train: 0.0983 f1_train: 0.8760 time: 0.1924s\n",
      "Epoch: 0980 loss_train: 0.1000 f1_train: 0.8787 time: 0.2116s\n",
      "Epoch: 0981 loss_train: 0.1014 f1_train: 0.8771 time: 0.2025s\n",
      "Epoch: 0982 loss_train: 0.0985 f1_train: 0.8802 time: 0.1916s\n",
      "Epoch: 0983 loss_train: 0.0973 f1_train: 0.8837 time: 0.1910s\n",
      "Epoch: 0984 loss_train: 0.0988 f1_train: 0.8743 time: 0.2032s\n",
      "Epoch: 0985 loss_train: 0.0979 f1_train: 0.8736 time: 0.1947s\n",
      "Epoch: 0986 loss_train: 0.0978 f1_train: 0.8785 time: 0.1935s\n",
      "Epoch: 0987 loss_train: 0.0997 f1_train: 0.8740 time: 0.1905s\n",
      "Epoch: 0988 loss_train: 0.0995 f1_train: 0.8725 time: 0.2132s\n",
      "Epoch: 0989 loss_train: 0.0984 f1_train: 0.8792 time: 0.1930s\n",
      "Epoch: 0990 loss_train: 0.0977 f1_train: 0.8783 time: 0.1925s\n",
      "Epoch: 0991 loss_train: 0.1003 f1_train: 0.8711 time: 0.1923s\n",
      "Epoch: 0992 loss_train: 0.0996 f1_train: 0.8771 time: 0.2046s\n",
      "Epoch: 0993 loss_train: 0.1001 f1_train: 0.8738 time: 0.2072s\n",
      "Epoch: 0994 loss_train: 0.0976 f1_train: 0.8791 time: 0.1934s\n",
      "Epoch: 0995 loss_train: 0.0974 f1_train: 0.8778 time: 0.2161s\n",
      "Epoch: 0996 loss_train: 0.0972 f1_train: 0.8742 time: 0.1934s\n",
      "Epoch: 0997 loss_train: 0.1010 f1_train: 0.8745 time: 0.1993s\n",
      "Epoch: 0998 loss_train: 0.0987 f1_train: 0.8792 time: 0.1934s\n",
      "Epoch: 0999 loss_train: 0.0967 f1_train: 0.8788 time: 0.1950s\n",
      "Epoch: 1000 loss_train: 0.0990 f1_train: 0.8763 time: 0.1904s\n",
      "Optimization Finished!\n",
      "torch.Size([29894, 100])\n"
     ]
    }
   ],
   "source": [
    "# setup training \n",
    "epochs = 1000\n",
    "best_loss = 1\n",
    "node_emb_train = None \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    gcn.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = gcn(features, adj)\n",
    "    loss_train = loss(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    f1_train = fscore(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epoch_finish = time.time() - epoch_start\n",
    "\n",
    "    if best_loss >= loss_train.item():\n",
    "        node_emb_train = gcn.node_embeddings[idx_train]\n",
    "\n",
    "    print(\n",
    "        \"Epoch: {:04d}\".format(epoch+1),\n",
    "        \"loss_train: {:.4f}\".format(loss_train.item()),\n",
    "        \"f1_train: {:.4f}\".format(f1_train),\n",
    "        \"time: {:.4f}s\".format(epoch_finish)\n",
    "    )\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(node_emb_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.3388 precision= 0.7145 recall= 0.5522 f1_test= 0.6229 f1_micro= 0.9566 confusion= [[15348   239]\n",
      " [  485   598]]\n"
     ]
    }
   ],
   "source": [
    "node_emb_test = None \n",
    "gcn.eval()\n",
    "output = gcn(features, adj)\n",
    "loss_test = loss(output[idx_test], labels[idx_test])\n",
    "\n",
    "\n",
    "precision_score = precision(output[idx_test], labels[idx_test])\n",
    "recall_score = recall(output[idx_test], labels[idx_test])\n",
    "f1_test = fscore(output[idx_test], labels[idx_test])\n",
    "f1_micro = fscore_micro(output[idx_test], labels[idx_test])\n",
    "confusion_score = confusion(output[idx_test], labels[idx_test])\n",
    "\n",
    "node_emb_test = gcn.node_embeddings[idx_test]\n",
    "print(\n",
    "    \"Test set results:\",\n",
    "    \"loss= {:.4f}\".format(loss_test.item()),\n",
    "    \"precision= {:.4f}\".format(precision_score),\n",
    "    \"recall= {:.4f}\".format(recall_score),\n",
    "    \"f1_test= {:.4f}\".format(f1_test),\n",
    "    \"f1_micro= {:.4f}\".format(f1_micro),\n",
    "    \"confusion= {}\".format(confusion_score)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0083, 0.3024,  ..., 0.1794, 1.6047, 0.8650],\n",
      "        [0.0000, 0.0713, 0.0217,  ..., 0.3394, 0.2266, 0.9797],\n",
      "        [0.0000, 0.2618, 0.3410,  ..., 0.0000, 0.1967, 0.6465],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.4406, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3884, 0.2667,  ..., 1.2470, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0845, 0.0000,  ..., 0.8972, 0.0000, 0.0000]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, 13.4875],\n",
      "        [ 0.4090,  1.5558,  0.0000,  ...,  0.0000,  0.0000,  0.7072],\n",
      "        [ 0.2371,  1.3477,  0.0000,  ...,  0.0000,  0.0000,  1.4946],\n",
      "        ...,\n",
      "        [ 0.0000,  0.2896,  0.0000,  ...,  1.1371,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.4188,  0.0000,  ...,  0.2333,  0.0000,  1.9477],\n",
      "        [ 0.0000,  0.2896,  0.0000,  ...,  1.1371,  0.0000,  0.0000]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "[[0.         0.00832899 0.30244613 ... 0.1793779  1.6046935  0.8649827 ]\n",
      " [0.         0.07125264 0.02169302 ... 0.339422   0.22663425 0.97971666]\n",
      " [0.         0.26177335 0.3409792  ... 0.         0.19667423 0.64653337]\n",
      " ...\n",
      " [0.         0.2895727  0.         ... 1.1370978  0.         0.        ]\n",
      " [0.         0.41875008 0.         ... 0.23334041 0.         1.9476739 ]\n",
      " [0.         0.2895727  0.         ... 1.1370978  0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NE_0</th>\n",
       "      <th>NE_1</th>\n",
       "      <th>NE_2</th>\n",
       "      <th>NE_3</th>\n",
       "      <th>NE_4</th>\n",
       "      <th>NE_5</th>\n",
       "      <th>NE_6</th>\n",
       "      <th>NE_7</th>\n",
       "      <th>NE_8</th>\n",
       "      <th>NE_9</th>\n",
       "      <th>...</th>\n",
       "      <th>NE_90</th>\n",
       "      <th>NE_91</th>\n",
       "      <th>NE_92</th>\n",
       "      <th>NE_93</th>\n",
       "      <th>NE_94</th>\n",
       "      <th>NE_95</th>\n",
       "      <th>NE_96</th>\n",
       "      <th>NE_97</th>\n",
       "      <th>NE_98</th>\n",
       "      <th>NE_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>0.302446</td>\n",
       "      <td>1.323299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.613717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.488811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046523</td>\n",
       "      <td>0.626475</td>\n",
       "      <td>1.034606</td>\n",
       "      <td>0.179378</td>\n",
       "      <td>1.604694</td>\n",
       "      <td>0.864983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071253</td>\n",
       "      <td>0.021693</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.384066</td>\n",
       "      <td>0.621358</td>\n",
       "      <td>0.339422</td>\n",
       "      <td>0.226634</td>\n",
       "      <td>0.979717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261773</td>\n",
       "      <td>0.340979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.273804</td>\n",
       "      <td>0.025174</td>\n",
       "      <td>0.059816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217886</td>\n",
       "      <td>0.049815</td>\n",
       "      <td>0.050207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.775647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196674</td>\n",
       "      <td>0.646533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.430564</td>\n",
       "      <td>0.261773</td>\n",
       "      <td>0.340979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234948</td>\n",
       "      <td>0.025174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.264930</td>\n",
       "      <td>0.466865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.269725</td>\n",
       "      <td>6.051972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.234658</td>\n",
       "      <td>6.183120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.412065</td>\n",
       "      <td>1.704852</td>\n",
       "      <td>0.475080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46559</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095185</td>\n",
       "      <td>0.010942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863316</td>\n",
       "      <td>2.010445</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46560</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095185</td>\n",
       "      <td>0.010942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.540508</td>\n",
       "      <td>1.055462</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46561</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.137098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46562</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.418750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.770373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.947674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46563</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.137098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46564 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           NE_0      NE_1      NE_2      NE_3  NE_4      NE_5  NE_6      NE_7  \\\n",
       "0      0.000000  0.008329  0.302446  1.323299   0.0  0.613717   0.0  0.488811   \n",
       "1      0.000000  0.071253  0.021693  0.072501   0.0  0.028073   0.0  0.108444   \n",
       "2      0.000000  0.261773  0.340979  0.000000   0.0  0.000000   0.0  0.273804   \n",
       "3      0.430564  0.261773  0.340979  0.000000   0.0  0.000000   0.0  0.234948   \n",
       "4      1.269725  6.051972  0.000000  0.000000   0.0  0.000000   0.0  0.000000   \n",
       "...         ...       ...       ...       ...   ...       ...   ...       ...   \n",
       "46559  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   0.0  0.000000   \n",
       "46560  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   0.0  0.085342   \n",
       "46561  0.000000  0.289573  0.000000  0.000000   0.0  0.000000   0.0  0.000000   \n",
       "46562  0.000000  0.418750  0.000000  0.000000   0.0  0.000000   0.0  0.000000   \n",
       "46563  0.000000  0.289573  0.000000  0.000000   0.0  0.000000   0.0  0.000000   \n",
       "\n",
       "           NE_8      NE_9  ...     NE_90     NE_91     NE_92     NE_93  \\\n",
       "0      0.000000  0.000000  ...  0.006043  0.000000  0.000000  0.000000   \n",
       "1      0.000000  0.021302  ...  0.006307  0.000000  0.000000  0.000000   \n",
       "2      0.025174  0.059816  ...  0.000000  0.217886  0.049815  0.050207   \n",
       "3      0.025174  0.000000  ...  0.000000  0.217886  0.000000  0.000000   \n",
       "4      0.000000  0.452678  ...  0.000000  0.000000  0.041546  0.000000   \n",
       "...         ...       ...  ...       ...       ...       ...       ...   \n",
       "46559  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.095185   \n",
       "46560  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.095185   \n",
       "46561  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "46562  0.000000  0.000000  ...  0.000000  0.127624  0.000000  0.321237   \n",
       "46563  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          NE_94     NE_95     NE_96     NE_97     NE_98     NE_99  \n",
       "0      0.046523  0.626475  1.034606  0.179378  1.604694  0.864983  \n",
       "1      0.061458  0.384066  0.621358  0.339422  0.226634  0.979717  \n",
       "2      0.000000  1.775647  0.000000  0.000000  0.196674  0.646533  \n",
       "3      0.000000  1.264930  0.466865  0.000000  0.000000  0.346438  \n",
       "4      2.234658  6.183120  0.000000  1.412065  1.704852  0.475080  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "46559  0.010942  0.000000  0.000000  0.863316  2.010445  0.000000  \n",
       "46560  0.010942  0.000000  0.000000  1.540508  1.055462  0.000000  \n",
       "46561  0.000000  0.000000  0.000000  1.137098  0.000000  0.000000  \n",
       "46562  0.770373  0.000000  0.000000  0.233340  0.000000  1.947674  \n",
       "46563  0.000000  0.000000  0.000000  1.137098  0.000000  0.000000  \n",
       "\n",
       "[46564 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(node_emb_train)\n",
    "print(node_emb_test)\n",
    "\n",
    "np_node_emb = np.concatenate((node_emb_train.cpu().detach().numpy(), \n",
    "                            node_emb_test.cpu().detach().numpy()))\n",
    "\n",
    "print(np_node_emb)\n",
    "\n",
    "# Create embeddings pandas DataFrame \n",
    "node_emb_pd = pd.DataFrame(np_node_emb) \n",
    "node_emb_pd.columns = [f\"NE_{i}\" for i in range(np_node_emb.shape[1])]\n",
    "display(node_emb_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[232438397 232029206 232344069 ... 158375075 147478192 158375402]\n"
     ]
    }
   ],
   "source": [
    "data = ellipticdr.dataset_.copy()\n",
    "txIds = data[(data[\"class\"] != -1)][\"txId\"].values\n",
    "node_emb_pd.insert(0, \"txId\", txIds)\n",
    "print(txIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>NE_0</th>\n",
       "      <th>NE_1</th>\n",
       "      <th>NE_2</th>\n",
       "      <th>NE_3</th>\n",
       "      <th>NE_4</th>\n",
       "      <th>NE_5</th>\n",
       "      <th>NE_6</th>\n",
       "      <th>NE_7</th>\n",
       "      <th>NE_8</th>\n",
       "      <th>...</th>\n",
       "      <th>NE_90</th>\n",
       "      <th>NE_91</th>\n",
       "      <th>NE_92</th>\n",
       "      <th>NE_93</th>\n",
       "      <th>NE_94</th>\n",
       "      <th>NE_95</th>\n",
       "      <th>NE_96</th>\n",
       "      <th>NE_97</th>\n",
       "      <th>NE_98</th>\n",
       "      <th>NE_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232438397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>0.302446</td>\n",
       "      <td>1.323299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.613717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.488811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046523</td>\n",
       "      <td>0.626475</td>\n",
       "      <td>1.034606</td>\n",
       "      <td>0.179378</td>\n",
       "      <td>1.604694</td>\n",
       "      <td>0.864983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232029206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071253</td>\n",
       "      <td>0.021693</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.384066</td>\n",
       "      <td>0.621358</td>\n",
       "      <td>0.339422</td>\n",
       "      <td>0.226634</td>\n",
       "      <td>0.979717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232344069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261773</td>\n",
       "      <td>0.340979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.273804</td>\n",
       "      <td>0.025174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217886</td>\n",
       "      <td>0.049815</td>\n",
       "      <td>0.050207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.775647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196674</td>\n",
       "      <td>0.646533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27553029</td>\n",
       "      <td>0.430564</td>\n",
       "      <td>0.261773</td>\n",
       "      <td>0.340979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234948</td>\n",
       "      <td>0.025174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.264930</td>\n",
       "      <td>0.466865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3881097</td>\n",
       "      <td>1.269725</td>\n",
       "      <td>6.051972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.234658</td>\n",
       "      <td>6.183120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.412065</td>\n",
       "      <td>1.704852</td>\n",
       "      <td>0.475080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46559</th>\n",
       "      <td>80329479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095185</td>\n",
       "      <td>0.010942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863316</td>\n",
       "      <td>2.010445</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46560</th>\n",
       "      <td>158406298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095185</td>\n",
       "      <td>0.010942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.540508</td>\n",
       "      <td>1.055462</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46561</th>\n",
       "      <td>158375075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.137098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46562</th>\n",
       "      <td>147478192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.418750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.770373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.947674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46563</th>\n",
       "      <td>158375402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.137098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46564 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            txId      NE_0      NE_1      NE_2      NE_3  NE_4      NE_5  \\\n",
       "0      232438397  0.000000  0.008329  0.302446  1.323299   0.0  0.613717   \n",
       "1      232029206  0.000000  0.071253  0.021693  0.072501   0.0  0.028073   \n",
       "2      232344069  0.000000  0.261773  0.340979  0.000000   0.0  0.000000   \n",
       "3       27553029  0.430564  0.261773  0.340979  0.000000   0.0  0.000000   \n",
       "4        3881097  1.269725  6.051972  0.000000  0.000000   0.0  0.000000   \n",
       "...          ...       ...       ...       ...       ...   ...       ...   \n",
       "46559   80329479  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "46560  158406298  0.000000  0.000000  0.000000  0.000000   0.0  0.000000   \n",
       "46561  158375075  0.000000  0.289573  0.000000  0.000000   0.0  0.000000   \n",
       "46562  147478192  0.000000  0.418750  0.000000  0.000000   0.0  0.000000   \n",
       "46563  158375402  0.000000  0.289573  0.000000  0.000000   0.0  0.000000   \n",
       "\n",
       "       NE_6      NE_7      NE_8  ...     NE_90     NE_91     NE_92     NE_93  \\\n",
       "0       0.0  0.488811  0.000000  ...  0.006043  0.000000  0.000000  0.000000   \n",
       "1       0.0  0.108444  0.000000  ...  0.006307  0.000000  0.000000  0.000000   \n",
       "2       0.0  0.273804  0.025174  ...  0.000000  0.217886  0.049815  0.050207   \n",
       "3       0.0  0.234948  0.025174  ...  0.000000  0.217886  0.000000  0.000000   \n",
       "4       0.0  0.000000  0.000000  ...  0.000000  0.000000  0.041546  0.000000   \n",
       "...     ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "46559   0.0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.095185   \n",
       "46560   0.0  0.085342  0.000000  ...  0.000000  0.000000  0.000000  0.095185   \n",
       "46561   0.0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "46562   0.0  0.000000  0.000000  ...  0.000000  0.127624  0.000000  0.321237   \n",
       "46563   0.0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          NE_94     NE_95     NE_96     NE_97     NE_98     NE_99  \n",
       "0      0.046523  0.626475  1.034606  0.179378  1.604694  0.864983  \n",
       "1      0.061458  0.384066  0.621358  0.339422  0.226634  0.979717  \n",
       "2      0.000000  1.775647  0.000000  0.000000  0.196674  0.646533  \n",
       "3      0.000000  1.264930  0.466865  0.000000  0.000000  0.346438  \n",
       "4      2.234658  6.183120  0.000000  1.412065  1.704852  0.475080  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "46559  0.010942  0.000000  0.000000  0.863316  2.010445  0.000000  \n",
       "46560  0.010942  0.000000  0.000000  1.540508  1.055462  0.000000  \n",
       "46561  0.000000  0.000000  0.000000  1.137098  0.000000  0.000000  \n",
       "46562  0.770373  0.000000  0.000000  0.233340  0.000000  1.947674  \n",
       "46563  0.000000  0.000000  0.000000  1.137098  0.000000  0.000000  \n",
       "\n",
       "[46564 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(node_emb_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_emb_pd.to_csv(\"elliptic_embs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(node_emb_pd[\"NE_99\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('btc-classifier': conda)",
   "language": "python",
   "name": "python37664bitbtcclassifiercondaf328939486114fc0aeb107830f867d66"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
