{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import cryptoaml.datareader as cdr\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic = cdr.get_data(\"elliptic\")\n",
    "data = elliptic.train_test_split(train_size=0.7, \n",
    "                                 feat_set=\"AF\", \n",
    "                                 inc_meta=False,\n",
    "                                 inc_unknown=False)\n",
    "\n",
    "train_data = data.train_X\n",
    "train_data[\"class\"] = data.train_y\n",
    "test_data = data.test_X\n",
    "test_data[\"class\"] = data.test_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "PROB A: [0.9989837  0.00101632]\n",
      "0\n",
      "-----------\n",
      "SIZE: 1\n",
      "PROB B: [9.9987447e-01 1.2552003e-04]\n",
      "PROB A: [9.9965274e-01 3.4728620e-04]\n",
      "1\n",
      "-----------\n",
      "SIZE: 2\n",
      "PROB B: [9.9960059e-01 3.9939774e-04]\n",
      "PROB B: [9.9922591e-01 7.7409094e-04]\n",
      "PROB A: [9.9981189e-01 1.8813132e-04]\n",
      "2\n",
      "-----------\n",
      "SIZE: 3\n",
      "PROB B: [0.9849953  0.01500468]\n",
      "PROB B: [0.94188005 0.05811994]\n",
      "PROB B: [0.99391216 0.00608784]\n",
      "PROB A: [0.99446124 0.00553876]\n",
      "3\n",
      "-----------\n",
      "SIZE: 4\n",
      "PROB B: [0.98452944 0.01547055]\n",
      "PROB B: [0.994727 0.005273]\n",
      "PROB B: [0.9789501  0.02104991]\n",
      "PROB B: [0.81304264 0.18695739]\n",
      "PROB A: [0.9928846  0.00711542]\n",
      "4\n",
      "-----------\n",
      "SIZE: 5\n",
      "PROB B: [0.9988693  0.00113071]\n",
      "PROB B: [9.9985301e-01 1.4697922e-04]\n",
      "PROB B: [9.999096e-01 9.041272e-05]\n",
      "PROB B: [0.76241195 0.23758803]\n",
      "PROB B: [0.9986924  0.00130761]\n",
      "ensemble is full\n",
      "PROB A: [0.9978377  0.00216227]\n",
      "\n",
      "5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-48142d2f1283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mtest_set_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0madpBoost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;31m#     y_pred = adpBoost.predict(test_set_X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-48142d2f1283>\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_window_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             self._train_on_mini_batch(X=self._X_buffer[0:self._window_size, :],\n\u001b[0;32m---> 30\u001b[0;31m                                       y=self._y_buffer[0:self._window_size])\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mdelete_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_window_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelete_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-48142d2f1283>\u001b[0m in \u001b[0;36m_train_on_mini_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensemble_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_model_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensemble_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_model_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensemble_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_model_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "class AdaptiveStackedBoostClassifier():\n",
    "    def __init__(self,\n",
    "                 n_estimators=5,\n",
    "                 window_size=2000,\n",
    "                 verbose=True):\n",
    "        self._first_run = True\n",
    "        self._n_estimators = n_estimators\n",
    "        self._window_size = window_size\n",
    "        self._X_buffer = np.array([])\n",
    "        self._y_buffer = np.array([])\n",
    "        self._meta_learner = xgb.XGBClassifier()\n",
    "        \n",
    "        # 3*N matrix \n",
    "        # 1st row - first level models\n",
    "        # 2nd row - number of training rounds \n",
    "        # 3rd row - performance in terms on feature importance of meta model for n previous rounds\n",
    "        self._ensemble_matrix = [[None for x in range(n_estimators)] for y in range(3)]\n",
    "        self._ensemble_size = 0 \n",
    "        \n",
    "    def partial_fit(self, X, y):\n",
    "        if self._first_run:\n",
    "            self._X_buffer = np.array([]).reshape(0, X.shape[1])\n",
    "            self._y_buffer = np.array([])\n",
    "            self._first_run = False\n",
    "                           \n",
    "        self._X_buffer = np.concatenate((self._X_buffer, X))\n",
    "        self._y_buffer = np.concatenate((self._y_buffer, y))\n",
    "        while self._X_buffer.shape[0] >= self._window_size:\n",
    "            self._train_on_mini_batch(X=self._X_buffer[0:self._window_size, :],\n",
    "                                      y=self._y_buffer[0:self._window_size])\n",
    "            delete_idx = [i for i in range(self._window_size)]\n",
    "            self._X_buffer = np.delete(self._X_buffer, delete_idx, axis=0)\n",
    "            self._y_buffer = np.delete(self._y_buffer, delete_idx, axis=0)\n",
    "    \n",
    "    def _train_on_mini_batch(self, X, y):\n",
    "        \n",
    "        print(\"-----------\")\n",
    "        \n",
    "        # split mini batch to train ensemble and meta learner \n",
    "        n_instances = X.shape[0]\n",
    "        n_instances_first = int(n_instances * 0.7)\n",
    "        X_first = X[0: n_instances_first, :]\n",
    "        y_first = y[0: n_instances_first]\n",
    "        X_second = X[n_instances_first:n_instances, :]\n",
    "        y_second = y[n_instances_first:n_instances]\n",
    "          \n",
    "        new_model_idx = self._ensemble_size \n",
    "        ensemble_full = self._ensemble_size == self._n_estimators \n",
    "        worst_score = 1 \n",
    "        worst_model = None \n",
    "        \n",
    "        # continue fitting model in ensemble \n",
    "        meta_model_X = []\n",
    "        if self._ensemble_size > 0:\n",
    "            print(\"SIZE: {}\".format(self._ensemble_size))\n",
    "            performances = self._meta_learner.feature_importances_\n",
    "            for i in range(self._ensemble_size):\n",
    "                \n",
    "                # partially fit and construct features for meta model\n",
    "                old_model = self._ensemble_matrix[0][i]\n",
    "                old_model_booster = old_model.get_booster()\n",
    "                old_model.fit(X_first, y_first, xgb_model=old_model_booster)\n",
    "                y_pred_prob_old_model = old_model.predict_proba(X_second) \n",
    "                print(\"PROB B: {}\".format(y_pred_prob_old_model[0]))\n",
    "\n",
    "                # construct features from probs for meta model \n",
    "                meta_model_X = y_pred_prob_old_model if i == 0 else np.hstack((meta_model_X, y_pred_prob_old_model))                    \n",
    "                                \n",
    "                # update model in matrix \n",
    "                self._ensemble_matrix[0][i] = old_model # update model \n",
    "                last_performance = performances[i * 2] + performances[(i*2)+1]  \n",
    "                current_round =  self._ensemble_matrix[1][i]\n",
    "                self._ensemble_matrix[2][i][current_round % self._n_estimators] = last_performance                \n",
    "\n",
    "                if ensemble_full and current_round >= 5:\n",
    "#                     print(\"IM HERE\")\n",
    "                    if self._ensemble_matrix[2][i].sum() <= worst_score:\n",
    "                        worst_score = self._ensemble_matrix[2][i].sum()\n",
    "                        new_model_idx = i\n",
    "                        print(\"WORST {} \".format(i))\n",
    "                    \n",
    "                self._ensemble_matrix[1][i] += 1\n",
    "                     \n",
    "#         print(new_model_idx)\n",
    "        \n",
    "        # check if ensemble is full \n",
    "        if ensemble_full:\n",
    "            print(\"ensemble is full\")\n",
    "            \n",
    "        # partially train a new model to become part of the ensemble \n",
    "        new_model = xgb.XGBClassifier()\n",
    "        new_model.fit(X_first, y_first)\n",
    "        y_pred_prob = new_model.predict_proba(X_second)\n",
    "        print(\"PROB A: {}\".format(y_pred_prob[0]))\n",
    "        if self._ensemble_size == 0:\n",
    "            meta_model_X = y_pred_prob\n",
    "        else: \n",
    "            if ensemble_full:\n",
    "                print(\"\")\n",
    "            else:\n",
    "                meta_model_X = np.hstack((meta_model_X, y_pred_prob))      \n",
    "            \n",
    "        # fit meta model  \n",
    "        n_current_meta_model_X = meta_model_X.shape[1]  \n",
    "        n_meta_model_X = self._n_estimators * 2\n",
    "        if n_current_meta_model_X < n_meta_model_X:\n",
    "            n_diff = n_meta_model_X - n_current_meta_model_X\n",
    "            fillers = np.zeros((meta_model_X.shape[0], n_diff))\n",
    "            meta_model_X = np.hstack((meta_model_X, fillers)) \n",
    "                \n",
    "#         print(meta_model_X.shape)\n",
    "        if self._ensemble_size == 0:\n",
    "            self._meta_learner.fit(meta_model_X, y_second)\n",
    "        else:\n",
    "            tmp_meta_learner_booster = self._meta_learner.get_booster()\n",
    "            self._meta_learner.fit(meta_model_X, y_second, xgb_model=tmp_meta_learner_booster)    \n",
    "        \n",
    "        print(new_model_idx)\n",
    "        self._ensemble_matrix[0][new_model_idx] = new_model \n",
    "        self._ensemble_matrix[1][new_model_idx] = 0 \n",
    "        self._ensemble_matrix[2][new_model_idx] = np.zeros(self._n_estimators) \n",
    "        \n",
    "        if self._ensemble_size != self._n_estimators:\n",
    "            self._ensemble_size += 1 \n",
    "               \n",
    "    def predict(self, X):\n",
    "      \n",
    "        # only one model in ensemble use its predictions \n",
    "        if self._ensemble_size == 1:\n",
    "            return self._ensemble_matrix[0][0].predict(X)\n",
    "        \n",
    "        # predict via meta learner \n",
    "        meta_model_X = []       \n",
    "        # construct stracked features\n",
    "        for i in range(self._ensemble_size):\n",
    "            y_pred_prob_tmp_model = self._ensemble_matrix[0][i].predict_proba(X) \n",
    "            if i == 0:\n",
    "                meta_model_X = y_pred_prob_tmp_model\n",
    "                continue\n",
    "            np.hstack((meta_model_X, y_pred_prob_tmp_model)) \n",
    "        \n",
    "        # add fillers if needed \n",
    "        n_current_meta_model_X = meta_model_X.shape[1]  \n",
    "        n_meta_model_X = self._n_estimators * 2\n",
    "        if n_current_meta_model_X < n_meta_model_X:\n",
    "            n_diff = n_meta_model_X - n_current_meta_model_X\n",
    "            fillers = np.zeros((meta_model_X.shape[0], n_diff))\n",
    "            meta_model_X = np.hstack((meta_model_X, fillers)) \n",
    "        \n",
    "        return self._meta_learner.predict(meta_model_X)\n",
    "        \n",
    "        \n",
    "    \n",
    "#         print(\"NOW\")\n",
    "#         print(meta_importance)\n",
    "        \n",
    "# #         print(meta_importance)\n",
    "#         if self._ensemble_size > 0:\n",
    "#             for i in range(self._ensemble_size):\n",
    "#                 f_idx = i * 2\n",
    "#                 new_measure = meta_importance[f_idx] + meta_importance[f_idx+1]\n",
    "#                 current_round = self._ensemble_matrix[1][i]\n",
    "                \n",
    "#                 # TODO -> CHECK THIS CURRENT ROUND THING \n",
    "#                 self._ensemble_matrix[2][i][current_round - 1 % self._n_estimators] = new_measure\n",
    "\n",
    "#                 print(new_measure)\n",
    "    \n",
    "#         print(\"TRAIN\")\n",
    "#         print(self._ensemble_matrix[1])\n",
    "#         print(self._ensemble_matrix[2])\n",
    "\n",
    "\n",
    "\n",
    "incremental_xgb = xgb.XGBClassifier()\n",
    "adpBoost = AdaptiveStackedBoostClassifier()\n",
    "\n",
    "for ts in np.arange(train_data[\"ts\"].min(), train_data[\"ts\"].max()):\n",
    "#     print(\"Start\")\n",
    "    train_set = train_data[train_data[\"ts\"] == ts]\n",
    "    train_set_X = train_set.iloc[:,:-1]\n",
    "    train_set_y = train_set[\"class\"]      \n",
    "        \n",
    "    test_set = train_data[train_data[\"ts\"] == ts + 1]\n",
    "    test_set_X = test_set.iloc[:,:-1].values\n",
    "    test_set_y = test_set[\"class\"].values\n",
    "   \n",
    "    adpBoost.partial_fit(train_set_X.values, train_set_y.values)\n",
    "    \n",
    "#     y_pred = adpBoost.predict(test_set_X)\n",
    "#     evaluation = f1_score(test_set_y, y_pred, average='binary')\n",
    "#     print(\"Proposed TS {}: {}\".format(ts+1, evaluation))\n",
    "            \n",
    "#     if ts == 1:\n",
    "#         incremental_xgb.fit(train_set_X.values, train_set_y.values)\n",
    "#     else:\n",
    "#         booster = incremental_xgb.get_booster()\n",
    "#         incremental_xgb.fit(train_set_X.values, train_set_y.values, xgb_model=booster)\n",
    "        \n",
    "#     y_pred = incremental_xgb.predict(test_set_X)\n",
    "#     evaluation = f1_score(test_set_y, y_pred, average='binary')\n",
    "#     print(\"Incremental TS {}: {}\".format(ts+1, evaluation))\n",
    "    \n",
    "    \n",
    "# train_data_tmp = train_data[train_data[\"ts\"] <= 34]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('btc-classifier': conda)",
   "language": "python",
   "name": "python37664bitbtcclassifiercondaf328939486114fc0aeb107830f867d66"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
