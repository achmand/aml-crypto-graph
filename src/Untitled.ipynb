{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import cryptoaml.datareader as cdr\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from skmultiflow.drift_detection import DDM\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3276, 42)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1405, 42)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "elliptic = cdr.get_data(\"eth_accounts\")\n",
    "\n",
    "\n",
    "# elliptic.dataset_.shape\n",
    "\n",
    "data = elliptic.train_test_split(train_size=0.7)\n",
    "\n",
    "\n",
    "display(data[\"ALL\"].train_X.shape)\n",
    "display(data[\"ALL\"].test_X.shape)\n",
    "\n",
    "# train_data = data.train_X\n",
    "\n",
    "# display(train_data)\n",
    "# train_data[\"class\"] = data.train_y\n",
    "# test_data = data.test_X\n",
    "# test_data[\"class\"] = data.test_y \n",
    "# # data = train_data.append(test_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from skmultiflow.core.base import BaseSKMObject, ClassifierMixin\n",
    "from skmultiflow.drift_detection import ADWIN\n",
    "from skmultiflow.utils import get_dimensions\n",
    "\n",
    "\n",
    "class AdaptiveXGBoostClassifier(BaseSKMObject, ClassifierMixin):\n",
    "    _PUSH_STRATEGY = 'push'\n",
    "    _REPLACE_STRATEGY = 'replace'\n",
    "    _UPDATE_STRATEGIES = [_PUSH_STRATEGY, _REPLACE_STRATEGY]\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_estimators=30,\n",
    "                 learning_rate=0.3,\n",
    "                 max_depth=6,\n",
    "                 max_window_size=1000,\n",
    "                 min_window_size=None,\n",
    "                 detect_drift=False,\n",
    "                 update_strategy='replace'):\n",
    "        \"\"\"\n",
    "        Adaptive XGBoost classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators: int (default=5)\n",
    "            The number of estimators in the ensemble.\n",
    "\n",
    "        learning_rate:\n",
    "            Learning rate, a.k.a eta.\n",
    "\n",
    "        max_depth: int (default = 6)\n",
    "            Max tree depth.\n",
    "\n",
    "        max_window_size: int (default=1000)\n",
    "            Max window size.\n",
    "\n",
    "        min_window_size: int (default=None)\n",
    "            Min window size. If this parameters is not set, then a fixed size\n",
    "            window of size ``max_window_size`` will be used.\n",
    "\n",
    "        detect_drift: bool (default=False)\n",
    "            If set will use a drift detector (ADWIN).\n",
    "\n",
    "        update_strategy: str (default='replace')\n",
    "            | The update strategy to use:\n",
    "            | 'push' - the ensemble resembles a queue\n",
    "            | 'replace' - oldest ensemble members are replaced by newer ones\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The Adaptive XGBoost [1]_ (AXGB) classifier is an adaptation of the\n",
    "        XGBoost algorithm for evolving data streams. AXGB creates new members\n",
    "        of the ensemble from mini-batches of data as new data becomes\n",
    "        available.  The maximum ensemble  size is fixed, but learning does not\n",
    "        stop once this size is reached, the ensemble is updated on new data to\n",
    "        ensure consistency with the current data distribution.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Montiel, Jacob, Mitchell, Rory, Frank, Eibe, Pfahringer,\n",
    "           Bernhard, Abdessalem, Talel, and Bifet, Albert. “AdaptiveXGBoost for\n",
    "           Evolving Data Streams”. In:IJCNN’20. International Joint Conference\n",
    "           on Neural Networks. 2020. Forthcoming.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_window_size = max_window_size\n",
    "        self.min_window_size = min_window_size\n",
    "        self._first_run = True\n",
    "        self._ensemble = None\n",
    "        self.detect_drift = detect_drift\n",
    "        self._drift_detector = None\n",
    "        self._X_buffer = np.array([])\n",
    "        self._y_buffer = np.array([])\n",
    "        self._samples_seen = 0\n",
    "        self._model_idx = 0\n",
    "        if update_strategy not in self._UPDATE_STRATEGIES:\n",
    "            raise AttributeError(\"Invalid update_strategy: {}\\n\"\n",
    "                                 \"Valid options: {}\".format(update_strategy,\n",
    "                                                            self._UPDATE_STRATEGIES))\n",
    "        self.update_strategy = update_strategy\n",
    "        self._configure()\n",
    "\n",
    "    def _configure(self):\n",
    "        if self.update_strategy == self._PUSH_STRATEGY:\n",
    "            self._ensemble = []\n",
    "        elif self.update_strategy == self._REPLACE_STRATEGY:\n",
    "            self._ensemble = [None] * self.n_estimators\n",
    "        self._reset_window_size()\n",
    "        self._init_margin = 0.0\n",
    "        self._boosting_params = {\"silent\": True,\n",
    "                                 \"objective\": \"binary:logistic\",\n",
    "                                 \"eta\": self.learning_rate,\n",
    "                                 \"max_depth\": self.max_depth}\n",
    "        if self.detect_drift:\n",
    "            self._drift_detector = ADWIN()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the estimator.\n",
    "        \"\"\"\n",
    "        self._first_run = True\n",
    "        self._configure()\n",
    "\n",
    "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Partially (incrementally) fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            An array of shape (n_samples, n_features) with the data upon which\n",
    "            the algorithm will create its model.\n",
    "\n",
    "        y: Array-like\n",
    "            An array of shape (, n_samples) containing the classification\n",
    "            targets for all samples in X. Only binary data is supported.\n",
    "\n",
    "        classes: Not used.\n",
    "\n",
    "        sample_weight: Not used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        AdaptiveXGBoostClassifier\n",
    "            self\n",
    "        \"\"\"\n",
    "        for i in range(X.shape[0]):\n",
    "            self._partial_fit(np.array([X[i, :]]), np.array([y[i]]))\n",
    "        return self\n",
    "\n",
    "    def _partial_fit(self, X, y):\n",
    "        if self._first_run:\n",
    "            self._X_buffer = np.array([]).reshape(0, get_dimensions(X)[1])\n",
    "            self._y_buffer = np.array([])\n",
    "            self._first_run = False\n",
    "        self._X_buffer = np.concatenate((self._X_buffer, X))\n",
    "        self._y_buffer = np.concatenate((self._y_buffer, y))\n",
    "        while self._X_buffer.shape[0] >= self.window_size:\n",
    "            self._train_on_mini_batch(X=self._X_buffer[0:self.window_size, :],\n",
    "                                      y=self._y_buffer[0:self.window_size])\n",
    "            delete_idx = [i for i in range(self.window_size)]\n",
    "            self._X_buffer = np.delete(self._X_buffer, delete_idx, axis=0)\n",
    "            self._y_buffer = np.delete(self._y_buffer, delete_idx, axis=0)\n",
    "\n",
    "            # Check window size and adjust it if necessary\n",
    "            self._adjust_window_size()\n",
    "\n",
    "        # Support for concept drift\n",
    "        if self.detect_drift:\n",
    "            correctly_classifies = self.predict(X) == y\n",
    "            # Check for warning\n",
    "            self._drift_detector.add_element(int(not correctly_classifies))\n",
    "            # Check if there was a change\n",
    "            if self._drift_detector.detected_change():\n",
    "                # Reset window size\n",
    "                self._reset_window_size()\n",
    "                if self.update_strategy == self._REPLACE_STRATEGY:\n",
    "                    self._model_idx = 0\n",
    "\n",
    "    def _adjust_window_size(self):\n",
    "        if self._dynamic_window_size < self.max_window_size:\n",
    "            self._dynamic_window_size *= 2\n",
    "            if self._dynamic_window_size > self.max_window_size:\n",
    "                self.window_size = self.max_window_size\n",
    "            else:\n",
    "                self.window_size = self._dynamic_window_size\n",
    "\n",
    "    def _reset_window_size(self):\n",
    "        if self.min_window_size:\n",
    "            self._dynamic_window_size = self.min_window_size\n",
    "        else:\n",
    "            self._dynamic_window_size = self.max_window_size\n",
    "        self.window_size = self._dynamic_window_size\n",
    "\n",
    "    def _train_on_mini_batch(self, X, y):\n",
    "        if self.update_strategy == self._REPLACE_STRATEGY:\n",
    "            booster = self._train_booster(X, y, self._model_idx)\n",
    "            # Update ensemble\n",
    "            self._ensemble[self._model_idx] = booster\n",
    "            self._samples_seen += X.shape[0]\n",
    "            self._update_model_idx()\n",
    "        else:   # self.update_strategy == self._PUSH_STRATEGY\n",
    "            booster = self._train_booster(X, y, len(self._ensemble))\n",
    "            # Update ensemble\n",
    "            if len(self._ensemble) == self.n_estimators:\n",
    "                self._ensemble.pop(0)\n",
    "            self._ensemble.append(booster)\n",
    "            self._samples_seen += X.shape[0]\n",
    "\n",
    "    def _train_booster(self, X: np.ndarray, y: np.ndarray, last_model_idx: int):\n",
    "        d_mini_batch_train = xgb.DMatrix(X, y.astype(int))\n",
    "        # Get margins from trees in the ensemble\n",
    "        margins = np.asarray([self._init_margin] * d_mini_batch_train.num_row())\n",
    "        for j in range(last_model_idx):\n",
    "            margins = np.add(margins,\n",
    "                             self._ensemble[j].predict(d_mini_batch_train, output_margin=True))\n",
    "        d_mini_batch_train.set_base_margin(margin=margins)\n",
    "        booster = xgb.train(params=self._boosting_params,\n",
    "                            dtrain=d_mini_batch_train,\n",
    "                            num_boost_round=1,\n",
    "                            verbose_eval=False)\n",
    "        return booster\n",
    "\n",
    "    def _update_model_idx(self):\n",
    "        self._model_idx += 1\n",
    "        if self._model_idx == self.n_estimators:\n",
    "            self._model_idx = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class label for sample X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            An array of shape (n_samples, n_features) with the samples to\n",
    "            predict the class label for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            A 1D array of shape (, n_samples), containing the\n",
    "            predicted class labels for all instances in X.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._ensemble:\n",
    "            if self.update_strategy == self._REPLACE_STRATEGY:\n",
    "                trees_in_ensemble = sum(i is not None for i in self._ensemble)\n",
    "            else:   # self.update_strategy == self._PUSH_STRATEGY\n",
    "                trees_in_ensemble = len(self._ensemble)\n",
    "            if trees_in_ensemble > 0:\n",
    "                d_test = xgb.DMatrix(X)\n",
    "                for i in range(trees_in_ensemble - 1):\n",
    "                    margins = self._ensemble[i].predict(d_test, output_margin=True)\n",
    "                    d_test.set_base_margin(margin=margins)\n",
    "                predicted = self._ensemble[trees_in_ensemble - 1].predict(d_test)\n",
    "                return np.array(predicted > 0.5).astype(int)\n",
    "        # Ensemble is empty, return default values (0)\n",
    "        return np.zeros(get_dimensions(X)[0])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Not implemented for this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"predict_proba is not implemented for this method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46564, 167)\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.data import ConceptDriftStream\n",
    "from skmultiflow.evaluation import EvaluatePrequential\n",
    "\n",
    "\n",
    "# Adaptive XGBoost classifier parameters\n",
    "n_estimators = 30       # Number of members in the ensemble\n",
    "learning_rate = 0.3     # Learning rate or eta\n",
    "max_depth = 6           # Max depth for each tree in the ensemble\n",
    "max_window_size = 1000  # Max window size\n",
    "min_window_size = 1     # set to activate the dynamic window strategy\n",
    "detect_drift = False    # Enable/disable drift detection\n",
    "\n",
    "AXGBp = AdaptiveXGBoostClassifier(update_strategy='push',\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  max_depth=max_depth,\n",
    "                                  max_window_size=max_window_size,\n",
    "                                  min_window_size=min_window_size,\n",
    "                                  detect_drift=detect_drift)\n",
    "\n",
    "AXGBr = AdaptiveXGBoostClassifier(update_strategy='replace',\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  max_depth=max_depth,\n",
    "                                  max_window_size=max_window_size,\n",
    "                                  min_window_size=min_window_size,\n",
    "                                  detect_drift=detect_drift)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803\n",
      "Timestep: 35 | F1-Score: 0.964\n",
      "Timestep: 36 | F1-Score: 0.892\n",
      "Timestep: 37 | F1-Score: 0.769\n",
      "Timestep: 38 | F1-Score: 0.935\n",
      "Timestep: 39 | F1-Score: 0.949\n",
      "Timestep: 40 | F1-Score: 0.772\n",
      "Timestep: 41 | F1-Score: 0.947\n",
      "Timestep: 42 | F1-Score: 0.862\n",
      "Timestep: 43 | F1-Score: 0.0\n",
      "Timestep: 44 | F1-Score: 0.048\n",
      "Timestep: 45 | F1-Score: 0.0\n",
      "Timestep: 46 | F1-Score: 0.143\n",
      "Timestep: 47 | F1-Score: 0.0\n",
      "Timestep: 48 | F1-Score: 0.053\n",
      "Timestep: 49 | F1-Score: 0.032\n",
      "F1-Score on test set: 0.803\n",
      "Recall on test set: 0.723\n",
      "Precision on test set: 0.902\n",
      "Confusion_matrix: [[15502    85]\n",
      " [  300   783]]\n"
     ]
    }
   ],
   "source": [
    "# display(train_data)\n",
    "# display(test_data)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(train_data.iloc[:,:-1], train_data[\"class\"])\n",
    "\n",
    "y_pred = model.predict(test_data.iloc[:,:-1])\n",
    "ts_f1 = round(f1_score(test_data[\"class\"], y_pred, average='binary'), 3)\n",
    "print(ts_f1)\n",
    "\n",
    "true_test = []\n",
    "f1_scores = []\n",
    "predictions_test = []\n",
    "timestep_range = np.arange(35, 50)\n",
    "for timestep in timestep_range:\n",
    "    test = test_data[test_data[\"ts\"] == timestep]\n",
    "    test_X = test.iloc[:,:-1]\n",
    "    test_y = test[\"class\"]\n",
    "    \n",
    "    y_pred = model.predict(test_X)\n",
    "    ts_f1 = round(f1_score(test_y, y_pred, average='binary'), 3)\n",
    "    print(\"Timestep: {} | F1-Score: {}\".format(timestep, ts_f1))\n",
    "    f1_scores.append(ts_f1)\n",
    "    true_test.append(test_y)\n",
    "    predictions_test.append(y_pred)\n",
    "        \n",
    "f1_score_test = f1_score(np.concatenate(true_test, axis=0),   \n",
    "                         np.concatenate(predictions_test, axis=0), \n",
    "                         average='binary')\n",
    "print(\"F1-Score on test set: {}\".format(round(f1_score_test, 3)))     \n",
    "\n",
    "recall_score_test = recall_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 average='binary')\n",
    "print(\"Recall on test set: {}\".format(round(recall_score_test, 3)))      \n",
    "\n",
    "precision_score_test = precision_score(np.concatenate(true_test, axis=0),   \n",
    "                                       np.concatenate(predictions_test, axis=0), \n",
    "                                       average='binary')\n",
    "print(\"Precision on test set: {}\".format(round(precision_score_test, 3)))    \n",
    "\n",
    "confusion_matrix_test = confusion_matrix(np.concatenate(true_test, axis=0), \n",
    "                                         np.concatenate(predictions_test, axis=0))\n",
    "print(\"Confusion_matrix: {}\".format(confusion_matrix_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 35 | F1-Score: 0.964\n",
      "Timestep: 36 | F1-Score: 1.0\n",
      "--------------------\n",
      "--------------------\n",
      "Timestep: 37 | F1-Score: 0.824\n",
      "--------------------\n",
      "0.75\n",
      "--------------------\n",
      "Timestep: 38 | F1-Score: 0.894\n",
      "--------------------\n",
      "0.902\n",
      "0.821\n",
      "--------------------\n",
      "Timestep: 39 | F1-Score: 0.91\n",
      "--------------------\n",
      "0.908\n",
      "0.893\n",
      "0.93\n",
      "--------------------\n",
      "Timestep: 40 | F1-Score: 0.718\n",
      "--------------------\n",
      "0.712\n",
      "0.712\n",
      "0.711\n",
      "0.753\n",
      "--------------------\n",
      "Timestep: 41 | F1-Score: 0.917\n",
      "--------------------\n",
      "0.921\n",
      "0.943\n",
      "0.918\n",
      "0.934\n",
      "0.946\n",
      "--------------------\n",
      "Timestep: 42 | F1-Score: 0.844\n",
      "--------------------\n",
      "0.855\n",
      "0.85\n",
      "0.85\n",
      "0.862\n",
      "0.849\n",
      "0.874\n",
      "--------------------\n",
      "Timestep: 43 | F1-Score: 0.174\n",
      "--------------------\n",
      "0.31\n",
      "0.292\n",
      "0.14\n",
      "0.178\n",
      "0.208\n",
      "0.186\n",
      "0.054\n",
      "--------------------\n",
      "Timestep: 44 | F1-Score: 0.185\n",
      "--------------------\n",
      "0.348\n",
      "0.4\n",
      "0.294\n",
      "0.316\n",
      "0.125\n",
      "0.207\n",
      "0.148\n",
      "0.286\n",
      "--------------------\n",
      "Timestep: 45 | F1-Score: 0.0\n",
      "--------------------\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "--------------------\n",
      "Timestep: 46 | F1-Score: 0.5\n",
      "--------------------\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.286\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "--------------------\n",
      "Timestep: 47 | F1-Score: 0.118\n",
      "--------------------\n",
      "0.556\n",
      "0.229\n",
      "0.19\n",
      "0.25\n",
      "0.4\n",
      "0.37\n",
      "0.5\n",
      "0.4\n",
      "0.08\n",
      "0.0\n",
      "0.0\n",
      "--------------------\n",
      "Timestep: 48 | F1-Score: 0.382\n",
      "--------------------\n",
      "0.441\n",
      "0.531\n",
      "0.225\n",
      "0.4\n",
      "0.351\n",
      "0.291\n",
      "0.431\n",
      "0.367\n",
      "0.292\n",
      "0.261\n",
      "0.15\n",
      "0.279\n",
      "--------------------\n",
      "Timestep: 49 | F1-Score: 0.875\n",
      "--------------------\n",
      "0.667\n",
      "0.635\n",
      "0.475\n",
      "0.588\n",
      "0.743\n",
      "0.753\n",
      "0.696\n",
      "0.212\n",
      "0.067\n",
      "0.585\n",
      "0.194\n",
      "0.423\n",
      "0.0\n",
      "--------------------\n",
      "F1-Score on test set: 0.806\n",
      "Recall on test set: 0.788\n",
      "Precision on test set: 0.825\n",
      "Confusion_matrix: [[15406   181]\n",
      " [  230   853]]\n"
     ]
    }
   ],
   "source": [
    "# display(train_data)\n",
    "# display(test_data)\n",
    "\n",
    "# ncr_X, ncr_y = ncr.fit_resample(train_data.iloc[:,:-1], train_data[\"class\"])\n",
    "# sm = SMOTE()\n",
    "# X, y = sm.fit_resample(ncr_X, ncr_y)\n",
    "\n",
    "# ncr = NeighbourhoodCleaningRule(n_neighbors=3, threshold_cleaning=0.5)\n",
    "# ncr_X, ncr_y = ncr.fit_resample(train_data.iloc[:,:-1], train_data[\"class\"])\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(train_data.iloc[:,:-1], train_data[\"class\"])\n",
    "\n",
    "# model.fit(train_data.iloc[:,:-1], train_data[\"class\"])\n",
    "\n",
    "max_timestep = test_data[\"ts\"].max()\n",
    "true_test = []\n",
    "predictions_test = []\n",
    "timestep_range = np.arange(35, max_timestep)\n",
    "\n",
    "ensembles = []\n",
    "\n",
    "for timestep in timestep_range:\n",
    "\n",
    "    train = test_data[test_data[\"ts\"] == timestep]\n",
    "    train_X = train.iloc[:,:-1]\n",
    "    train_y = train[\"class\"]\n",
    "    \n",
    "    if timestep == 35:\n",
    "        y_pred = model.predict(train_X)\n",
    "        ts_f1 = round(f1_score(train_y, y_pred, average='binary'), 3)     \n",
    "        print(\"Timestep: {} | F1-Score: {}\".format(timestep, ts_f1))\n",
    "        true_test.append(train_y)\n",
    "        predictions_test.append(y_pred)\n",
    "     \n",
    "    #ncr_X, ncr_y = ncr.fit_resample(train_X, train_y)\n",
    "    booster = model.get_booster()\n",
    "    model.fit(train_X, train_y, xgb_model=booster)\n",
    "       \n",
    "    test = test_data[test_data[\"ts\"] == timestep + 1]\n",
    "    test_X = test.iloc[:,:-1]\n",
    "    test_y = test[\"class\"]\n",
    "    \n",
    "    y_pred = model.predict(test_X)\n",
    "    ts_f1 = round(f1_score(test_y, y_pred, average='binary'), 3)\n",
    "    print(\"Timestep: {} | F1-Score: {}\".format(timestep + 1, ts_f1))\n",
    "    true_test.append(test_y)\n",
    "    predictions_test.append(y_pred)\n",
    "        \n",
    "    tmp_model = xgb.XGBClassifier()\n",
    "    tmp_model.fit(train_X, train_y)\n",
    "    ensembles.append(tmp_model) \n",
    "    \n",
    "    print(\"--------------------\")\n",
    "    for i in range(len(ensembles) - 1):\n",
    "        tmp = ensembles[i]\n",
    "        tmp_booster = tmp.get_booster()\n",
    "        tmp.fit(train_X, train_y, xgb_model=tmp_booster)\n",
    "        ensembles[i] = tmp \n",
    "        y_pred = tmp.predict(test_X)\n",
    "        f1_tmp = round(f1_score(test_y, y_pred, average='binary'), 3)\n",
    "        print(f1_tmp)\n",
    "    print(\"--------------------\")\n",
    "\n",
    "    \n",
    "#     y_pred = tmp_model.predict(test_X)\n",
    "#     ts_f1_tmp = round(f1_score(test_y, y_pred, average='binary'), 3)\n",
    "    \n",
    "#     if ts_f1_tmp > ts_f1:\n",
    "#         print(\"TMP_Timestep: {} | F1-Score: {}\".format(timestep + 1, ts_f1_tmp))\n",
    "\n",
    "    \n",
    "    \n",
    "f1_score_test = f1_score(np.concatenate(true_test, axis=0),   \n",
    "                         np.concatenate(predictions_test, axis=0), \n",
    "                         average='binary')\n",
    "print(\"F1-Score on test set: {}\".format(round(f1_score_test, 3)))     \n",
    "\n",
    "recall_score_test = recall_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 average='binary')\n",
    "print(\"Recall on test set: {}\".format(round(recall_score_test, 3)))      \n",
    "\n",
    "precision_score_test = precision_score(np.concatenate(true_test, axis=0),   \n",
    "                                       np.concatenate(predictions_test, axis=0), \n",
    "                                       average='binary')\n",
    "print(\"Precision on test set: {}\".format(round(precision_score_test, 3)))    \n",
    "\n",
    "confusion_matrix_test = confusion_matrix(np.concatenate(true_test, axis=0), \n",
    "                                         np.concatenate(predictions_test, axis=0))\n",
    "print(\"Confusion_matrix: {}\".format(confusion_matrix_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('btc-classifier': conda)",
   "language": "python",
   "name": "python37664bitbtcclassifiercondaf328939486114fc0aeb107830f867d66"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
