{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/delinvas/anaconda3/envs/btc-classifier/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "###### importing dependencies #############################################\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "import cryptoaml.datareader as cdr\n",
    "from collections import OrderedDict\n",
    "from cryptoaml.metrics import results_table, plot_confusion_matrix, plot_time_indexed_results\n",
    "from cryptoaml.models import RandomForestAlgo, XgboostAlgo, LightGbmAlgo, CatBoostAlgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models without tuning \n",
    "elliptic = cdr.get_data(\"elliptic\")\n",
    "elliptic_sets = elliptic.train_test_split(train_size=0.7, \n",
    "                                          feat_set=[\"LF\",\"LF_NE\"], \n",
    "                                          inc_meta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models with default parameters  \n",
    "models_default = OrderedDict()\n",
    "\n",
    "rf_default = RandomForestAlgo(n_jobs=-1, n_estimators=50, max_features=50)\n",
    "# models_default[rf_default.model_name_] = rf_default\n",
    "\n",
    "# Using the default values for XGBoost Classifier will obtain reproducable results \n",
    "# => 'gblinear' booster with shotgun updater is nondeterministic as it uses Hogwild algorithm [Default='gbtree']\n",
    "# =>  parameters such as subsample and colsample_by_* are set to 1, meaning no random sampling will be used \n",
    "xgb_default = XgboostAlgo(n_jobs=-1)\n",
    "# models_default[xgb_default.model_name_] = xgb_default\n",
    "\n",
    "light_default = LightGbmAlgo(n_jobs=-1)\n",
    "models_default[light_default.model_name_] = light_default\n",
    "\n",
    "cat_default = CatBoostAlgo(thread_count=-1, verbose=False)\n",
    "# models_default[cat_default.model_name_] = cat_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# light_default.fit(elliptic_sets.train_X, elliptic_sets.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_default.fit(elliptic_sets.train_X, elliptic_sets.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_default.fit(elliptic_sets.train_X, elliptic_sets.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_default.fit(elliptic_sets.train_X, elliptic_sets.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lf_split = elliptic_sets\n",
    "# # display(lf_split.test_X)\n",
    "# # display(lf_split.test_y)\n",
    "# mergedDf = lf_split.test_X.merge(lf_split.test_y, left_index=True, right_index=True)\n",
    "\n",
    "# # display(mergedDf)\n",
    "\n",
    "# gb = mergedDf.groupby(\"ts\")    \n",
    "\n",
    "# time_indexed = [gb.get_group(x) for x in gb.groups]\n",
    "# time_indexed[1]\n",
    "# len(time_indexed)\n",
    "\n",
    "\n",
    "# time_steps = [] \n",
    "# total_samples = []\n",
    "# scores_random = []\n",
    "# scores_light = []\n",
    "# scores_xgb = []\n",
    "# scores_cat = []\n",
    "\n",
    "\n",
    "# for ts in time_indexed:\n",
    "#     tmp_x = ts.loc[:, ts.columns != \"class\"]\n",
    "#     tmp_y = ts[\"class\"]\n",
    "# #     display(tmp_x)\n",
    "# #     display(tmp_y)\n",
    "   \n",
    "#     f1 = light_default.evaluate(metrics=\"f1\", X=tmp_x, y=tmp_y)\n",
    "#     scores_light.append(f1[\"f1\"])    \n",
    "#     print(\"LIGHT: \" + str(f1[\"f1\"]))\n",
    "    \n",
    "#     f1 = xgb_default.evaluate(metrics=\"f1\", X=tmp_x, y=tmp_y)\n",
    "#     scores_xgb.append(f1[\"f1\"])\n",
    "#     print(\"XGB: \" + str(f1[\"f1\"]))\n",
    "\n",
    "#     f1 = rf_default.evaluate(metrics=\"f1\", X=tmp_x, y=tmp_y)\n",
    "#     scores_random.append(f1[\"f1\"])\n",
    "#     print(\"RF: \" + str(f1[\"f1\"]))\n",
    "\n",
    "    \n",
    "#     f1 = cat_default.evaluate(metrics=\"f1\", X=tmp_x, y=tmp_y)\n",
    "#     scores_cat.append(f1[\"f1\"])\n",
    "#     print(\"CAT: \" + str(f1[\"f1\"]))\n",
    "        \n",
    "#     time_steps.append(ts[\"ts\"].unique()[0])\n",
    "#     total_pos_label = ts[ts[\"class\"] == 1].shape[0]\n",
    "#     total_samples.append(total_pos_label)\n",
    "\n",
    "\n",
    "# scores = [(\"LightGBM\", scores_light), (\"XGB\", scores_xgb), (\"RF\", scores_random), (\"Cat\", scores_cat)]\n",
    "\n",
    "# # ax = sns.barplot(x=)\n",
    "\n",
    "# print(scores)\n",
    "\n",
    "# # print(time_steps)\n",
    "# # print(total_samples)\n",
    "\n",
    "# # import matplotlib.pyplot as plt\n",
    "\n",
    "# plot_time_indexed_results(time_steps, \n",
    "#                           total_samples,\n",
    "#                           scores,\n",
    "#                           metric_title=\"Illicit F1\")\n",
    "\n",
    "\n",
    "# # import matplotlib.pyplot as plt\n",
    "# # import seaborn as sns\n",
    "# # import numpy as np\n",
    "# # import pandas as pd\n",
    "\n",
    "# # fig = plt.figure()|\n",
    "# # ax1 = fig.add_subplot(111)\n",
    "# # ax1.plot(pd.Series(np.random.uniform(0,1,size=10)), color='g')\n",
    "# # ax2 = ax1.twinx()\n",
    "# # ax2.plot(pd.Series(np.random.uniform(0,17,size=10)), color='r')\n",
    "# # ax2.grid(False)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################\n",
      "Elliptic Dataset - Feature Set [light_boost] \n",
      "######################################################\n",
      "- Training & Extracting Results - Feature Set [LF]\n",
      "OrderedDict([('precision', 0.8612975391498882),\n",
      "             ('recall', 0.7109879963065558),\n",
      "             ('f1', 0.7789580171977744),\n",
      "             ('f1_micro', 0.9737852429514097),\n",
      "             ('confusion', array([[15463,   124],\n",
      "       [  313,   770]]))])\n",
      "{'boosting_type': 'gbdt',\n",
      " 'class_weight': None,\n",
      " 'colsample_bytree': 1.0,\n",
      " 'importance_type': 'split',\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': -1,\n",
      " 'min_child_samples': 20,\n",
      " 'min_child_weight': 0.001,\n",
      " 'min_split_gain': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': -1,\n",
      " 'num_leaves': 31,\n",
      " 'objective': None,\n",
      " 'random_state': None,\n",
      " 'reg_alpha': 0.0,\n",
      " 'reg_lambda': 0.0,\n",
      " 'silent': True,\n",
      " 'subsample': 1.0,\n",
      " 'subsample_for_bin': 200000,\n",
      " 'subsample_freq': 0}\n",
      "- Training & Extracting Results - Feature Set [LF_NE]\n",
      "OrderedDict([('precision', 0.9813333333333333),\n",
      "             ('recall', 0.6795937211449676),\n",
      "             ('f1', 0.8030551009274413),\n",
      "             ('f1_micro', 0.9783443311337733),\n",
      "             ('confusion', array([[15573,    14],\n",
      "       [  347,   736]]))])\n",
      "{'boosting_type': 'gbdt',\n",
      " 'class_weight': None,\n",
      " 'colsample_bytree': 1.0,\n",
      " 'importance_type': 'split',\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': -1,\n",
      " 'min_child_samples': 20,\n",
      " 'min_child_weight': 0.001,\n",
      " 'min_split_gain': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': -1,\n",
      " 'num_leaves': 31,\n",
      " 'objective': None,\n",
      " 'random_state': None,\n",
      " 'reg_alpha': 0.0,\n",
      " 'reg_lambda': 0.0,\n",
      " 'silent': True,\n",
      " 'subsample': 1.0,\n",
      " 'subsample_for_bin': 200000,\n",
      " 'subsample_freq': 0}\n"
     ]
    }
   ],
   "source": [
    "# extract results for elliptic dataset on different feature sets\n",
    "\n",
    "results = OrderedDict()\n",
    "results[\"elliptic\"] = OrderedDict()\n",
    "metrics=[\"precision\", \"recall\", \"f1\", \"f1_micro\", \"confusion\"]\n",
    "\n",
    "#extracting dataset results \n",
    "for model_key, model in models_default.items():\n",
    "    print(\"\\n######################################################\")\n",
    "    print(\"Elliptic Dataset - Feature Set [{0}] \".format(model_key))\n",
    "    print(\"######################################################\")\n",
    "    results[\"elliptic\"][model_key] = OrderedDict()\n",
    "        \n",
    "    for feature_set, feature_set_data in elliptic_sets.items():\n",
    "        print(\"- Training & Extracting Results - Feature Set [{}]\".format(feature_set))\n",
    "\n",
    "        if feature_set not in results[\"elliptic\"][model_key]:\n",
    "            results[\"elliptic\"][model_key][feature_set] = OrderedDict()\n",
    "\n",
    "        # train model with default parameters\n",
    "        tmp_train_X = feature_set_data.train_X\n",
    "        tmp_train_y = feature_set_data.train_y \n",
    "        model.fit(tmp_train_X, tmp_train_y)\n",
    "\n",
    "        # extract results \n",
    "        tmp_test_X = feature_set_data.test_X\n",
    "        tmp_test_y = feature_set_data.test_y\n",
    "        tmp_result = model.evaluate(metrics=metrics,\n",
    "                                    X=tmp_test_X, \n",
    "                                    y=tmp_test_y)\n",
    "\n",
    "\n",
    "        print(pprint.pformat(tmp_result))\n",
    "        print(pprint.pformat(model.get_params()))\n",
    "        results[\"elliptic\"][model_key][feature_set] = tmp_result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF \n",
    "# 0.7801920161697827\n",
    "# 0.8146214099216711\n",
    "\n",
    "# 0.7828715365239295\n",
    "# 0.8103626943005181\n",
    "\n",
    "# AF_NE\n",
    "# 0.8217299578059071\n",
    "\n",
    "# XGBoost \n",
    "# 0.7794871794871795\n",
    "# 0.7794871794871795\n",
    "\n",
    "# 0.8026652998462326\n",
    "# 0.8026652998462326\n",
    "\n",
    "# 0.7980241492864983\n",
    "\n",
    "\n",
    "# Light \n",
    "# 0.7789580171977744\n",
    "# 0.7789580171977744\n",
    "\n",
    "# 0.813929313929314\n",
    "# 0.813929313929314\n",
    "\n",
    "# 0.8063112078346028\n",
    "\n",
    "# \n",
    "\n",
    "# CAT \n",
    "# 0.7892004153686397\n",
    "# 0.7892004153686397\n",
    "# 0.7892004153686397\n",
    "\n",
    "# 0.8187565858798735\n",
    "# 0.8187565858798735\n",
    "# 0.8187565858798735\n",
    "\n",
    "# 0.8058887677208288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results \n",
    "# print(results)\n",
    "# models = [x[0] for x in results[\"elliptic\"][\"LF\"].items()]\n",
    "# f1_scores = [x[\"f1\"] for x in results[\"elliptic\"][\"LF\"].values()]\n",
    "\n",
    "# print(f1_scores)\n",
    "# ax = sns.barplot(x=models, y=f1_scores)\n",
    "# ax.set(ylim=(0, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_scores = [x[\"f1\"] for x in results[\"elliptic\"][\"AF\"].values()]\n",
    "\n",
    "# print(f1_scores)\n",
    "# ax = sns.barplot(x=models, y=f1_scores)\n",
    "# ax.set(ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrices = []\n",
    "\n",
    "# for model, x in results[\"elliptic\"].items():\n",
    "#     for f, n in x.items():\n",
    "# #         confusion_matrices.append(n[\"confusion\"])\n",
    "#          confusion_matrices.append((model + \"_\" + f, n[\"confusion\"]))\n",
    "\n",
    "# print(confusion_matrices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# confusion_plt(confusion_matrices[0], \"test\")\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(17,15))\n",
    "# fig.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "# i = 1\n",
    "\n",
    "# for x in confusion_matrix:\n",
    "#     ax = fig.add_subplot(2, 2, i)\n",
    "#     sns.set(font_scale=1.4) \n",
    "#     confusion_plt(x, \"test\")\n",
    "#     i = i+1\n",
    "       \n",
    "        \n",
    "# plot_confusion_matrix(confusion_matrices, figsize=(17,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = elliptic_sets[\"AF\"]\n",
    "# display(split.test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:btc-classifier] *",
   "language": "python",
   "name": "conda-env-btc-classifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
