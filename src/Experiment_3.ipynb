{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import cryptoaml.datareader as cdr\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from skmultiflow.core.base import BaseSKMObject, ClassifierMixin\n",
    "from skmultiflow.drift_detection import ADWIN\n",
    "from skmultiflow.utils import get_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic = cdr.get_data(\"elliptic\")\n",
    "data = elliptic.train_test_split(train_size=0.7, \n",
    "                                 feat_set=\"AF\", \n",
    "                                 inc_meta=False,\n",
    "                                 inc_unknown=False)\n",
    "\n",
    "train_data = data.train_X\n",
    "train_data[\"class\"] = data.train_y\n",
    "test_data = data.test_X\n",
    "test_data[\"class\"] = data.test_y \n",
    "data = train_data.append(test_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed TS 2: 0.7142857142857143\n",
      "Proposed TS 3: 0.6666666666666665\n",
      "Proposed TS 4: 0.8524590163934426\n",
      "Proposed TS 5: 0.2105263157894737\n",
      "Proposed TS 6: 0.045454545454545456\n",
      "Proposed TS 7: 0.4714285714285715\n",
      "Proposed TS 8: 0.45714285714285713\n",
      "Proposed TS 9: 0.9444444444444444\n",
      "Proposed TS 10: 0.6666666666666666\n",
      "Proposed TS 11: 0.9392712550607287\n",
      "Proposed TS 12: 0.6666666666666666\n",
      "Proposed TS 13: 0.9513513513513514\n",
      "Proposed TS 14: 0.8478260869565216\n",
      "Proposed TS 15: 0.9605263157894738\n",
      "Proposed TS 16: 0.9571984435797667\n",
      "Proposed TS 17: 0.867579908675799\n",
      "Proposed TS 18: 0.8620689655172413\n",
      "Proposed TS 19: 0.8313253012048193\n",
      "Proposed TS 20: 0.8352941176470587\n",
      "Proposed TS 21: 0.8725490196078431\n",
      "Proposed TS 22: 0.8722741433021807\n",
      "Proposed TS 23: 0.6391752577319587\n",
      "Proposed TS 24: 0.45771144278606957\n",
      "Proposed TS 25: 0.9009009009009009\n",
      "Proposed TS 26: 0.7980769230769232\n",
      "Proposed TS 27: 0.9387755102040817\n",
      "Proposed TS 28: 0.9767441860465116\n",
      "Proposed TS 29: 0.8189509306260576\n",
      "Proposed TS 30: 0.9285714285714286\n",
      "Proposed TS 31: 0.8622222222222222\n",
      "Proposed TS 32: 0.9488636363636364\n",
      "Proposed TS 33: 0.782608695652174\n",
      "Proposed TS 34: 0.8395061728395061\n",
      "Proposed TS 35: 0.9007633587786259\n",
      "Proposed TS 36: 0.7654320987654321\n",
      "Proposed TS 37: 0.7761194029850746\n",
      "Proposed TS 38: 0.8799999999999999\n",
      "Proposed TS 39: 0.8695652173913043\n",
      "Proposed TS 40: 0.6862745098039216\n",
      "Proposed TS 41: 0.8365019011406843\n",
      "Proposed TS 42: 0.752808988764045\n",
      "Proposed TS 43: 0.038834951456310676\n",
      "Proposed TS 44: 0.13675213675213677\n",
      "Proposed TS 45: 0.0\n",
      "Proposed TS 46: 0.0\n",
      "Proposed TS 47: 0.1714285714285714\n",
      "Proposed TS 48: 0.05128205128205128\n",
      "Proposed TS 49: 0.8256880733944955\n",
      "F1-Score on test set: 0.718\n",
      "Recall on test set: 0.768\n",
      "Precision on test set: 0.674\n",
      "Confusion_matrix: [[15185   402]\n",
      " [  251   832]]\n"
     ]
    }
   ],
   "source": [
    "class AdaptiveStackedBoostClassifier():\n",
    "    def __init__(self,\n",
    "                 window_size=2000,\n",
    "                 n_base_models=5,\n",
    "                 n_rounds_eval_base_model=3,\n",
    "                 meta_learner_train_ratio=0.4,\n",
    "                 verbose=True):\n",
    "        \n",
    "        self._first_run = True\n",
    "        self._window_size = window_size\n",
    "        # validate 'n_base_models' \n",
    "        if n_base_models <= 1:\n",
    "            raise ValueError(\"'n_base_models' must be > 1\")\n",
    "        self._n_base_models = n_base_models\n",
    "        # validate 'n_rounds_eval_base_model' \n",
    "        if n_rounds_eval_base_model > n_base_models or n_rounds_eval_base_model <= 0:\n",
    "            raise ValueError(\"'n_rounds_eval_base_model' must be > 0 and <= to 'n_base_models'\")\n",
    "        self._n_rounds_eval_base_model = n_rounds_eval_base_model\n",
    "        self._meta_learner = xgb.XGBClassifier()\n",
    "        self.meta_learner_train_ratio = meta_learner_train_ratio\n",
    "        self._X_buffer = np.array([])\n",
    "        self._y_buffer = np.array([])\n",
    "\n",
    "        # 3*N matrix \n",
    "        # 1st row - base-level model\n",
    "        # 2nd row - evaluation rounds \n",
    "        self._base_models = [[None for x in range(n_base_models)] for y in range(3)]\n",
    "        \n",
    "    def partial_fit(self, X, y):\n",
    "        if self._first_run:\n",
    "            self._X_buffer = np.array([]).reshape(0, X.shape[1])\n",
    "            self._y_buffer = np.array([])\n",
    "            self._first_run = False\n",
    "                           \n",
    "        self._X_buffer = np.concatenate((self._X_buffer, X))\n",
    "        self._y_buffer = np.concatenate((self._y_buffer, y))\n",
    "        while self._X_buffer.shape[0] >= self._window_size:\n",
    "            self._train_on_mini_batch(X=self._X_buffer[0:self._window_size, :],\n",
    "                                      y=self._y_buffer[0:self._window_size])\n",
    "            delete_idx = [i for i in range(self._window_size)]\n",
    "            self._X_buffer = np.delete(self._X_buffer, delete_idx, axis=0)\n",
    "            self._y_buffer = np.delete(self._y_buffer, delete_idx, axis=0)\n",
    "    \n",
    "    def _train_new_base_model(self, X_base, y_base, X_meta, y_meta):\n",
    "        \n",
    "        # new base-level model  \n",
    "        new_base_model = xgb.XGBClassifier()\n",
    "        # first train the base model on the base-level training set \n",
    "        new_base_model.fit(X_base, y_base)\n",
    "        # then extract the predicted probabilities to be added as meta-level features\n",
    "        y_predicted = new_base_model.predict_proba(X_meta)   \n",
    "        # once the meta-features for this specific base-model are extracted,\n",
    "        # we incrementally fit this base-model to the rest of the data,\n",
    "        # this is done so this base-model is trained on a full batch \n",
    "        new_base_model.fit(X_meta, y_meta, xgb_model=new_base_model.get_booster())\n",
    "        return new_base_model, y_predicted\n",
    "    \n",
    "    def _construct_meta_features(self, meta_features):\n",
    "        \n",
    "        # get size of of meta-features\n",
    "        meta_features_shape = meta_features.shape[1]  \n",
    "        # get expected number of features,\n",
    "        # binary probabilities from the total number of base-level models\n",
    "        meta_features_expected = self._n_base_models * 2\n",
    "        \n",
    "        # since the base-level models list is not full, \n",
    "        # we need to fill the features until the list is full, \n",
    "        # so we set the remaining expected meta-features as 0\n",
    "        if meta_features_shape < meta_features_expected:\n",
    "            diff = meta_features_expected - meta_features_shape\n",
    "            empty_features = np.zeros((meta_features.shape[0], diff))\n",
    "            meta_features = np.hstack((meta_features, empty_features)) \n",
    "        return meta_features \n",
    "        \n",
    "    def _get_weakest_base_learner(self):\n",
    "        \n",
    "        # loop rounds\n",
    "        worst_model_idx = None \n",
    "        worst_performance = 1\n",
    "        for idx in range(len(self._base_models[0])):\n",
    "            current_round = self._base_models[1][idx]\n",
    "            if current_round < self._n_rounds_eval_base_model:\n",
    "                continue \n",
    "            \n",
    "            current_performance = self._base_models[2][idx].sum()\n",
    "            if current_performance < worst_performance:\n",
    "                worst_performance = current_performance \n",
    "                worst_model_idx = idx\n",
    "                \n",
    "#             print(\"ID: {} Round:{} Performance: {}\".format(idx, current_round, current_performance))        \n",
    "#         print(\"FIND WEAKEST LINK, ID: {}\".format(worst_model_idx))\n",
    "        \n",
    "        return worst_model_idx\n",
    "    \n",
    "    def _train_on_mini_batch(self, X, y):\n",
    "        \n",
    "#         print(\"-----------\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------------\n",
    "        # STEP 1: split mini batch to base-level and meta-level training set\n",
    "        # ----------------------------------------------------------------------------\n",
    "        base_idx = int(self._window_size * (1.0 - self.meta_learner_train_ratio))\n",
    "        X_base = X[0: base_idx, :]\n",
    "        y_base = y[0: base_idx] \n",
    "        # this part will be used to train the meta-level model,\n",
    "        # and to continue training the base-level models on the rest of this batch\n",
    "        X_meta = X[base_idx:self._window_size, :]  \n",
    "        y_meta = y[base_idx:self._window_size]\n",
    "        \n",
    "        # ----------------------------------------------------------------------------\n",
    "        # STEP 2: train previous base-models \n",
    "        # ----------------------------------------------------------------------------\n",
    "        meta_features = []\n",
    "        base_models_len = self._n_base_models - self._base_models[0].count(None)\n",
    "        if base_models_len > 0: # check if we have any base-level models         \n",
    "            base_model_performances = self._meta_learner.feature_importances_\n",
    "#             print(base_model_performances)\n",
    "            for b_idx in range(base_models_len): # loop and train and extract meta-level features \n",
    "                    \n",
    "                # continuation of training (incremental) on base-level model,\n",
    "                # using the base-level training set \n",
    "                base_model = self._base_models[0][b_idx]\n",
    "                base_model.fit(X_base, y_base, xgb_model=base_model.get_booster())\n",
    "                y_predicted = base_model.predict_proba(X_meta) # extract meta-level features \n",
    "                \n",
    "#                 print(\"BASE MODELS: {}\".format(y_predicted[0]))\n",
    "                \n",
    "                # extract meta-features \n",
    "                meta_features = y_predicted if b_idx == 0 else np.hstack((meta_features, y_predicted))                    \n",
    "                \n",
    "                # once the meta-features for this specific base-model are extracted,\n",
    "                # we incrementally fit this base-model to the rest of the data,\n",
    "                # this is done so this base-model is trained on a full batch \n",
    "                base_model.fit(X_meta, y_meta, xgb_model=base_model.get_booster())\n",
    "                                \n",
    "                # update base-level model list \n",
    "                self._base_models[0][b_idx] = base_model\n",
    "                current_round = self._base_models[1][b_idx]\n",
    "                last_performance = base_model_performances[b_idx * 2] + base_model_performances[(b_idx*2)+1] \n",
    "                self._base_models[2][b_idx][current_round%self._n_rounds_eval_base_model] = last_performance\n",
    "                self._base_models[1][b_idx] = current_round + 1\n",
    "                \n",
    "        # ----------------------------------------------------------------------------\n",
    "        # STEP 3: with each new batch, we create/train a new base model \n",
    "        # ----------------------------------------------------------------------------\n",
    "        new_base_model, new_base_model_meta_features = self._train_new_base_model(X_base, y_base, X_meta, y_meta)\n",
    "\n",
    "        insert_idx = base_models_len\n",
    "        if base_models_len == 0:\n",
    "            meta_features = new_base_model_meta_features\n",
    "        elif base_models_len > 0 and base_models_len < self._n_base_models: \n",
    "            meta_features = np.hstack((meta_features, new_base_model_meta_features))     \n",
    "        else: \n",
    "            insert_idx = self._get_weakest_base_learner()           \n",
    "            meta_features[:, insert_idx * 2] = new_base_model_meta_features[:,0]\n",
    "            meta_features[:, (insert_idx * 2) + 1] = new_base_model_meta_features[:,1]\n",
    "            \n",
    "        self._base_models[0][insert_idx] = new_base_model \n",
    "        self._base_models[1][insert_idx] = 0 \n",
    "        self._base_models[2][insert_idx] = np.zeros(self._n_rounds_eval_base_model) \n",
    "        \n",
    "#         print(self._base_models[1])\n",
    "#         print(self._base_models[2])\n",
    "        \n",
    "        # STEP 4: train the meta-level model \n",
    "        meta_features = self._construct_meta_features(meta_features)\n",
    "        if base_models_len == 0:\n",
    "            self._meta_learner.fit(meta_features, y_meta)\n",
    "        else:\n",
    "            self._meta_learner.fit(meta_features, y_meta, xgb_model=self._meta_learner.get_booster())\n",
    "\n",
    "    def predict(self, X):\n",
    "      \n",
    "        # only one model in ensemble use its predictions \n",
    "        base_models_len = self._n_base_models - self._base_models[0].count(None)\n",
    "        if base_models_len == 1:\n",
    "            return self._base_models[0][0].predict(X)\n",
    "        \n",
    "        # predict via meta learner \n",
    "        meta_features = []           \n",
    "        for b_idx in range(base_models_len):\n",
    "            y_predicted = self._base_models[0][b_idx].predict_proba(X) \n",
    "            meta_features = y_predicted if b_idx == 0 else np.hstack((meta_features, y_predicted))                    \n",
    "        meta_features = self._construct_meta_features(meta_features)\n",
    "        return self._meta_learner.predict(meta_features)\n",
    "\n",
    "true_test = []\n",
    "predictions_test = []\n",
    "\n",
    "ncr = NeighbourhoodCleaningRule(n_neighbors=3, threshold_cleaning=0.5)\n",
    "sm = SMOTE()\n",
    "adpBoost = AdaptiveStackedBoostClassifier()\n",
    "\n",
    "for ts in np.arange(data[\"ts\"].min(), data[\"ts\"].max()):\n",
    "    train_set = data[data[\"ts\"] == ts]\n",
    "    train_set_X = train_set.iloc[:,:-1]\n",
    "    train_set_y = train_set[\"class\"]      \n",
    "    \n",
    "    adpBoost.partial_fit(train_set_X.values, train_set_y.values)    \n",
    "    \n",
    "    test_set = data[data[\"ts\"] == ts + 1]\n",
    "    test_set_X = test_set.iloc[:,:-1].values\n",
    "    test_set_y = test_set[\"class\"].values\n",
    "\n",
    "    y_pred = adpBoost.predict(test_set_X)\n",
    "    evaluation = f1_score(test_set_y, y_pred, average='binary')\n",
    "    print(\"Proposed TS {}: {}\".format(ts+1, evaluation))\n",
    "    \n",
    "    if ts+1 >= 35:\n",
    "        true_test.append(test_set_y)\n",
    "        predictions_test.append(y_pred)\n",
    "\n",
    "f1_score_test = f1_score(np.concatenate(true_test, axis=0),   \n",
    "                         np.concatenate(predictions_test, axis=0), \n",
    "                         average='binary')\n",
    "print(\"F1-Score on test set: {}\".format(round(f1_score_test, 3)))     \n",
    "\n",
    "recall_score_test = recall_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 average='binary')\n",
    "print(\"Recall on test set: {}\".format(round(recall_score_test, 3)))      \n",
    "\n",
    "precision_score_test = precision_score(np.concatenate(true_test, axis=0),   \n",
    "                                       np.concatenate(predictions_test, axis=0), \n",
    "                                       average='binary')\n",
    "print(\"Precision on test set: {}\".format(round(precision_score_test, 3)))    \n",
    "\n",
    "confusion_matrix_test = confusion_matrix(np.concatenate(true_test, axis=0), \n",
    "                                         np.concatenate(predictions_test, axis=0))\n",
    "print(\"Confusion_matrix: {}\".format(confusion_matrix_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveXGBoostClassifier(BaseSKMObject, ClassifierMixin):\n",
    "    _PUSH_STRATEGY = 'push'\n",
    "    _REPLACE_STRATEGY = 'replace'\n",
    "    _UPDATE_STRATEGIES = [_PUSH_STRATEGY, _REPLACE_STRATEGY]\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_estimators=30,\n",
    "                 learning_rate=0.3,\n",
    "                 max_depth=6,\n",
    "                 max_window_size=1000,\n",
    "                 min_window_size=None,\n",
    "                 detect_drift=False,\n",
    "                 update_strategy='replace'):\n",
    "        \"\"\"\n",
    "        Adaptive XGBoost classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators: int (default=5)\n",
    "            The number of estimators in the ensemble.\n",
    "        learning_rate:\n",
    "            Learning rate, a.k.a eta.\n",
    "        max_depth: int (default = 6)\n",
    "            Max tree depth.\n",
    "        max_window_size: int (default=1000)\n",
    "            Max window size.\n",
    "        min_window_size: int (default=None)\n",
    "            Min window size. If this parameters is not set, then a fixed size\n",
    "            window of size ``max_window_size`` will be used.\n",
    "        detect_drift: bool (default=False)\n",
    "            If set will use a drift detector (ADWIN).\n",
    "        update_strategy: str (default='replace')\n",
    "            | The update strategy to use:\n",
    "            | 'push' - the ensemble resembles a queue\n",
    "            | 'replace' - oldest ensemble members are replaced by newer ones\n",
    "        Notes\n",
    "        -----\n",
    "        The Adaptive XGBoost [1]_ (AXGB) classifier is an adaptation of the\n",
    "        XGBoost algorithm for evolving data streams. AXGB creates new members\n",
    "        of the ensemble from mini-batches of data as new data becomes\n",
    "        available.  The maximum ensemble  size is fixed, but learning does not\n",
    "        stop once this size is reached, the ensemble is updated on new data to\n",
    "        ensure consistency with the current data distribution.\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Montiel, Jacob, Mitchell, Rory, Frank, Eibe, Pfahringer,\n",
    "           Bernhard, Abdessalem, Talel, and Bifet, Albert. “AdaptiveXGBoost for\n",
    "           Evolving Data Streams”. In:IJCNN’20. International Joint Conference\n",
    "           on Neural Networks. 2020. Forthcoming.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_window_size = max_window_size\n",
    "        self.min_window_size = min_window_size\n",
    "        self._first_run = True\n",
    "        self._ensemble = None\n",
    "        self.detect_drift = detect_drift\n",
    "        self._drift_detector = None\n",
    "        self._X_buffer = np.array([])\n",
    "        self._y_buffer = np.array([])\n",
    "        self._samples_seen = 0\n",
    "        self._model_idx = 0\n",
    "        if update_strategy not in self._UPDATE_STRATEGIES:\n",
    "            raise AttributeError(\"Invalid update_strategy: {}\\n\"\n",
    "                                 \"Valid options: {}\".format(update_strategy,\n",
    "                                                            self._UPDATE_STRATEGIES))\n",
    "        self.update_strategy = update_strategy\n",
    "        self._configure()\n",
    "\n",
    "    def _configure(self):\n",
    "        if self.update_strategy == self._PUSH_STRATEGY:\n",
    "            self._ensemble = []\n",
    "        elif self.update_strategy == self._REPLACE_STRATEGY:\n",
    "            self._ensemble = [None] * self.n_estimators\n",
    "        self._reset_window_size()\n",
    "        self._init_margin = 0.0\n",
    "        self._boosting_params = {\"silent\": True,\n",
    "                                 \"objective\": \"binary:logistic\",\n",
    "                                 \"eta\": self.learning_rate,\n",
    "                                 \"max_depth\": self.max_depth}\n",
    "        if self.detect_drift:\n",
    "            self._drift_detector = ADWIN()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the estimator.\n",
    "        \"\"\"\n",
    "        self._first_run = True\n",
    "        self._configure()\n",
    "\n",
    "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Partially (incrementally) fit the model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            An array of shape (n_samples, n_features) with the data upon which\n",
    "            the algorithm will create its model.\n",
    "        y: Array-like\n",
    "            An array of shape (, n_samples) containing the classification\n",
    "            targets for all samples in X. Only binary data is supported.\n",
    "        classes: Not used.\n",
    "        sample_weight: Not used.\n",
    "        Returns\n",
    "        -------\n",
    "        AdaptiveXGBoostClassifier\n",
    "            self\n",
    "        \"\"\"\n",
    "        for i in range(X.shape[0]):\n",
    "            self._partial_fit(np.array([X[i, :]]), np.array([y[i]]))\n",
    "        return self\n",
    "\n",
    "    def _partial_fit(self, X, y):\n",
    "        if self._first_run:\n",
    "            self._X_buffer = np.array([]).reshape(0, get_dimensions(X)[1])\n",
    "            self._y_buffer = np.array([])\n",
    "            self._first_run = False\n",
    "        self._X_buffer = np.concatenate((self._X_buffer, X))\n",
    "        self._y_buffer = np.concatenate((self._y_buffer, y))\n",
    "        while self._X_buffer.shape[0] >= self.window_size:\n",
    "            self._train_on_mini_batch(X=self._X_buffer[0:self.window_size, :],\n",
    "                                      y=self._y_buffer[0:self.window_size])\n",
    "            delete_idx = [i for i in range(self.window_size)]\n",
    "            self._X_buffer = np.delete(self._X_buffer, delete_idx, axis=0)\n",
    "            self._y_buffer = np.delete(self._y_buffer, delete_idx, axis=0)\n",
    "\n",
    "            # Check window size and adjust it if necessary\n",
    "            self._adjust_window_size()\n",
    "\n",
    "        # Support for concept drift\n",
    "        if self.detect_drift:\n",
    "            correctly_classifies = self.predict(X) == y\n",
    "            # Check for warning\n",
    "            self._drift_detector.add_element(int(not correctly_classifies))\n",
    "            # Check if there was a change\n",
    "            if self._drift_detector.detected_change():\n",
    "                # Reset window size\n",
    "                self._reset_window_size()\n",
    "                if self.update_strategy == self._REPLACE_STRATEGY:\n",
    "                    self._model_idx = 0\n",
    "\n",
    "    def _adjust_window_size(self):\n",
    "        if self._dynamic_window_size < self.max_window_size:\n",
    "            self._dynamic_window_size *= 2\n",
    "            if self._dynamic_window_size > self.max_window_size:\n",
    "                self.window_size = self.max_window_size\n",
    "            else:\n",
    "                self.window_size = self._dynamic_window_size\n",
    "\n",
    "    def _reset_window_size(self):\n",
    "        if self.min_window_size:\n",
    "            self._dynamic_window_size = self.min_window_size\n",
    "        else:\n",
    "            self._dynamic_window_size = self.max_window_size\n",
    "        self.window_size = self._dynamic_window_size\n",
    "\n",
    "    def _train_on_mini_batch(self, X, y):\n",
    "        if self.update_strategy == self._REPLACE_STRATEGY:\n",
    "            booster = self._train_booster(X, y, self._model_idx)\n",
    "            # Update ensemble\n",
    "            self._ensemble[self._model_idx] = booster\n",
    "            self._samples_seen += X.shape[0]\n",
    "            self._update_model_idx()\n",
    "        else:   # self.update_strategy == self._PUSH_STRATEGY\n",
    "            booster = self._train_booster(X, y, len(self._ensemble))\n",
    "            # Update ensemble\n",
    "            if len(self._ensemble) == self.n_estimators:\n",
    "                self._ensemble.pop(0)\n",
    "            self._ensemble.append(booster)\n",
    "            self._samples_seen += X.shape[0]\n",
    "\n",
    "    def _train_booster(self, X: np.ndarray, y: np.ndarray, last_model_idx: int):\n",
    "        d_mini_batch_train = xgb.DMatrix(X, y.astype(int))\n",
    "        # Get margins from trees in the ensemble\n",
    "        margins = np.asarray([self._init_margin] * d_mini_batch_train.num_row())\n",
    "        for j in range(last_model_idx):\n",
    "            margins = np.add(margins,\n",
    "                             self._ensemble[j].predict(d_mini_batch_train, output_margin=True))\n",
    "        d_mini_batch_train.set_base_margin(margin=margins)\n",
    "        booster = xgb.train(params=self._boosting_params,\n",
    "                            dtrain=d_mini_batch_train,\n",
    "                            num_boost_round=1,\n",
    "                            verbose_eval=False)\n",
    "        return booster\n",
    "\n",
    "    def _update_model_idx(self):\n",
    "        self._model_idx += 1\n",
    "        if self._model_idx == self.n_estimators:\n",
    "            self._model_idx = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class label for sample X\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            An array of shape (n_samples, n_features) with the samples to\n",
    "            predict the class label for.\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            A 1D array of shape (, n_samples), containing the\n",
    "            predicted class labels for all instances in X.\n",
    "        \"\"\"\n",
    "        if self._ensemble:\n",
    "            if self.update_strategy == self._REPLACE_STRATEGY:\n",
    "                trees_in_ensemble = sum(i is not None for i in self._ensemble)\n",
    "            else:   # self.update_strategy == self._PUSH_STRATEGY\n",
    "                trees_in_ensemble = len(self._ensemble)\n",
    "            if trees_in_ensemble > 0:\n",
    "                d_test = xgb.DMatrix(X)\n",
    "                for i in range(trees_in_ensemble - 1):\n",
    "                    margins = self._ensemble[i].predict(d_test, output_margin=True)\n",
    "                    d_test.set_base_margin(margin=margins)\n",
    "                predicted = self._ensemble[trees_in_ensemble - 1].predict(d_test)\n",
    "                return np.array(predicted > 0.5).astype(int)\n",
    "        # Ensemble is empty, return default values (0)\n",
    "        return np.zeros(get_dimensions(X)[0])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Not implemented for this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"predict_proba is not implemented for this method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive XGBoost classifier parameters\n",
    "n_estimators = 30       # Number of members in the ensemble\n",
    "learning_rate = 0.3     # Learning rate or eta\n",
    "max_depth = 6           # Max depth for each tree in the ensemble\n",
    "max_window_size = 1000  # Max window size\n",
    "min_window_size = 1     # set to activate the dynamic window strategy\n",
    "detect_drift = False    # Enable/disable drift detection\n",
    "\n",
    "AXGBr = AdaptiveXGBoostClassifier(update_strategy='replace',\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  max_depth=max_depth,\n",
    "                                  max_window_size=max_window_size,\n",
    "                                  min_window_size=min_window_size,\n",
    "                                  detect_drift=detect_drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed TS 2: 0.0\n",
      "Proposed TS 3: 0.3076923076923077\n",
      "Proposed TS 4: 0.8679245283018869\n",
      "Proposed TS 5: 0.2222222222222222\n",
      "Proposed TS 6: 0.0\n",
      "Proposed TS 7: 0.03809523809523809\n",
      "Proposed TS 8: 0.48888888888888893\n",
      "Proposed TS 9: 0.7562189054726367\n",
      "Proposed TS 10: 0.30769230769230765\n",
      "Proposed TS 11: 0.9000000000000001\n",
      "Proposed TS 12: 0.9032258064516129\n",
      "Proposed TS 13: 0.9382716049382716\n",
      "Proposed TS 14: 0.8860759493670887\n",
      "Proposed TS 15: 0.9791666666666666\n",
      "Proposed TS 16: 0.9435483870967741\n",
      "Proposed TS 17: 0.8791208791208791\n",
      "Proposed TS 18: 0.8723404255319148\n",
      "Proposed TS 19: 0.822695035460993\n",
      "Proposed TS 20: 0.84\n",
      "Proposed TS 21: 0.8361581920903954\n",
      "Proposed TS 22: 0.9019607843137254\n",
      "Proposed TS 23: 0.5591397849462365\n",
      "Proposed TS 24: 0.33333333333333337\n",
      "Proposed TS 25: 0.9292035398230089\n",
      "Proposed TS 26: 0.8023255813953488\n",
      "Proposed TS 27: 0.9199999999999999\n",
      "Proposed TS 28: 0.9822485207100591\n",
      "Proposed TS 29: 0.47240618101545256\n",
      "Proposed TS 30: 0.8679245283018868\n",
      "Proposed TS 31: 0.593939393939394\n",
      "Proposed TS 32: 0.7263339070567986\n",
      "Proposed TS 33: 0.6976744186046512\n",
      "Proposed TS 34: 0.8219178082191781\n",
      "Proposed TS 35: 0.843501326259947\n",
      "Proposed TS 36: 0.537037037037037\n",
      "Proposed TS 37: 0.7462686567164178\n",
      "Proposed TS 38: 0.8305084745762712\n",
      "Proposed TS 39: 0.7075471698113207\n",
      "Proposed TS 40: 0.6083333333333333\n",
      "Proposed TS 41: 0.8492063492063493\n",
      "Proposed TS 42: 0.7744510978043913\n",
      "Proposed TS 43: 0.0\n",
      "Proposed TS 44: 0.11363636363636363\n",
      "Proposed TS 45: 0.05063291139240506\n",
      "Proposed TS 46: 0.07692307692307693\n",
      "Proposed TS 47: 0.0\n",
      "Proposed TS 48: 0.08695652173913045\n",
      "Proposed TS 49: 0.23076923076923078\n",
      "F1-Score on test set: 0.648\n",
      "Recall on test set: 0.719\n",
      "Precision on test set: 0.59\n",
      "Confusion_matrix: [[15045   542]\n",
      " [  304   779]]\n"
     ]
    }
   ],
   "source": [
    "true_test = []\n",
    "predictions_test = []\n",
    "for ts in np.arange(data[\"ts\"].min(), data[\"ts\"].max()):\n",
    "    train_set = data[data[\"ts\"] == ts]\n",
    "    train_set_X = train_set.iloc[:,:-1]\n",
    "    train_set_y = train_set[\"class\"]      \n",
    "    AXGBr.partial_fit(train_set_X.values, train_set_y.values)    \n",
    "    \n",
    "    test_set = data[data[\"ts\"] == ts + 1]\n",
    "    test_set_X = test_set.iloc[:,:-1].values\n",
    "    test_set_y = test_set[\"class\"].values\n",
    "\n",
    "    y_pred = AXGBr.predict(test_set_X)\n",
    "    evaluation = f1_score(test_set_y, y_pred, average='binary')\n",
    "    print(\"Proposed TS {}: {}\".format(ts+1, evaluation))\n",
    "    \n",
    "    if ts+1 >= 35:\n",
    "        true_test.append(test_set_y)\n",
    "        predictions_test.append(y_pred)\n",
    "\n",
    "f1_score_test = f1_score(np.concatenate(true_test, axis=0),   \n",
    "                         np.concatenate(predictions_test, axis=0), \n",
    "                         average='binary')\n",
    "print(\"F1-Score on test set: {}\".format(round(f1_score_test, 3)))     \n",
    "\n",
    "recall_score_test = recall_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 average='binary')\n",
    "\n",
    "print(\"Recall on test set: {}\".format(round(recall_score_test, 3)))      \n",
    "\n",
    "precision_score_test = precision_score(np.concatenate(true_test, axis=0),   \n",
    "                                       np.concatenate(predictions_test, axis=0), \n",
    "                                       average='binary')\n",
    "print(\"Precision on test set: {}\".format(round(precision_score_test, 3)))    \n",
    "\n",
    "confusion_matrix_test = confusion_matrix(np.concatenate(true_test, axis=0), \n",
    "                                         np.concatenate(predictions_test, axis=0))\n",
    "print(\"Confusion_matrix: {}\".format(confusion_matrix_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed TS 2: 0.0\n",
      "Proposed TS 3: 0.3076923076923077\n",
      "Proposed TS 4: 0.8679245283018869\n",
      "Proposed TS 5: 0.2222222222222222\n",
      "Proposed TS 6: 0.0\n",
      "Proposed TS 7: 0.03809523809523809\n",
      "Proposed TS 8: 0.48888888888888893\n",
      "Proposed TS 9: 0.7562189054726367\n",
      "Proposed TS 10: 0.30769230769230765\n",
      "Proposed TS 11: 0.9000000000000001\n",
      "Proposed TS 12: 0.9032258064516129\n",
      "Proposed TS 13: 0.9382716049382716\n",
      "Proposed TS 14: 0.8860759493670887\n",
      "Proposed TS 15: 0.9791666666666666\n",
      "Proposed TS 16: 0.9435483870967741\n",
      "Proposed TS 17: 0.8791208791208791\n",
      "Proposed TS 18: 0.8723404255319148\n",
      "Proposed TS 19: 0.822695035460993\n",
      "Proposed TS 20: 0.84\n",
      "Proposed TS 21: 0.8361581920903954\n",
      "Proposed TS 22: 0.9019607843137254\n",
      "Proposed TS 23: 0.5591397849462365\n",
      "Proposed TS 24: 0.3278688524590164\n",
      "Proposed TS 25: 0.9090909090909091\n",
      "Proposed TS 26: 0.7042253521126761\n",
      "Proposed TS 27: 0.8518518518518519\n",
      "Proposed TS 28: 0.9655172413793103\n",
      "Proposed TS 29: 0.8655834564254062\n",
      "Proposed TS 30: 0.8426966292134832\n",
      "Proposed TS 31: 0.5268292682926828\n",
      "Proposed TS 32: 0.7678571428571429\n",
      "Proposed TS 33: 0.6666666666666667\n",
      "Proposed TS 34: 0.7654320987654323\n",
      "Proposed TS 35: 0.7755102040816326\n",
      "Proposed TS 36: 0.5344827586206897\n",
      "Proposed TS 37: 0.7605633802816901\n",
      "Proposed TS 38: 0.816\n",
      "Proposed TS 39: 0.6754385964912281\n",
      "Proposed TS 40: 0.5892857142857143\n",
      "Proposed TS 41: 0.8464730290456431\n",
      "Proposed TS 42: 0.7566462167689161\n",
      "Proposed TS 43: 0.02702702702702703\n",
      "Proposed TS 44: 0.13333333333333333\n",
      "Proposed TS 45: 0.0\n",
      "Proposed TS 46: 0.0\n",
      "Proposed TS 47: 0.0\n",
      "Proposed TS 48: 0.0\n",
      "Proposed TS 49: 0.38202247191011235\n",
      "F1-Score on test set: 0.639\n",
      "Recall on test set: 0.722\n",
      "Precision on test set: 0.572\n",
      "Confusion_matrix: [[15003   584]\n",
      " [  301   782]]\n"
     ]
    }
   ],
   "source": [
    "# Adaptive XGBoost classifier parameters\n",
    "n_estimators = 30       # Number of members in the ensemble\n",
    "learning_rate = 0.3     # Learning rate or eta\n",
    "max_depth = 6           # Max depth for each tree in the ensemble\n",
    "max_window_size = 1000  # Max window size\n",
    "min_window_size = 1     # set to activate the dynamic window strategy\n",
    "detect_drift = False    # Enable/disable drift detection\n",
    "\n",
    "AXGBp = AdaptiveXGBoostClassifier(update_strategy='push',\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  max_depth=max_depth,\n",
    "                                  max_window_size=max_window_size,\n",
    "                                  min_window_size=min_window_size,\n",
    "                                  detect_drift=detect_drift)\n",
    "\n",
    "true_test = []\n",
    "predictions_test = []\n",
    "for ts in np.arange(data[\"ts\"].min(), data[\"ts\"].max()):\n",
    "    train_set = data[data[\"ts\"] == ts]\n",
    "    train_set_X = train_set.iloc[:,:-1]\n",
    "    train_set_y = train_set[\"class\"]      \n",
    "    AXGBp.partial_fit(train_set_X.values, train_set_y.values)    \n",
    "    \n",
    "    test_set = data[data[\"ts\"] == ts + 1]\n",
    "    test_set_X = test_set.iloc[:,:-1].values\n",
    "    test_set_y = test_set[\"class\"].values\n",
    "\n",
    "    y_pred = AXGBp.predict(test_set_X)\n",
    "    evaluation = f1_score(test_set_y, y_pred, average='binary')\n",
    "    print(\"Proposed TS {}: {}\".format(ts+1, evaluation))\n",
    "    \n",
    "    if ts+1 >= 35:\n",
    "        true_test.append(test_set_y)\n",
    "        predictions_test.append(y_pred)\n",
    "        \n",
    "\n",
    "f1_score_test = f1_score(np.concatenate(true_test, axis=0),   \n",
    "                         np.concatenate(predictions_test, axis=0), \n",
    "                         average='binary')\n",
    "print(\"F1-Score on test set: {}\".format(round(f1_score_test, 3)))     \n",
    "\n",
    "recall_score_test = recall_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 average='binary')\n",
    "\n",
    "print(\"Recall on test set: {}\".format(round(recall_score_test, 3)))      \n",
    "\n",
    "precision_score_test = precision_score(np.concatenate(true_test, axis=0),   \n",
    "                                       np.concatenate(predictions_test, axis=0), \n",
    "                                       average='binary')\n",
    "print(\"Precision on test set: {}\".format(round(precision_score_test, 3)))    \n",
    "\n",
    "confusion_matrix_test = confusion_matrix(np.concatenate(true_test, axis=0), \n",
    "                                         np.concatenate(predictions_test, axis=0))\n",
    "print(\"Confusion_matrix: {}\".format(confusion_matrix_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed TS 2: 0.9411764705882353\n",
      "Proposed TS 3: 0.9\n",
      "Proposed TS 4: 0.9090909090909091\n",
      "Proposed TS 5: 0.23529411764705882\n",
      "Proposed TS 6: 0.0\n",
      "Proposed TS 7: 0.7955801104972375\n",
      "Proposed TS 8: 0.832116788321168\n",
      "Proposed TS 9: 0.9637096774193549\n",
      "Proposed TS 10: 0.8648648648648649\n",
      "Proposed TS 11: 0.9453124999999999\n",
      "Proposed TS 12: 0.9032258064516129\n",
      "Proposed TS 13: 0.9775474956822107\n",
      "Proposed TS 14: 0.9047619047619047\n",
      "Proposed TS 15: 0.9795918367346939\n",
      "Proposed TS 16: 0.9561752988047808\n",
      "Proposed TS 17: 0.8807339449541284\n",
      "Proposed TS 18: 0.8867924528301887\n",
      "Proposed TS 19: 0.8456375838926175\n",
      "Proposed TS 20: 0.896414342629482\n",
      "Proposed TS 21: 0.8703703703703703\n",
      "Proposed TS 22: 0.8513119533527698\n",
      "Proposed TS 23: 0.6236559139784946\n",
      "Proposed TS 24: 0.5311203319502076\n",
      "Proposed TS 25: 0.9380530973451328\n",
      "Proposed TS 26: 0.8627450980392156\n",
      "Proposed TS 27: 0.9166666666666666\n",
      "Proposed TS 28: 0.9764705882352941\n",
      "Proposed TS 29: 0.858085808580858\n",
      "Proposed TS 30: 0.8862275449101796\n",
      "Proposed TS 31: 0.7692307692307693\n",
      "Proposed TS 32: 0.810228802153432\n",
      "Proposed TS 33: 0.7391304347826085\n",
      "Proposed TS 34: 0.9166666666666667\n",
      "Proposed TS 35: 0.902061855670103\n",
      "Proposed TS 36: 0.8051948051948052\n",
      "Proposed TS 37: 0.7647058823529412\n",
      "Proposed TS 38: 0.825242718446602\n",
      "Proposed TS 39: 0.781725888324873\n",
      "Proposed TS 40: 0.5892857142857143\n",
      "Proposed TS 41: 0.6956521739130435\n",
      "Proposed TS 42: 0.8222222222222222\n",
      "Proposed TS 43: 0.11627906976744184\n",
      "Proposed TS 44: 0.1170731707317073\n",
      "Proposed TS 45: 0.052631578947368425\n",
      "Proposed TS 46: 0.0\n",
      "Proposed TS 47: 0.22222222222222218\n",
      "Proposed TS 48: 0.463768115942029\n",
      "Proposed TS 49: 0.7563025210084034\n",
      "Accuracy Score on test set: 0.952\n",
      "F1-Score on test set: 0.675\n",
      "Recall on test set: 0.76\n",
      "Precision on test set: 0.607\n",
      "Confusion_matrix: [[15054   533]\n",
      " [  260   823]]\n"
     ]
    }
   ],
   "source": [
    "incremental_xgb = xgb.XGBClassifier()\n",
    "true_test = []\n",
    "predictions_test = []\n",
    "for ts in np.arange(data[\"ts\"].min(), data[\"ts\"].max()):\n",
    "    train_set = data[data[\"ts\"] == ts]\n",
    "    train_set_X = train_set.iloc[:,:-1]\n",
    "    train_set_y = train_set[\"class\"]      \n",
    "    \n",
    "    if ts > 1:\n",
    "        incremental_xgb.fit(train_set_X.values, train_set_y.values, xgb_model=incremental_xgb.get_booster())\n",
    "    else:\n",
    "        incremental_xgb.fit(train_set_X.values, train_set_y.values)    \n",
    "    \n",
    "    test_set = data[data[\"ts\"] == ts + 1]\n",
    "    test_set_X = test_set.iloc[:,:-1].values\n",
    "    test_set_y = test_set[\"class\"].values\n",
    "\n",
    "    y_pred = incremental_xgb.predict(test_set_X)\n",
    "    evaluation = f1_score(test_set_y, y_pred, average='binary')\n",
    "    print(\"Proposed TS {}: {}\".format(ts+1, evaluation))\n",
    "    \n",
    "    if ts+1 >= 35:\n",
    "        true_test.append(test_set_y)\n",
    "        predictions_test.append(y_pred)\n",
    "\n",
    "f1_score_test = f1_score(np.concatenate(true_test, axis=0),   \n",
    "                         np.concatenate(predictions_test, axis=0), \n",
    "                         average='binary')\n",
    "\n",
    "accuracy_test = accuracy_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 normalize=True)\n",
    "\n",
    "print(\"Accuracy Score on test set: {}\".format(round(accuracy_test, 3)))      \n",
    "\n",
    "\n",
    "print(\"F1-Score on test set: {}\".format(round(f1_score_test, 3)))     \n",
    "\n",
    "recall_score_test = recall_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 average='binary')\n",
    "\n",
    "print(\"Recall on test set: {}\".format(round(recall_score_test, 3)))      \n",
    "\n",
    "precision_score_test = precision_score(np.concatenate(true_test, axis=0),   \n",
    "                                       np.concatenate(predictions_test, axis=0), \n",
    "                                       average='binary')\n",
    "print(\"Precision on test set: {}\".format(round(precision_score_test, 3)))    \n",
    "\n",
    "confusion_matrix_test = confusion_matrix(np.concatenate(true_test, axis=0), \n",
    "                                         np.concatenate(predictions_test, axis=0))\n",
    "print(\"Confusion_matrix: {}\".format(confusion_matrix_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed TS 2: 0.0\n",
      "Proposed TS 3: 0.0\n",
      "Proposed TS 4: 0.0\n",
      "Proposed TS 5: 0.0\n",
      "Proposed TS 6: 0.020408163265306124\n",
      "Proposed TS 7: 0.15632183908045977\n",
      "Proposed TS 8: 0.0\n",
      "Proposed TS 9: 0.0\n",
      "Proposed TS 10: 0.0\n",
      "Proposed TS 11: 0.0\n",
      "Proposed TS 12: 0.0\n",
      "Proposed TS 13: 0.0\n",
      "Proposed TS 14: 0.0\n",
      "Proposed TS 15: 0.0\n",
      "Proposed TS 16: 0.0\n",
      "Proposed TS 17: 0.0\n",
      "Proposed TS 18: 0.0\n",
      "Proposed TS 19: 0.0\n",
      "Proposed TS 20: 0.0\n",
      "Proposed TS 21: 0.0\n",
      "Proposed TS 22: 0.0\n",
      "Proposed TS 23: 0.0\n",
      "Proposed TS 24: 0.0\n",
      "Proposed TS 25: 0.0\n",
      "Proposed TS 26: 0.0\n",
      "Proposed TS 27: 0.0\n",
      "Proposed TS 28: 0.0\n",
      "Proposed TS 29: 0.0\n",
      "Proposed TS 30: 0.0\n",
      "Proposed TS 31: 0.0\n",
      "Proposed TS 32: 0.0\n",
      "Proposed TS 33: 0.0\n",
      "Proposed TS 34: 0.0\n",
      "Proposed TS 35: 0.0\n",
      "Proposed TS 36: 0.0\n",
      "Proposed TS 37: 0.0\n",
      "Proposed TS 38: 0.0\n",
      "Proposed TS 39: 0.0\n",
      "Proposed TS 40: 0.0\n",
      "Proposed TS 41: 0.0\n",
      "Proposed TS 42: 0.0\n",
      "Proposed TS 43: 0.0\n",
      "Proposed TS 44: 0.0\n",
      "Proposed TS 45: 0.0\n",
      "Proposed TS 46: 0.0\n",
      "Proposed TS 47: 0.0\n",
      "Proposed TS 48: 0.0\n",
      "Proposed TS 49: 0.0\n",
      "F1-Score on test set: 0.21\n",
      "Recall on test set: 0.127\n",
      "Accuracy Score on test set: 0.938\n",
      "Precision on test set: 0.607\n",
      "Accuracy on test set: 0.607\n",
      "Confusion_matrix: [[92989   533]\n",
      " [ 5675   823]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skmultiflow.meta import AccuracyWeightedEnsemble\n",
    "\n",
    "awej485 = AccuracyWeightedEnsemble(n_estimators=30, \n",
    "                                   window_size=1000, \n",
    "                                   base_estimator=DecisionTreeClassifier())\n",
    "\n",
    "for ts in np.arange(data[\"ts\"].min(), data[\"ts\"].max()):\n",
    "    train_set = data[data[\"ts\"] == ts]\n",
    "    train_set_X = train_set.iloc[:,:-1]\n",
    "    train_set_y = train_set[\"class\"]      \n",
    "\n",
    "    awej485.partial_fit(train_set_X.values, train_set_y.values)    \n",
    "    \n",
    "    test_set = data[data[\"ts\"] == ts + 1]\n",
    "    test_set_X = test_set.iloc[:,:-1].values\n",
    "    test_set_y = test_set[\"class\"].values\n",
    "\n",
    "    y_pred = awej485.predict(test_set_X)\n",
    "    evaluation = f1_score(test_set_y, y_pred, average='binary')\n",
    "    print(\"Proposed TS {}: {}\".format(ts+1, evaluation))\n",
    "    \n",
    "    if ts+1 >= 35:\n",
    "        true_test.append(test_set_y)\n",
    "        predictions_test.append(y_pred)\n",
    "\n",
    "f1_score_test = f1_score(np.concatenate(true_test, axis=0),   \n",
    "                         np.concatenate(predictions_test, axis=0), \n",
    "                         average='binary')\n",
    "print(\"F1-Score on test set: {}\".format(round(f1_score_test, 3)))     \n",
    "\n",
    "recall_score_test = recall_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 average='binary')\n",
    "\n",
    "print(\"Recall on test set: {}\".format(round(recall_score_test, 3)))      \n",
    "\n",
    "accuracy_test = accuracy_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 normalize=True)\n",
    "\n",
    "print(\"Accuracy Score on test set: {}\".format(round(accuracy_test, 3)))      \n",
    "\n",
    "\n",
    "precision_score_test = precision_score(np.concatenate(true_test, axis=0),   \n",
    "                                       np.concatenate(predictions_test, axis=0), \n",
    "                                       average='binary')\n",
    "print(\"Precision on test set: {}\".format(round(precision_score_test, 3)))    \n",
    "\n",
    "\n",
    "precision_score_test = precision_score(np.concatenate(true_test, axis=0),   \n",
    "                                       np.concatenate(predictions_test, axis=0), \n",
    "                                       average='binary')\n",
    "print(\"Accuracy on test set: {}\".format(round(precision_score_test, 3)))    \n",
    "\n",
    "\n",
    "confusion_matrix_test = confusion_matrix(np.concatenate(true_test, axis=0), \n",
    "                                         np.concatenate(predictions_test, axis=0))\n",
    "print(\"Confusion_matrix: {}\".format(confusion_matrix_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed TS 2: 0.1739130434782609\n",
      "Proposed TS 3: 0.16666666666666669\n",
      "Proposed TS 4: 0.18181818181818182\n",
      "Proposed TS 5: 0.0\n",
      "Proposed TS 6: 0.0\n",
      "Proposed TS 7: 0.01941747572815534\n",
      "Proposed TS 8: 0.66\n",
      "Proposed TS 9: 0.9075268817204301\n",
      "Proposed TS 10: 0.7058823529411765\n",
      "Proposed TS 11: 0.8851063829787233\n",
      "Proposed TS 12: 0.896551724137931\n",
      "Proposed TS 13: 0.9516994633273702\n",
      "Proposed TS 14: 0.875\n",
      "Proposed TS 15: 0.9757785467128027\n",
      "Proposed TS 16: 0.959349593495935\n",
      "Proposed TS 17: 0.9417989417989419\n",
      "Proposed TS 18: 0.9052631578947368\n",
      "Proposed TS 19: 0.8904109589041096\n",
      "Proposed TS 20: 0.8842105263157894\n",
      "Proposed TS 21: 0.8272251308900523\n",
      "Proposed TS 22: 0.9266666666666666\n",
      "Proposed TS 23: 0.6506024096385542\n",
      "Proposed TS 24: 0.34523809523809523\n",
      "Proposed TS 25: 0.9174311926605504\n",
      "Proposed TS 26: 0.8143712574850299\n",
      "Proposed TS 27: 0.9787234042553191\n",
      "Proposed TS 28: 0.9820359281437125\n",
      "Proposed TS 29: 0.9064039408866995\n",
      "Proposed TS 30: 0.9056603773584905\n",
      "Proposed TS 31: 0.6424242424242425\n",
      "Proposed TS 32: 0.9361702127659574\n",
      "Proposed TS 33: 0.75\n",
      "Proposed TS 34: 0.8615384615384616\n",
      "Proposed TS 35: 0.9458689458689459\n",
      "Proposed TS 36: 0.9846153846153847\n",
      "Proposed TS 37: 0.6206896551724138\n",
      "Proposed TS 38: 0.911764705882353\n",
      "Proposed TS 39: 0.8979591836734693\n",
      "Proposed TS 40: 0.6705882352941177\n",
      "Proposed TS 41: 0.9107981220657276\n",
      "Proposed TS 42: 0.8284313725490197\n",
      "Proposed TS 43: 0.0\n",
      "Proposed TS 44: 0.0\n",
      "Proposed TS 45: 0.0\n",
      "Proposed TS 46: 0.0\n",
      "Proposed TS 47: 0.0\n",
      "Proposed TS 48: 0.0\n",
      "Proposed TS 49: 0.0689655172413793\n",
      "F1-Score on test set: 0.72\n",
      "Recall on test set: 0.703\n",
      "Accuracy Score on test set: 0.965\n",
      "Precision on test set: 0.738\n",
      "Accuracy on test set: 0.738\n",
      "Confusion_matrix: [[30634   540]\n",
      " [  643  1523]]\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.meta import AdaptiveRandomForest\n",
    "\n",
    "arf = AdaptiveRandomForest(performance_metric=\"kappa\")\n",
    "\n",
    "\n",
    "for ts in np.arange(data[\"ts\"].min(), data[\"ts\"].max()):\n",
    "    train_set = data[data[\"ts\"] == ts]\n",
    "    train_set_X = train_set.iloc[:,:-1]\n",
    "    train_set_y = train_set[\"class\"]      \n",
    "\n",
    "    arf.partial_fit(train_set_X.values, train_set_y.values)    \n",
    "    \n",
    "    test_set = data[data[\"ts\"] == ts + 1]\n",
    "    test_set_X = test_set.iloc[:,:-1].values\n",
    "    test_set_y = test_set[\"class\"].values\n",
    "\n",
    "    y_pred = arf.predict(test_set_X)\n",
    "    evaluation = f1_score(test_set_y, y_pred, average='binary')\n",
    "    print(\"Proposed TS {}: {}\".format(ts+1, evaluation))\n",
    "    \n",
    "    if ts+1 >= 35:\n",
    "        true_test.append(test_set_y)\n",
    "        predictions_test.append(y_pred)\n",
    "        \n",
    "f1_score_test = f1_score(np.concatenate(true_test, axis=0),   \n",
    "                         np.concatenate(predictions_test, axis=0), \n",
    "                         average='binary')\n",
    "print(\"F1-Score on test set: {}\".format(round(f1_score_test, 3)))     \n",
    "\n",
    "recall_score_test = recall_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 average='binary')\n",
    "\n",
    "print(\"Recall on test set: {}\".format(round(recall_score_test, 3)))      \n",
    "\n",
    "accuracy_test = accuracy_score(np.concatenate(true_test, axis=0),   \n",
    "                                 np.concatenate(predictions_test, axis=0), \n",
    "                                 normalize=True)\n",
    "\n",
    "print(\"Accuracy Score on test set: {}\".format(round(accuracy_test, 3)))      \n",
    "\n",
    "\n",
    "precision_score_test = precision_score(np.concatenate(true_test, axis=0),   \n",
    "                                       np.concatenate(predictions_test, axis=0), \n",
    "                                       average='binary')\n",
    "print(\"Precision on test set: {}\".format(round(precision_score_test, 3)))    \n",
    "\n",
    "\n",
    "precision_score_test = precision_score(np.concatenate(true_test, axis=0),   \n",
    "                                       np.concatenate(predictions_test, axis=0), \n",
    "                                       average='binary')\n",
    "print(\"Accuracy on test set: {}\".format(round(precision_score_test, 3)))    \n",
    "\n",
    "\n",
    "confusion_matrix_test = confusion_matrix(np.concatenate(true_test, axis=0), \n",
    "                                         np.concatenate(predictions_test, axis=0))\n",
    "print(\"Confusion_matrix: {}\".format(confusion_matrix_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('btc-classifier': conda)",
   "language": "python",
   "name": "python37664bitbtcclassifiercondaf328939486114fc0aeb107830f867d66"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
